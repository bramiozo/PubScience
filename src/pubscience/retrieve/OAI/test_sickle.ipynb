{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import lxml\n",
    "import bs4\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import oai\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'UVA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_topiclist = ['athero', \n",
    "                        'plaque', \n",
    "                        'cardiovascular',\n",
    "                        'cardiogram', \n",
    "                        'cardiology', \n",
    "                        'cardiologie',\n",
    "                        'hartvaten',\n",
    "                        'healthcare',\n",
    "                        'klinsch',\n",
    "                        'medische',\n",
    "                        'hartvaat',\n",
    "                        'heart', \n",
    "                        'vascular',\n",
    "                        'angiogram', \n",
    "                        'cardiologie', \n",
    "                        'hartziekte', \n",
    "                        'vaatziekte',\n",
    "                        'medicine',\n",
    "                        'disease', \n",
    "                        'medical', \n",
    "                        'therapy',\n",
    "                        'therapeutic',\n",
    "                        'diagnosic',\n",
    "                        'clinical',\n",
    "                        'surgical', \n",
    "                        'metabolic',\n",
    "                        'myocard']\n",
    "\n",
    "# accepted_topiclist = ['recht', 'wetten', 'juridisch',\n",
    "#                       'rechtspraak', 'verordering', 'wetgeving',\n",
    "#                       'richtlijn',\n",
    "#                       'regelgeving',\n",
    "#                       'wetboek', 'bevoegdheid', 'toezicht',\n",
    "#                       'wetboek', 'jurisprudentie', \n",
    "#                       'precedent', 'wet bibop',\n",
    "#                       'wetboek van strafrecht',\n",
    "#                       'wetboek van strafvordering']\n",
    "\n",
    "accepted_dtypes = ['doctoral', 'book', 'article']\n",
    "accepted_languages = ['nl','nld','dut', 'und'] \n",
    "\n",
    "\n",
    "base_url =   oai.sources[source]['link'] #'https://repository.ubn.ru.nl/oai/openaire'  https://scholarlypublications.universiteitleiden.nl/oai2, http://dspace.library.uu.nl/oai/dissertation\n",
    "pdf_path = f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses/{source}/'\n",
    "pdf_path = f'/media/koekiemonster/DATA-FAST/text_data/pubscience/PDF/LAW/RUG' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIRE_institutes = ['VU', 'UVA', 'Maastricht',\n",
    "                       'Tilburg', 'RUG', 'UTwente', \n",
    "                       'TUE', 'UU', 'Erasmus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['clinical', 'medisch', 'medical', 'dissertation', 'umc', 'medicine',\n",
    "            'diss', 'phd', 'thesis', 'doctorate', 'dissertatie', 'doctoralthesis',\n",
    "            'doctoraat', 'proefschrift']\n",
    "if source in OpenAIRE_institutes:\n",
    "    keywords = keywords + ['publications:withfiles']\n",
    "\n",
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    #print(key,val)\n",
    "    if any([c in val.lower() for c in keywords]) | any([c in key.lower() for c in keywords]):\n",
    "        print(f'Set: {key} contains keyword')\n",
    "        Sets_to_mine.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get records \n",
    "# Beware: this takes a long time.\n",
    "from collections import defaultdict\n",
    "records_lists = defaultdict(list)\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    #if set_to_mine in ['com_1874_298213']:\n",
    "    #    continue\n",
    "    print(f\"Mining from set: {set_to_mine}\")\n",
    "    try:\n",
    "        records = sickler.ListRecords(metadataPrefix='oai_dc', \n",
    "                                    ignore_deleted=True, \n",
    "                                    set=set_to_mine) # dissertation com_1874_298213\n",
    "        for record in tqdm(records):\n",
    "            records_lists[set_to_mine].append(record)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic search\n",
    "\n",
    "1. generic bi-encoder model\n",
    "2. generate seed-phrases from the search terms using GPT4\n",
    "3. use the seed-phrases to search for similar documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate seed phrases with GPT-4\n",
    "N_phrases = 50\n",
    "seed_keywords = accepted_topiclist\n",
    "\n",
    "# make seed phrases\n",
    "#\n",
    "prompt = f\"\"\"Please, make {N_phrases} sentences related to the list of keywords that is provided by the user.\\n.\n",
    "Requirements: \\n\n",
    "The output MUST be in Dutch. \\n\n",
    "The output MUST ONLY contain the the list created sentences, seperated by a newline.\n",
    "\"\"\"\n",
    "\n",
    "OAI_ASYNC_CLIENT = AsyncOpenAI(api_key=os.getenv(\"OPENAI_KEY\"), max_retries=2)\n",
    "OAI_CLIENT = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"), max_retries=2)\n",
    "\n",
    "\n",
    "def get_chat_res(USER_TEXT='Good day', \n",
    "                 SYSTEM_PROMPT=prompt, \n",
    "                 n = 1,\n",
    "                 MODEL=\"gpt-4\"):\n",
    "    return OAI_CLIENT.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            n = n,\n",
    "            messages=[\n",
    "                        {\"role\": \"system\",\n",
    "                        \"content\": SYSTEM_PROMPT\n",
    "                        },\n",
    "                        {\"role\": \"user\", \n",
    "                        \"content\": USER_TEXT\n",
    "                        }],\n",
    "            stream=False,\n",
    "        )\n",
    "    \n",
    "GPT_VERSION = 'gpt-4-1106-preview'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = get_chat_res(USER_TEXT=\"\\n\".join(seed_keywords), \n",
    "                       SYSTEM_PROMPT=prompt,\n",
    "                       n=1,\n",
    "                       MODEL=GPT_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_PHRASES = phrases.choices[0].message.content.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADD SEMANTIC SEARCH \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "SBERT = SentenceTransformer('NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_records_lists = defaultdict(list)\n",
    "excluded_records_lists = defaultdict(list)\n",
    "\n",
    "cond_list = []\n",
    "\n",
    "RAW_MINE_RESULTS = {}\n",
    "\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    relevant_counter = 0\n",
    "    for idx, r in tqdm(enumerate(records_lists[set_to_mine])):\n",
    "        _id = f'{set_to_mine}_{idx}'\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        TOPIC = False\n",
    "        PDF = False\n",
    "        DOCTORATE = True if 'dissertation' in set_to_mine.lower() else False\n",
    "        EMBARGO = False\n",
    "        LANG = False\n",
    "        \n",
    "        try:\n",
    "            _lang = meta['language'][0]\n",
    "        except KeyError:\n",
    "            _lang = 'unknown'\n",
    "            \n",
    "        if _lang.strip().lower() in accepted_languages:\n",
    "            LANG = True\n",
    "        \n",
    "\n",
    "        if source in ['Radboud']: \n",
    "            PDF=True\n",
    "        \n",
    "        try:\n",
    "            _subj = \",\".join(meta['subject'])\n",
    "            if any([t in _subj.lower() for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            _subj = \"\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            _title = meta['title']\n",
    "            if any([t in subj.lower() for subj in _title for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            _title = \"\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            _description = meta['description']\n",
    "            if any([t in subj.lower() for subj in _description for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            _description = \"\"\n",
    "            pass                        \n",
    "        \n",
    "        try:\n",
    "            _format = meta['format'][0].lower()\n",
    "            if ('pdf' in _format):\n",
    "                PDF = True\n",
    "        except:\n",
    "            _format = \"\"\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            _type = meta['type'][0].lower()\n",
    "            if any([t in _type for t in accepted_dtypes]):\n",
    "                DOCTORATE = True\n",
    "        except:\n",
    "            _type = \"\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            _rights = meta['rights'][0].lower()\n",
    "            if ('embargo' in _rights) |\\\n",
    "                    ('restricted' in _rights):\n",
    "                EMBARGO = True\n",
    "        except:\n",
    "            _rights = \"\"\n",
    "            pass  \n",
    "        \n",
    "        try:\n",
    "            _date = meta['date'][0].lower()\n",
    "        except:\n",
    "            _date = \"\"\n",
    "            pass\n",
    "        \n",
    "        RAW_MINE_RESULTS[_id] = {                \n",
    "                                'LANG': _lang,\n",
    "                                'TOPIC': _subj,\n",
    "                                'TITLE': _title,\n",
    "                                'DESCRIPTION': _description,            \n",
    "                                'FORMAT': _format,\n",
    "                                'TYPE': _type,\n",
    "                                'RIGHTS': _rights,\n",
    "                                'DATE': _date,\n",
    "                                'TOPIC_OF_INTEREST': TOPIC,\n",
    "                                'META': meta\n",
    "                              }\n",
    "        \n",
    "        \n",
    "        cond_list.append({'Topic': TOPIC, \n",
    "                          'PDF': PDF, \n",
    "                          'Doctorate': DOCTORATE, \n",
    "                          'Embargo': EMBARGO,\n",
    "                          'LANG': LANG}\n",
    "                         )\n",
    "        \n",
    "        if TOPIC & PDF & DOCTORATE & LANG & ~EMBARGO:\n",
    "            relevant_counter += 1\n",
    "            filtered_records_lists[set_to_mine].append(r)\n",
    "        else:\n",
    "            excluded_records_lists[set_to_mine].append(r)\n",
    "\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the embeddings\n",
    "for k, doc in tqdm(RAW_MINE_RESULTS.items()):\n",
    "    TEXT = \" \".join(doc['TITLE']) + \" \".join(doc['DESCRIPTION'])\n",
    "    EMB = SBERT.encode(TEXT)\n",
    "    RAW_MINE_RESULTS[k]['EMB'] = EMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_PHRASE_EMBEDDINGS = {}\n",
    "for i,d in enumerate(SEED_PHRASES):\n",
    "    SEED_PHRASE_EMBEDDINGS[i] = SBERT.encode(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "cos_sim = lambda x,y: 1-cosine(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar seed phrase\n",
    "for k, doc in tqdm(RAW_MINE_RESULTS.items()):\n",
    "    SIMILARITY = []\n",
    "    for i, seed_emb in SEED_PHRASE_EMBEDDINGS.items():\n",
    "        SIMILARITY.append(cos_sim(seed_emb, doc['EMB']))\n",
    "    RAW_MINE_RESULTS[k]['SIMILARITY'] = SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_filtered =  []\n",
    "for k, doc in tqdm(RAW_MINE_RESULTS.items()):\n",
    "    if any([s > 0.50 for s in doc['SIMILARITY']]):\n",
    "        sem_filtered.append(k)    \n",
    "len(sem_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_emb_df = pd.DataFrame.from_dict({f'{k}_SEED':v \n",
    "                                      for k,v in SEED_PHRASE_EMBEDDINGS.items()}, orient='index')\n",
    "\n",
    "result_emb_df = pd.DataFrame.from_dict({k:v['EMB'] \n",
    "                                        for k,v in RAW_MINE_RESULTS.items()}, \n",
    "                                     orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(result_emb_df.values)\n",
    "pca_seed_result = pca.transform(seed_emb_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15,5))\n",
    "ax[0].plot(pca_result[:,0], pca_result[:,1], 'o', alpha=0.05)\n",
    "ax[0].plot(pca_seed_result[:,0], pca_seed_result[:,1], 'o', alpha=1.0)\n",
    "\n",
    "ax[1].plot(pca_result[:,0], pca_result[:,2], 'o', alpha=0.05)\n",
    "ax[1].plot(pca_seed_result[:,0], pca_seed_result[:,2], 'o', alpha=1.0)\n",
    "\n",
    "ax[2].plot(pca_result[:,1], pca_result[:,2], 'o', alpha=0.05)\n",
    "ax[2].plot(pca_seed_result[:,1], pca_seed_result[:,2], 'o', alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapper = UMAP(n_components=3)\n",
    "umap_result = umapper.fit_transform(result_emb_df.values)\n",
    "umap_seed_result = umapper.transform(seed_emb_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15,5))\n",
    "ax[0].plot(umap_result[:,0], umap_result[:,1], 'o', alpha=0.05)\n",
    "ax[0].plot(umap_seed_result[:,0], umap_seed_result[:,1], 'o', alpha=1.0)\n",
    "ax[0].set_xlim(5,15)\n",
    "ax[0].set_ylim(-5,10)\n",
    "\n",
    "ax[1].plot(umap_result[:,0], umap_result[:,2], 'o', alpha=0.05)\n",
    "ax[1].plot(umap_seed_result[:,0], umap_seed_result[:,2], 'o', alpha=1.0)\n",
    "ax[1].set_xlim(5,15)\n",
    "ax[1].set_ylim(-5,10)\n",
    "\n",
    "ax[2].plot(umap_result[:,1], umap_result[:,2], 'o', alpha=0.05)\n",
    "ax[2].plot(umap_seed_result[:,1], umap_seed_result[:,2], 'o', alpha=1.0)\n",
    "ax[2].set_xlim(-5,10)\n",
    "ax[2].set_ylim(-5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_counts = defaultdict(int)\n",
    "for r in excluded_records_lists['publications:withFiles']:\n",
    "    try:\n",
    "        types_counts[r.metadata['type'][0]] += 1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_df = pd.DataFrame(cond_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_theses_with_pdf = (conds_df.Doctorate & conds_df.PDF).sum()\n",
    "print(f\"Total number of theses with PDF: {tot_theses_with_pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "\n",
    "for set_to_mine in filtered_records_lists.keys():\n",
    "    for record in tqdm(filtered_records_lists[set_to_mine]):\n",
    "        META = record.get_metadata()\n",
    "        try:\n",
    "            List_of_identifiers = META['identifier'] if 'identifier' in META.keys() else ['']\n",
    "            Title = META['title'] if 'title' in META.keys() else ['']\n",
    "            Description = META['description'] if 'description' in META.keys() else ['']\n",
    "            Date = META['date'] if 'date' in META.keys() else ['']\n",
    "            Language = META['language'] if 'language' in META.keys() else ['']       \n",
    "            Creator = META['creator'] if 'creator' in META.keys() else ['']\n",
    "            \n",
    "            \n",
    "            List_of_identifiers = [id for id in List_of_identifiers if id is not None]\n",
    "            \n",
    "            if len(List_of_identifiers)>0:\n",
    "                link_list.append({'Set': set_to_mine, \n",
    "                                'Link': List_of_identifiers, \n",
    "                                'Title': Title,\n",
    "                                'Description': Description,\n",
    "                                'Date': Date ,\n",
    "                                'Language': Language,\n",
    "                                'Creator': Creator,\n",
    "                                'Publisher': META.get('publisher'),\n",
    "                                }\n",
    "                                )\n",
    "            else:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            error_list.append(f\"Error parsing {e}: {META}: \")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_counts = defaultdict(int)\n",
    "for r in link_list:\n",
    "    try:\n",
    "        publisher_counts[r['Publisher']] += 1\n",
    "    except:\n",
    "        pass\n",
    "publisher_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs_Leiden(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        # random number between 0.5 and 2.5 seconds\n",
    "        rndSleep = round(random.uniform(1, 5), 2)\n",
    "        sleep(rndSleep)\n",
    "        soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "        found, dsfound, esfound = False, False, False\n",
    "        for _res in soup.findAll('li', {'class':'ubl-file-view'}):\n",
    "            if _res.a is not None:\n",
    "                if _res.a.contents[0].strip().lower() == 'full text':\n",
    "                    _pdfdir = _res.a['href']\n",
    "                    found = True\n",
    "                elif _res.a.contents[0].strip().lower() == 'summary in dutch':\n",
    "                    _dutch_summary = _res.a['href']\n",
    "                    dsfound = True\n",
    "                elif _res.a.contents[0].strip().lower() == 'summary in english':\n",
    "                    _english_summary = _res.a['href']\n",
    "                    esfound = True\n",
    "\n",
    "        linkPdfAlt = f\"https://scholarlypublications.universiteitleiden.nl{_pdfdir}\" if found else None\n",
    "        DutchSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_dutch_summary}\" if dsfound else None\n",
    "        EnglishSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_english_summary}\" if esfound else None\n",
    "        \n",
    "        return linkPdfAlt, DutchSummaryLink, EnglishSummaryLink, r.status_code\n",
    "    except Exception as e:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_links(links, institute=None):\n",
    "    pdf_links = []\n",
    "    if institute in ['VU', 'UVA', 'UTwente']:\n",
    "        inclusion_terms = [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in ['Radboud']:\n",
    "        inclusion_terms = [r'handle', r'bitstream']\n",
    "    elif institute in ['Maastricht']:\n",
    "        inclusion_terms = [r'ASSET1', r'c[0-9]{3,4}\\.pdf']\n",
    "    elif institute in ['Tilburg']:\n",
    "        inclusion_terms = [r'\\.pdf']\n",
    "    elif institute in ['RUG']:\n",
    "        inclusion_terms = [r'summ\\.pdf', r'summary\\.pdf',  r'samenv\\.pdf', r'samenvat\\.pdf', r'[ch][0-9]{1,2}\\.pdf', \n",
    "                           r'thesis\\.pdf', r'proefschrift\\.pdf', r'dissertation\\.pdf', r'dissertatie\\.pdf']\n",
    "    elif institute in ['TUE']:\n",
    "        inclusion_terms = [r'summ\\.pdf', r'summary\\.pdf',  r'samenv\\.pdf', r'samenvat\\.pdf', r'[ch][0-9]{1,2}\\.pdf', \n",
    "                           r'thesis\\.pdf', r'proefschrift\\.pdf', r'dissertation\\.pdf', r'dissertatie\\.pdf']\n",
    "        inclusion_terms = inclusion_terms + [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in ['UU']:\n",
    "        inclusion_terms = [r'dspace\\.library\\.uu\\.nl']\n",
    "    elif institute in ['Leiden']:\n",
    "        inclusion_terms = [r'handle']\n",
    "    elif institute in ['Erasmus']:\n",
    "        inclusion_terms = [r'files']\n",
    "    else:\n",
    "        raise ValueError(f'Institute {institute} not recognized')\n",
    "    \n",
    "    inclusion_terms = [re.compile(rs) for rs in inclusion_terms]\n",
    "    \n",
    "    if institute in ['UU']:\n",
    "        _pdf_links = []\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')):\n",
    "                baselink = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "                _pdf_links.append(baselink + '/full.pdf')\n",
    "        pdf_links = _pdf_links\n",
    "    elif institute in ['Leiden']:\n",
    "        _pdf_links = []\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & (any([t.search(link) is not None for t in inclusion_terms])):\n",
    "                main, dutch_summ, engl_summ, return_code = get_pdfs_Leiden(link)\n",
    "                if return_code == 429:\n",
    "                    print(\"Too many requests. Waiting 60 seconds\")\n",
    "                    sleep(60)\n",
    "                else:\n",
    "                    _pdf_links.extend([main, dutch_summ, engl_summ])\n",
    "        pdf_links = [l for l in _pdf_links if l is not None]\n",
    "    else:\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & \\\n",
    "                    (link.lower().endswith('.pdf')):\n",
    "                if any([t.search(link) is not None for t in inclusion_terms]):\n",
    "                    pdf_links.append(link)        \n",
    "\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pdf_list = []\n",
    "for l in tqdm(link_list):\n",
    "    links = extract_pdf_links(l['Link'], institute=source)\n",
    "    tmp = []\n",
    "    for _l in links:\n",
    "        Creator = l['Creator'][0] if l['Creator'][0] is not None else 'Unknown'\n",
    "        Date = l['Date'][0] if l['Date'][0] is not None else 'Unknown'\n",
    "        \n",
    "        pdfPath = _l.split(\"/\")[-1].replace(\"%20\", \"_\").rstrip('.pdf')\n",
    "        pdfPath = source + \"_\" + Creator + \"_\" + Date + \"_\" + pdfPath\n",
    "        pdfPath = pdfPath.replace(\",\", \"\")\n",
    "        pdfPath = pdfPath.replace(\":\", \"\")\n",
    "        pdfPath = pdfPath.replace(\".\", \"\")\n",
    "        pdfPath = pdfPath.replace(\" \", \"\")\n",
    "        pdfPath = os.path.join(pdf_path, pdfPath+\".pdf\")\n",
    "        tmp.append((_l, pdfPath))\n",
    "    if len(tmp)==0:\n",
    "        continue\n",
    "    l['pdf_links'] = tmp\n",
    "    link_pdf_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(link_pdf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries, based on the title\n",
    "title_set = set()\n",
    "unique_link_list = []\n",
    "for el in link_pdf_list:\n",
    "    try:\n",
    "        title = el['Title'][0]\n",
    "        if title not in title_set:\n",
    "            unique_link_list.append(el)\n",
    "            title_set.add(title)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
    "\n",
    "# add sleep\n",
    "def pdf_writer(pdf_url, _pdf_path):\n",
    "    r = requests.get(pdf_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for i,chunk in enumerate(r.iter_content(chunk_size=1024)): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return True, 200\n",
    "    else:\n",
    "        return False, r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source=='Radboud':\n",
    "    filelist = os.listdir(pdf_path)\n",
    "    pre_list = [f.split(\"_\")[-1] for f in filelist]\n",
    "if source=='UU':\n",
    "    filelist = os.listdir(pdf_path)\n",
    "    pre_list = [f.split(\".\")[-2] for f in filelist]\n",
    "\n",
    "pdf_error_list = []\n",
    "skipped_list = []\n",
    "src_list = []\n",
    "rcode_list = []\n",
    "success_list = []\n",
    "extract_full_text = True\n",
    "for link in tqdm(unique_link_list):\n",
    "    for lt in link['pdf_links']:\n",
    "        _pdf_path = lt[1]\n",
    "        pdf_url = lt[0]\n",
    "        return_code = None\n",
    "\n",
    "        if source=='Radboud':\n",
    "            if _pdf_path.split(\"_\")[-1] in pre_list:\n",
    "                skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "                continue\n",
    "        if source=='UU':\n",
    "            if link['pdf_links'][0][0].split(\"/\")[-2] in pre_list:\n",
    "                skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "                continue\n",
    "            \n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "        else:\n",
    "            # try to download the pdf\n",
    "            try:\n",
    "                write_file, return_code = pdf_writer(pdf_url, _pdf_path)\n",
    "                success_list.append(f'Pdf downloaded: {_pdf_path}')\n",
    "            except Exception as e:\n",
    "                pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')  \n",
    "\n",
    "            if return_code == 429:\n",
    "                print(f'Rate limit exceeded...sleeping 30 seconds')\n",
    "                sleep(30)\n",
    "                continue\n",
    "            else:\n",
    "                sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Found {len(pdf_error_list)} errors while downloading pdfs')\n",
    "print(f'Skipped {len(skipped_list)} pdfs because they already existed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
