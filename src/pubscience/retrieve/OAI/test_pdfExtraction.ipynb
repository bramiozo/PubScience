{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pdf and extract text\n",
    "# We want to ignore all the decorum and only extract the text (i.e. no page number etc.)\n",
    "import pytesseract\n",
    "#from PyPDF2 import PdfReader\n",
    "from pypdf import PdfReader\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import ftfy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Literal\n",
    "\n",
    "import signal\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import threading\n",
    "from time import sleep\n",
    "import _thread as thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "institute = 'Maastricht'\n",
    "pdf_path = f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PRETRAINING/PhDTheses/{institute}/theses/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quit_function(fn_name):\n",
    "    # print to stderr, unbuffered in Python 2.\n",
    "    print('{0} took too long'.format(fn_name), file=sys.stderr)\n",
    "    sys.stderr.flush() # Python 3 stderr is likely buffered.\n",
    "    #raise SystemExit \n",
    "    raise ValueError(f'{fn_name} took too long')\n",
    "\n",
    "def exit_after(s):\n",
    "    '''\n",
    "    use as decorator to exit process if \n",
    "    function takes longer than s seconds\n",
    "    '''\n",
    "    def outer(fn):\n",
    "        def inner(*args, **kwargs):\n",
    "            timer = threading.Timer(s, quit_function, args=[fn.__name__])\n",
    "            timer.start()\n",
    "            try:\n",
    "                result = fn(*args, **kwargs)\n",
    "            finally:\n",
    "                timer.cancel()\n",
    "            return result\n",
    "        return inner\n",
    "    return outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@exit_after(180)\n",
    "def pdf_to_text(path, \n",
    "                backends: List[str]=['pypdf', 'fitz', 'pdfminer'],\n",
    "                ocr_backends:List[str]=['pytesseract','dotocr', 'paddleocr']) -> Tuple[List[str], str]:\n",
    "    '''Extract text from pdf documents\n",
    "        Source: https://towardsdatascience.com/pdf-preprocessing-with-python-19829752af9f\n",
    "    '''\n",
    "\n",
    "    def pdfminer(_path):\n",
    "        manager = PDFResourceManager()\n",
    "        retstr = StringIO()\n",
    "        layout = LAParams(all_texts=False, detect_vertical=True)\n",
    "        device = TextConverter(manager, retstr, laparams=layout)\n",
    "        interpreter = PDFPageInterpreter(manager, device)\n",
    "        with open(_path, 'rb') as filepath:\n",
    "            for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "                interpreter.process_page(page)\n",
    "        text = retstr.getvalue()\n",
    "        device.close()\n",
    "        retstr.close()\n",
    "        return text.split('\\n')\n",
    "\n",
    "    def pypdfer(_path):\n",
    "        bytes_stream = open(_path, 'rb')\n",
    "        reader = PdfReader(bytes_stream, strict=True)\n",
    "        return [p.extract_text(0) for p in reader.pages]\n",
    "    \n",
    "    def check_pdf_scan(_path):\n",
    "        bytes_stream = open(_path, 'rb')\n",
    "        reader = PdfReader(bytes_stream, strict=True)    \n",
    "        meta = reader.metadata\n",
    "        \n",
    "        if any([s in str(meta.producer).lower() for s in ['adobe', 'scanner', 'scan', \n",
    "                                                        'image', 'finereader', 'tesseract']]):\n",
    "            return True\n",
    "        \n",
    "        for page_num, p in enumerate(reader.pages):\n",
    "            text = p.extract_text()\n",
    "            scan_count = 0\n",
    "            if text is None or len(text.strip())==0:\n",
    "                scan_count += 1\n",
    "            if (scan_count / (page_num+1) > 0.75) and (page_num>15):\n",
    "                return True\n",
    "            elif (scan_count / (page_num+1) < 0.25) and (page_num>15):\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def pdf2imagelist(_path, backend: Literal['fitz', 'pdf2image']='fitz') -> List[Image.Image]:\n",
    "        '''Convert each page of a PDF into a list of PIL Image objects.\n",
    "        '''\n",
    "        if backend == 'fitz':\n",
    "            pdf_document = fitz.open(_path)\n",
    "            images = []\n",
    "            for page_num in range(len(pdf_document)):\n",
    "                page = pdf_document.load_page(page_num)\n",
    "                pix = page.get_pixmap()\n",
    "                img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                images.append(img)\n",
    "        elif backend == 'pdf2image':\n",
    "            from pdf2image import convert_from_path\n",
    "            images = convert_from_path(_path)\n",
    "        return images\n",
    "    \n",
    "    pages = []\n",
    "    error = \"\"\n",
    "\n",
    "    if check_pdf_scan(path):      \n",
    "        print(f\"Document {os.path.basename(path)} is detected as a scanned document.\")    \n",
    "        ###########################################\n",
    "        # OCR for scanned documents\n",
    "        ###########################################\n",
    "        if not any([m in ocr_backends for m in ['pytesseract', 'paddleocr', 'dotocr']]):\n",
    "            return None, \"It is a scanned document, but no OCR backend is selected\"\n",
    "        \n",
    "        image_list = pdf2imagelist(path, backend='fitz')\n",
    "\n",
    "        # dotocr\n",
    "\n",
    "        # paddleocr\n",
    "\n",
    "        # LAST RESORT\n",
    "        if 'pytesseract' in ocr_backends:\n",
    "            try:\n",
    "                psm = 1  # Fully automatic page segmentation, but no OSD. (Default)\n",
    "                config_str = f'--psm {psm}' \n",
    "                return [pytesseract.image_to_string(img, config=config_str) \n",
    "                         for img in image_list]\n",
    "            except Exception as e_4:\n",
    "                error = error + f\"\\n PyTesseract failed: {e_4}\" \n",
    "            \n",
    "        return pages, error\n",
    "    else:\n",
    "        ###########################################\n",
    "        # Normal text extraction\n",
    "        ###############################\n",
    "        if 'pypdf' in backends:\n",
    "            try:\n",
    "                pages, error = pypdfer(path, ), None\n",
    "            except Exception as e_1:\n",
    "                error = error + f\"\\n PyPDF failed: {e_1}\"\n",
    "\n",
    "        if pages and len(pages)>0:\n",
    "            return pages, error\n",
    "        ###############################\n",
    "        if 'fitz' in backends:\n",
    "            try:\n",
    "                pdf_file = fitz.open(path)\n",
    "                pages = []\n",
    "                for _p in pdf_file:\n",
    "                    pages.append(_p.get_text())\n",
    "                return pages, error\n",
    "            except Exception as e_2:\n",
    "                error = error + f\"\\n PyMuPDF failed: {e_2}\"\n",
    "\n",
    "        if pages and len(pages)>0:\n",
    "            return pages, error\n",
    "        ###############################\n",
    "        \n",
    "        if 'pdfminer' in backends:\n",
    "            try:\n",
    "                return pdfminer(path), error\n",
    "            except Exception as e_3:\n",
    "                error = error + f\"pdfminer failed: {e_3}\"\n",
    "    \n",
    "    return pages, error\n",
    "\n",
    "        \n",
    "# https://arxiv.org/abs/2308.13418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to ignore \n",
    "# Title pages\n",
    "# Table of contents\n",
    "# Reference lists \n",
    "# Acknowledgements\n",
    "# List of abbreviations\n",
    "# List of figures\n",
    "\n",
    "# Remove\n",
    "# all empty lines or lines that only have numbers\n",
    "# all pages with less than K words\n",
    "\n",
    "# We want to extract\n",
    "# body, summary(english), summary(dutch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_numbers_at_start_of_sentence_plus = re.compile(r'(\\d+\\n\\d*)')  # Matches numbers at the start of a sentence\n",
    "re_numbers_at_start_of_sentence = re.compile(r'(\\d+)\\n')  # Matches numbers at the start of a sentence\n",
    "\n",
    "re_numbers_at_start_of_string = re.compile(r'^(\\d+)')  # Matches numbers at the start of a sentence\n",
    "re_lines_with_only_numbers = re.compile(r'^\\s*\\d+\\s*$', re.MULTILINE)  # Matches lines that contain only numbers\n",
    "re_multiple_newlines = re.compile(r'\\n+')\n",
    "re_empty_lines = re.compile(r'\\n\\s*\\n')\n",
    "re_empty_lines_start = re.compile(r'^\\s*\\n')\n",
    "re_empty_lines_end = re.compile(r'\\n\\s*$')\n",
    "re_multiple_spaces = re.compile(r'\\s+')\n",
    "\n",
    "re_section_num = re.compile(r'^\\d+\\n(\\d*)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(Texts: List[str], max_scount: int=20) -> str:\n",
    "    samenvatting = []\n",
    "    capture = False\n",
    "    scount = 0\n",
    "\n",
    "    init_section_num =\"\"\n",
    "    for page in Texts:\n",
    "        section_num = re_section_num.findall(page)        \n",
    "        page = re_numbers_at_start_of_sentence_plus.sub(r'', page)  # Remove numbers at the start of a sentence\n",
    "        if any((x in page.lower()[:60]) | (x in page.lower()[-60:]) for x in ['s amenvatting', 'samenvatting', \n",
    "                                                'nederlandse samenvatting', \n",
    "                                                'samenvatting in het nederlands',\n",
    "                                                's amenvatting in het nederlands',     \n",
    "                                                    'd utch summary',\n",
    "                                                    'dutch summary',\n",
    "                                                    'n ederlandse samenvatting']):\n",
    "            capture = True\n",
    "            init_section_num =  section_num\n",
    "            scount += 1\n",
    "        elif any((x in page.lower()[:60]) | (x in page.lower()[-60:])for x in ['d ankwoord', \n",
    "                                              'na woord',\n",
    "                                              'a cknowledgment',\n",
    "                                              'c ontents', \n",
    "                                              't able of contents', \n",
    "                                              'l ist of figures', \n",
    "                                              'l ist of abbreviations', \n",
    "                                              'a cknowledgements', \n",
    "                                              'r eferences',\n",
    "                                              'dankwoord',\n",
    "                                              'nawoord', \n",
    "                                              'acknowledgment',\n",
    "                                              'contents', \n",
    "                                              'table of contents', \n",
    "                                              'list of figures', \n",
    "                                              'list of abbreviations', \n",
    "                                              'acknowledgements', \n",
    "                                              'references',\n",
    "                                              's ummary', \n",
    "                                              'summary',\n",
    "                                              'english summary']):\n",
    "            capture = False\n",
    "        elif section_num != init_section_num:\n",
    "            capture = False\n",
    "        \n",
    "        if capture:\n",
    "            scount += 1\n",
    "            samenvatting.append(page)\n",
    "        if scount >= max_scount:\n",
    "            break\n",
    "    summary = []\n",
    "    capture = False\n",
    "    scount = 0\n",
    "    for page in Texts:\n",
    "        page = re_numbers_at_start_of_sentence_plus.sub(r'', page)\n",
    "        if any((x in page.lower()[:60]) | (x in page.lower()[-60:]) for x in ['s ummary', 'summary', 'english summary', 'summery']):\n",
    "            capture = True\n",
    "            init_section_num =  section_num\n",
    "            scount += 1\n",
    "        elif any((x in page.lower()[:60]) | (x in page.lower()[-60:]) for x in ['d ankwoord', \n",
    "                                              'na woord',\n",
    "                                              'a cknowledgment',\n",
    "                                              'c ontents', \n",
    "                                              't able of contents', \n",
    "                                              'l ist of figures', \n",
    "                                              'l ist of abbreviations', \n",
    "                                              'a cknowledgements', \n",
    "                                              'r eferences',\n",
    "                                              'dankwoord',\n",
    "                                              'nawoord', \n",
    "                                              'acknowledgment',\n",
    "                                              'contents', \n",
    "                                              'table of contents', \n",
    "                                              'list of figures', \n",
    "                                              'list of abbreviations', \n",
    "                                              'acknowledgements', \n",
    "                                              'references',\n",
    "                                              's amenvatting', 'samenvatting', \n",
    "                                              'nederlandse samenvatting', \n",
    "                                              'd utch summary',\n",
    "                                              'dutch summary',\n",
    "                                              'n ederlandse samenvatting']):\n",
    "            capture = False\n",
    "        elif section_num != init_section_num:\n",
    "            capture = False\n",
    "            \n",
    "        if capture:\n",
    "            scount += 1\n",
    "            summary.append(page)\n",
    "        if scount >= max_scount:\n",
    "            break\n",
    "    # remove numbers of the start of sentences\n",
    "    # remove multiple newlines\n",
    "    # remove empty lines\n",
    "    # remove multiple spaces\n",
    "    # remove lines with only numbers\n",
    "\n",
    "    summary = [re_numbers_at_start_of_sentence.sub(r'', s) for s in summary]\n",
    "    summary = [re_empty_lines_start.sub(r'', s) for s in summary]\n",
    "    summary = [re_empty_lines_end.sub(r'', s) for s in summary]\n",
    "    summary = [re_empty_lines.sub(r'', s) for s in summary]\n",
    "    summary = [re_lines_with_only_numbers.sub(r'', s) for s in summary]\n",
    "    summary = [re_numbers_at_start_of_string.sub(r'\\n', s) for s in summary]\n",
    "\n",
    "    samenvatting = [re_numbers_at_start_of_sentence.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_empty_lines_start.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_empty_lines_end.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_empty_lines.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_lines_with_only_numbers.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_numbers_at_start_of_string.sub(r'\\n', s) for s in samenvatting]\n",
    "    return '\\n'.join(summary), '\\n'.join(samenvatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(Text: List[str], min_words: int=100) -> List[str]:\n",
    "    Text = [ftfy.fix_text(t) for t in Text]\n",
    "\n",
    "    Text = [t for t in Text if len(t.split())>50]\n",
    "    Text = [re_numbers_at_start_of_sentence.sub('', t) for t in Text]\n",
    "    Text = [re_numbers_at_start_of_string.sub('', t) for t in Text]\n",
    "    Text = [re_lines_with_only_numbers.sub('', t) for t in Text]\n",
    "    Text = [re_multiple_newlines.sub('\\n', t) for t in Text]\n",
    "    Text = [re_empty_lines.sub('\\n', t) for t in Text]\n",
    "    Text = [re_empty_lines_start.sub('', t) for t in Text]\n",
    "    Text = [re_empty_lines_end.sub('', t) for t in Text]\n",
    "    Text = [re_multiple_spaces.sub(' ', t) for t in Text]\n",
    "    Text = [t for t in Text if len(t.split())>50]\n",
    "\n",
    "    # ignore references\n",
    "    reference_phrases = ['references', 'literature', 'bibliography', 'referenties', 'literatuurlijst']\n",
    "    Text = [t for t in Text if not any(reference_phrase in t.lower() for reference_phrase in reference_phrases)]\n",
    "    # ignore lime that start with numbers after the linebreak or have \"doi:10\" in them\n",
    "    # scan the page for lines that start with numbers after a linebreak\n",
    "    _TEXT = []\n",
    "    for page in Text:\n",
    "        lines = page.split('\\n')\n",
    "        __page= []\n",
    "        for line in lines:\n",
    "            if (not re.search(r'^\\d+', line)) and ('doi:10' not in line.lower()):\n",
    "                __page.append(line)\n",
    "        _TEXT.append('\\n'.join(__page))\n",
    "    Text = _TEXT\n",
    "\n",
    "    # ignore list of figures\n",
    "    figure_phrases = ['list of figures', 'lijst van figuren']\n",
    "    Text = [t for t in Text if not any(figure_phrase in t.lower() for figure_phrase in figure_phrases)]\n",
    "\n",
    "    # ignore list of abbreviations\n",
    "    abbreviation_phrases = ['list of abbreviations', 'lijst van afkortingen']\n",
    "    Text = [t for t in Text if not any(abbreviation_phrase in t.lower() for abbreviation_phrase in abbreviation_phrases)]\n",
    "\n",
    "    # ignore copyright page\n",
    "    copyright_phrases = ['all rights reserved', 'no part of this publication may be reproduced', 'copyright', 'uitgeverij']\n",
    "    Text = [t for t in Text if not any(copyright_phrase in t.lower() for copyright_phrase in copyright_phrases)]\n",
    "\n",
    "    phd_phrases = ['volgens besluit van het college voor promoties', 'de graad van doctor aan']\n",
    "    Text = [t for t in Text if not any(phd_phrase in t.lower() for phd_phrase in phd_phrases)]\n",
    "\n",
    "    # ignore table of contents\n",
    "    toc_phrases = ['inhoudsopgave', 'table of contents']\n",
    "    Text = [t for t in Text if not any(toc_phrase in t.lower() for toc_phrase in toc_phrases)]\n",
    "    # ignore if multiple sentences in a page start with \"chapter \\d\"\n",
    "    chapter_phrases = ['chapter ', 'hoofdstuk ']\n",
    "    Text = [t for t in Text if sum(t.lower().count(chapter_phrase) for chapter_phrase in chapter_phrases)<2]\n",
    "\n",
    "    # ignore acknowledgements\n",
    "    acknowledgement_phrases = ['acknowledgements', 'acknowledgements', 'dankwoord', 'dankbetuiging']\n",
    "    Text = [t for t in Text if not any(acknowledgement_phrase in t.lower() for acknowledgement_phrase in acknowledgement_phrases)]\n",
    "\n",
    "    # ignore list of publications\n",
    "    publication_phrases = ['list of publications', 'lijst van publicaties', 'bibliography', 'bibliografie']\n",
    "    Text = [t for t in Text if not any(publication_phrase in t.lower() for publication_phrase in publication_phrases)]\n",
    "\n",
    "\n",
    "    Text = [t for t in Text if len(t.split())>25]\n",
    "\n",
    "    TextNumWords = [len(t.split()) for t in Text]\n",
    "    \n",
    "\n",
    "    return Text, TextNumWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "\n",
    "ListOfTexts = []    \n",
    "ListOfSummaries = []\n",
    "ListOfNumWordLists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ListOfNumWordLists), len(Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, _File in enumerate(Files):\n",
    "    print(f\"Processing file {k+1}/{len(Files)}: {_File}\")\n",
    "    _path = os.path.join(pdf_path, _File)\n",
    "    try:\n",
    "        Text, error = pdf_to_text(_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {_File}: {e}\")\n",
    "        continue\n",
    "    SummaryEnglish, SummaryDutch = extract_summary(Text)\n",
    "    CleanedText, NumWordList = extractor(Text, min_words=30)\n",
    "\n",
    "    for k, page in enumerate(CleanedText):\n",
    "        ListOfTexts.append({\n",
    "            'institute': institute,\n",
    "            'file': _File,\n",
    "            'pseudo_pagenum': k,\n",
    "            'text': page\n",
    "        })\n",
    "    ListOfSummaries.append({\n",
    "        'institute': institute,\n",
    "        'file': _File,\n",
    "        'summary_english': SummaryEnglish,\n",
    "        'summary_dutch': SummaryDutch\n",
    "    })\n",
    "    ListOfNumWordLists.append({\n",
    "        'institute': institute,\n",
    "        'file': _File,\n",
    "        'num_words_per_page': NumWordList\n",
    "    })          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([_r for r in ListOfNumWordLists for _r in r['num_words_per_page']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfSummariesWith = [d for d in ListOfSummaries if (d['summary_english']!='') or (d['summary_dutch']!='')]\n",
    "with_sum = len(ListOfSummariesWith)\n",
    "ListOfSummariesWithout = [d for d in ListOfSummaries if (d['summary_english']=='') and (d['summary_dutch']=='')]\n",
    "without_sum = len(ListOfSummariesWithout)\n",
    "print(f'Out of {len(ListOfSummaries)} theses, {with_sum} have summaries and {without_sum} do not have summaries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PRETRAINING/PhDTheses/{institute}_texts.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in ListOfTexts:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same for the summaries\n",
    "with open(f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PRETRAINING/PhDTheses/{institute}_summaries.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in ListOfSummariesWith:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Maastricht_AckerstaffRGA_1985-01-01_guid-ff2f4913-aa44-41db-806a-1cac7440012c-ASSET10.pdf is detected as a scanned document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pdf_to_text took too long\n",
      "Exception in thread Thread-873:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bes3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\bes3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1431, in run\n",
      "    self.function(*self.args, **self.kwargs)\n",
      "  File \"C:\\Users\\bes3\\AppData\\Local\\Temp\\ipykernel_28996\\2091112411.py\", line 6, in quit_function\n",
      "ValueError: pdf_to_text took too long\n"
     ]
    }
   ],
   "source": [
    "_File = 'Maastricht_AckerstaffRGA_1985-01-01_guid-ff2f4913-aa44-41db-806a-1cac7440012c-ASSET10.pdf'\n",
    "\n",
    "_path = os.path.join(pdf_path, _File)\n",
    "\n",
    "pdf_to_text(_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPage segmentation modes (PSM):\\n  0    Orientation and script detection (OSD) only.\\n  1    Automatic page segmentation with OSD.\\n  2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\\n  3    Fully automatic page segmentation, but no OSD. (Default)\\n  4    Assume a single column of text of variable sizes.\\n  5    Assume a single uniform block of vertically aligned text.\\n  6    Assume a single uniform block of text.\\n  7    Treat the image as a single text line.\\n  8    Treat the image as a single word.\\n  9    Treat the image as a single word in a circle.\\n 10    Treat the image as a single character.\\n 11    Sparse text. Find as much text as possible in no particular order.\\n 12    Sparse text with OSD.\\n 13    Raw line. Treat the image as a single text line,\\n       bypassing hacks that are Tesseract-specific.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page segmentation modes (PSM):\n",
    "  0    Orientation and script detection (OSD) only.\n",
    "  1    Automatic page segmentation with OSD.\n",
    "  2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
    "  3    Fully automatic page segmentation, but no OSD. (Default)\n",
    "  4    Assume a single column of text of variable sizes.\n",
    "  5    Assume a single uniform block of vertically aligned text.\n",
    "  6    Assume a single uniform block of text.\n",
    "  7    Treat the image as a single text line.\n",
    "  8    Treat the image as a single word.\n",
    "  9    Treat the image as a single word in a circle.\n",
    " 10    Treat the image as a single character.\n",
    " 11    Sparse text. Find as much text as possible in no particular order.\n",
    " 12    Sparse text with OSD.\n",
    " 13    Raw line. Treat the image as a single text line,\n",
    "       bypassing hacks that are Tesseract-specific.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubscience-GFzZkKDp-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
