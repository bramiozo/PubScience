{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pdf and extract text\n",
    "# We want to ignore all the decorum and only extract the text (i.e. no page number etc.)\n",
    "import pytesseract\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import ftfy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "institute = 'UU'\n",
    "pdf_path = f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PRETRAINING/PhDTheses/{institute}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path, backends=['pypdf', 'fitz']):\n",
    "    '''Extract text from pdf documents\n",
    "        Source: https://towardsdatascience.com/pdf-preprocessing-with-python-19829752af9f\n",
    "    '''\n",
    "    def pdfminer(_path):\n",
    "        manager = PDFResourceManager()\n",
    "        retstr = StringIO()\n",
    "        layout = LAParams(all_texts=False, detect_vertical=True)\n",
    "        device = TextConverter(manager, retstr, laparams=layout)\n",
    "        interpreter = PDFPageInterpreter(manager, device)\n",
    "        with open(_path, 'rb') as filepath:\n",
    "            for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "                interpreter.process_page(page)\n",
    "        text = retstr.getvalue()\n",
    "        device.close()\n",
    "        retstr.close()\n",
    "        return text\n",
    "\n",
    "    def pypdfer(_path):\n",
    "        reader = PdfReader(_path)\n",
    "        return [p.extract_text(0) for p in reader.pages]\n",
    "    \n",
    "    error = \"\"\n",
    "\n",
    "    if 'pypdf' in backends:\n",
    "        try:\n",
    "            return pypdfer(path), None\n",
    "        except Exception as e_1:\n",
    "            error = error + f\"\\n PyPDF failed: {e_1}\"\n",
    "   \n",
    "    if 'fitz' in backends:\n",
    "        try:\n",
    "            pdf_file = fitz.open(path)\n",
    "            pages = []\n",
    "            for _p in pdf_file:\n",
    "                pages.append(_p.getText())\n",
    "            return pages, error\n",
    "        except Exception as e_2:\n",
    "            error = error + f\"\\n PyMuPDF failed: {e_2}\"\n",
    "        \n",
    "    if 'pdfminer' in backends:\n",
    "        try:\n",
    "            return pdfminer(path), None\n",
    "        except Exception as e_3:\n",
    "            error = error + f\"pdfminer failed: {e_3}\"\n",
    "\n",
    "    if 'pytesseract' in backends:\n",
    "        try:\n",
    "            return pytesseract.image_to_string(path, lang='en'), None\n",
    "        except Exception as e_4:\n",
    "            return error + f\"\\n PyTesseract failed: {e_4}\" \n",
    "        \n",
    "# https://arxiv.org/abs/2308.13418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to ignore \n",
    "# Title pages\n",
    "# Table of contents\n",
    "# Reference lists \n",
    "# Acknowledgements\n",
    "# List of abbreviations\n",
    "# List of figures\n",
    "\n",
    "# Remove\n",
    "# all empty lines or lines that only have numbers\n",
    "# all pages with less than K words\n",
    "\n",
    "# We want to extract\n",
    "# body, summary(english), summary(dutch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(Texts: List[str], max_scount: int=20) -> str:\n",
    "    samenvatting = []\n",
    "    capture = False\n",
    "    scount = 0\n",
    "    for page in Texts:\n",
    "        if any(page.lower().startswith(x) for x in ['s amenvatting', 'samenvatting', \n",
    "                                                    'nederlandse samenvatting', 'n ederlandse samenvatting']):\n",
    "            capture = True\n",
    "            scount += 1\n",
    "        elif any(page.lower().startswith(x) for x in ['d ankwoord', \n",
    "                                              'na woord',\n",
    "                                              'a cknowledgment',\n",
    "                                              'c ontents', \n",
    "                                              't able of contents', \n",
    "                                              'l ist of figures', \n",
    "                                              'l ist of abbreviations', \n",
    "                                              'a cknowledgements', \n",
    "                                              'r eferences',\n",
    "                                              'dankwoord',\n",
    "                                              'nawoord', \n",
    "                                              'acknowledgment',\n",
    "                                              'contents', \n",
    "                                              'table of contents', \n",
    "                                              'list of figures', \n",
    "                                              'list of abbreviations', \n",
    "                                              'acknowledgements', \n",
    "                                              'references']):\n",
    "            capture = False\n",
    "        if capture:\n",
    "            scount += 1\n",
    "            samenvatting.append(page)\n",
    "        if scount >= max_scount:\n",
    "            break\n",
    "    summary = []\n",
    "    capture = False\n",
    "    scount = 0\n",
    "    for page in Texts:\n",
    "        if any(page.lower().startswith(x) for x in ['s ummary', 'summary', 'english summary']):\n",
    "            capture = True\n",
    "            scount += 1\n",
    "        elif any(page.lower().startswith(x) for x in ['d ankwoord', \n",
    "                                              'na woord',\n",
    "                                              'a cknowledgment',\n",
    "                                              'c ontents', \n",
    "                                              't able of contents', \n",
    "                                              'l ist of figures', \n",
    "                                              'l ist of abbreviations', \n",
    "                                              'a cknowledgements', \n",
    "                                              'r eferences',\n",
    "                                              'dankwoord',\n",
    "                                              'nawoord', \n",
    "                                              'acknowledgment',\n",
    "                                              'contents', \n",
    "                                              'table of contents', \n",
    "                                              'list of figures', \n",
    "                                              'list of abbreviations', \n",
    "                                              'acknowledgements', \n",
    "                                              'references']):\n",
    "            capture = False\n",
    "        if capture:\n",
    "            scount += 1\n",
    "            summary.append(page)\n",
    "        if scount >= max_scount:\n",
    "            break\n",
    "    return '\\n'.join(summary), '\\n'.join(samenvatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_numbers_at_start_of_sentence = re.compile(r'(\\d+)\\n')  # Matches numbers at the start of a sentence\n",
    "re_numbers_at_start_of_string = re.compile(r'^(\\d+)')  # Matches numbers at the start of a sentence\n",
    "re_lines_with_only_numbers = re.compile(r'^\\s*\\d+\\s*$', re.MULTILINE)  # Matches lines that contain only numbers\n",
    "re_multiple_newlines = re.compile(r'\\n+')\n",
    "re_empty_lines = re.compile(r'\\n\\s*\\n')\n",
    "re_empty_lines_start = re.compile(r'^\\s*\\n')\n",
    "re_empty_lines_end = re.compile(r'\\n\\s*$')\n",
    "re_multiple_spaces = re.compile(r'\\s+')\n",
    "\n",
    "\n",
    "def extractor(Text: List[str], min_words: int=100) -> List[str]:\n",
    "    Text = [ftfy.fix_text(t) for t in Text]\n",
    "\n",
    "    Text = [t for t in Text if len(t.split())>50]\n",
    "    Text = [re_numbers_at_start_of_sentence.sub('', t) for t in Text]\n",
    "    Text = [re_numbers_at_start_of_string.sub('', t) for t in Text]\n",
    "    Text = [re_lines_with_only_numbers.sub('', t) for t in Text]\n",
    "    Text = [re_multiple_newlines.sub('\\n', t) for t in Text]\n",
    "    Text = [re_empty_lines.sub('\\n', t) for t in Text]\n",
    "    Text = [re_empty_lines_start.sub('', t) for t in Text]\n",
    "    Text = [re_empty_lines_end.sub('', t) for t in Text]\n",
    "    Text = [re_multiple_spaces.sub(' ', t) for t in Text]\n",
    "    Text = [t for t in Text if len(t.split())>50]\n",
    "\n",
    "    # ignore references\n",
    "    reference_phrases = ['references', 'literature', 'bibliography', 'referenties', 'literatuurlijst']\n",
    "    Text = [t for t in Text if not any(reference_phrase in t.lower() for reference_phrase in reference_phrases)]\n",
    "    # ignore lime that start with numbers after the linebreak or have \"doi:10\" in them\n",
    "    # scan the page for lines that start with numbers after a linebreak\n",
    "    _TEXT = []\n",
    "    for page in Text:\n",
    "        lines = page.split('\\n')\n",
    "        __page= []\n",
    "        for line in lines:\n",
    "            if (not re.search(r'^\\d+', line)) and ('doi:10' not in line.lower()):\n",
    "                __page.append(line)\n",
    "        _TEXT.append('\\n'.join(__page))\n",
    "    Text = _TEXT\n",
    "\n",
    "    # ignore list of figures\n",
    "    figure_phrases = ['list of figures', 'lijst van figuren']\n",
    "    Text = [t for t in Text if not any(figure_phrase in t.lower() for figure_phrase in figure_phrases)]\n",
    "\n",
    "    # ignore list of abbreviations\n",
    "    abbreviation_phrases = ['list of abbreviations', 'lijst van afkortingen']\n",
    "    Text = [t for t in Text if not any(abbreviation_phrase in t.lower() for abbreviation_phrase in abbreviation_phrases)]\n",
    "\n",
    "    # ignore copyright page\n",
    "    copyright_phrases = ['all rights reserved', 'no part of this publication may be reproduced', 'copyright', 'uitgeverij']\n",
    "    Text = [t for t in Text if not any(copyright_phrase in t.lower() for copyright_phrase in copyright_phrases)]\n",
    "\n",
    "    phd_phrases = ['volgens besluit van het college voor promoties', 'de graad van doctor aan']\n",
    "    Text = [t for t in Text if not any(phd_phrase in t.lower() for phd_phrase in phd_phrases)]\n",
    "\n",
    "    # ignore table of contents\n",
    "    toc_phrases = ['inhoudsopgave', 'table of contents']\n",
    "    Text = [t for t in Text if not any(toc_phrase in t.lower() for toc_phrase in toc_phrases)]\n",
    "    # ignore if multiple sentences in a page start with \"chapter \\d\"\n",
    "    chapter_phrases = ['chapter ', 'hoofdstuk ']\n",
    "    Text = [t for t in Text if sum(t.lower().count(chapter_phrase) for chapter_phrase in chapter_phrases)<2]\n",
    "\n",
    "    # ignore acknowledgements\n",
    "    acknowledgement_phrases = ['acknowledgements', 'acknowledgements', 'dankwoord', 'dankbetuiging']\n",
    "    Text = [t for t in Text if not any(acknowledgement_phrase in t.lower() for acknowledgement_phrase in acknowledgement_phrases)]\n",
    "\n",
    "    # ignore list of publications\n",
    "    publication_phrases = ['list of publications', 'lijst van publicaties', 'bibliography', 'bibliografie']\n",
    "    Text = [t for t in Text if not any(publication_phrase in t.lower() for publication_phrase in publication_phrases)]\n",
    "\n",
    "\n",
    "    Text = [t for t in Text if len(t.split())>25]\n",
    "\n",
    "    TextNumWords = [len(t.split()) for t in Text]\n",
    "    \n",
    "\n",
    "    return Text, TextNumWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 32/1223 [02:10<52:35,  2.65s/it]  "
     ]
    }
   ],
   "source": [
    "Files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "\n",
    "ListOfTexts = []    \n",
    "ListOfSummaries = []\n",
    "ListOfNumWordLists = []\n",
    "for _File in tqdm(Files):\n",
    "    _path = os.path.join(pdf_path, _File)\n",
    "    try:\n",
    "        Text, error = pdf_to_text(_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {_File}: {e}\")\n",
    "        continue\n",
    "    SummaryEnglish, SummaryDutch = extract_summary(Text)\n",
    "    CleanedText, NumWordList = extractor(Text, min_words=25)\n",
    "\n",
    "    for k, page in enumerate(CleanedText):\n",
    "        ListOfTexts.append({\n",
    "            'institute': institute,\n",
    "            'file': _File,\n",
    "            'pseudo_pagenum': k,\n",
    "            'text': page\n",
    "        })\n",
    "    ListOfSummaries.append({\n",
    "        'institute': institute,\n",
    "        'file': _File,\n",
    "        'summary_english': SummaryEnglish,\n",
    "        'summary_dutch': SummaryDutch\n",
    "    })\n",
    "    ListOfNumWordLists.append({\n",
    "        'institute': institute,\n",
    "        'file': _File,\n",
    "        'num_words_per_page': NumWordList\n",
    "    })          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubscience-GFzZkKDp-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
