{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pdf and extract text\n",
    "# We want to ignore all the decorum and only extract the text (i.e. no page number etc.)\n",
    "import pytesseract\n",
    "#from PyPDF2 import PdfReader\n",
    "from pypdf import PdfReader\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import ftfy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "import signal\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import threading\n",
    "from time import sleep\n",
    "import _thread as thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "institute = 'Erasmus'\n",
    "pdf_path = f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PRETRAINING/PhDTheses/{institute}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quit_function(fn_name):\n",
    "    # print to stderr, unbuffered in Python 2.\n",
    "    print('{0} took too long'.format(fn_name), file=sys.stderr)\n",
    "    sys.stderr.flush() # Python 3 stderr is likely buffered.\n",
    "    raise SystemExit \n",
    "    #raise TimeoutError\n",
    "\n",
    "def exit_after(s):\n",
    "    '''\n",
    "    use as decorator to exit process if \n",
    "    function takes longer than s seconds\n",
    "    '''\n",
    "    def outer(fn):\n",
    "        def inner(*args, **kwargs):\n",
    "            timer = threading.Timer(s, quit_function, args=[fn.__name__])\n",
    "            timer.start()\n",
    "            try:\n",
    "                result = fn(*args, **kwargs)\n",
    "            finally:\n",
    "                timer.cancel()\n",
    "            return result\n",
    "        return inner\n",
    "    return outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@exit_after(10)\n",
    "def pdf_to_text(path, backends=['pypdf', 'fitz', 'pdfminer', 'pytesseract']) -> Tuple[List[str], str]:\n",
    "    '''Extract text from pdf documents\n",
    "        Source: https://towardsdatascience.com/pdf-preprocessing-with-python-19829752af9f\n",
    "    '''\n",
    "    def pdfminer(_path):\n",
    "        manager = PDFResourceManager()\n",
    "        retstr = StringIO()\n",
    "        layout = LAParams(all_texts=False, detect_vertical=True)\n",
    "        device = TextConverter(manager, retstr, laparams=layout)\n",
    "        interpreter = PDFPageInterpreter(manager, device)\n",
    "        with open(_path, 'rb') as filepath:\n",
    "            for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "                interpreter.process_page(page)\n",
    "        text = retstr.getvalue()\n",
    "        device.close()\n",
    "        retstr.close()\n",
    "        return text.split('\\n')\n",
    "\n",
    "    def pypdfer(_path):\n",
    "        bytes_stream = open(_path, 'rb')\n",
    "        reader = PdfReader(bytes_stream, strict=True)\n",
    "        return [p.extract_text(0) for p in reader.pages]\n",
    "    \n",
    "    error = \"\"\n",
    "\n",
    "    if 'pypdf' in backends:\n",
    "        try:\n",
    "            return pypdfer(path), None\n",
    "        except Exception as e_1:\n",
    "            error = error + f\"\\n PyPDF failed: {e_1}\"\n",
    "   \n",
    "    if 'fitz' in backends:\n",
    "        try:\n",
    "            pdf_file = fitz.open(path)\n",
    "            pages = []\n",
    "            for _p in pdf_file:\n",
    "                pages.append(_p.getText())\n",
    "            return pages, error\n",
    "        except Exception as e_2:\n",
    "            error = error + f\"\\n PyMuPDF failed: {e_2}\"\n",
    "        \n",
    "    if 'pdfminer' in backends:\n",
    "        try:\n",
    "            return pdfminer(path), error\n",
    "        except Exception as e_3:\n",
    "            error = error + f\"pdfminer failed: {e_3}\"\n",
    "\n",
    "    if 'pytesseract' in backends:\n",
    "        try:\n",
    "            return pytesseract.image_to_string(path, lang='en'), error\n",
    "        except Exception as e_4:\n",
    "            return error + f\"\\n PyTesseract failed: {e_4}\" \n",
    "        \n",
    "# https://arxiv.org/abs/2308.13418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to ignore \n",
    "# Title pages\n",
    "# Table of contents\n",
    "# Reference lists \n",
    "# Acknowledgements\n",
    "# List of abbreviations\n",
    "# List of figures\n",
    "\n",
    "# Remove\n",
    "# all empty lines or lines that only have numbers\n",
    "# all pages with less than K words\n",
    "\n",
    "# We want to extract\n",
    "# body, summary(english), summary(dutch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_numbers_at_start_of_sentence_plus = re.compile(r'(\\d+\\n\\d*)')  # Matches numbers at the start of a sentence\n",
    "re_numbers_at_start_of_sentence = re.compile(r'(\\d+)\\n')  # Matches numbers at the start of a sentence\n",
    "\n",
    "re_numbers_at_start_of_string = re.compile(r'^(\\d+)')  # Matches numbers at the start of a sentence\n",
    "re_lines_with_only_numbers = re.compile(r'^\\s*\\d+\\s*$', re.MULTILINE)  # Matches lines that contain only numbers\n",
    "re_multiple_newlines = re.compile(r'\\n+')\n",
    "re_empty_lines = re.compile(r'\\n\\s*\\n')\n",
    "re_empty_lines_start = re.compile(r'^\\s*\\n')\n",
    "re_empty_lines_end = re.compile(r'\\n\\s*$')\n",
    "re_multiple_spaces = re.compile(r'\\s+')\n",
    "\n",
    "re_section_num = re.compile(r'^\\d+\\n(\\d*)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(Texts: List[str], max_scount: int=20) -> str:\n",
    "    samenvatting = []\n",
    "    capture = False\n",
    "    scount = 0\n",
    "\n",
    "    init_section_num =\"\"\n",
    "    for page in Texts:\n",
    "        section_num = re_section_num.findall(page)        \n",
    "        page = re_numbers_at_start_of_sentence_plus.sub(r'', page)  # Remove numbers at the start of a sentence\n",
    "        if any((x in page.lower()[:60]) | (x in page.lower()[-60:]) for x in ['s amenvatting', 'samenvatting', \n",
    "                                                'nederlandse samenvatting', \n",
    "                                                'samenvatting in het nederlands',\n",
    "                                                's amenvatting in het nederlands',     \n",
    "                                                    'd utch summary',\n",
    "                                                    'dutch summary',\n",
    "                                                    'n ederlandse samenvatting']):\n",
    "            capture = True\n",
    "            init_section_num =  section_num\n",
    "            scount += 1\n",
    "        elif any((x in page.lower()[:60]) | (x in page.lower()[-60:])for x in ['d ankwoord', \n",
    "                                              'na woord',\n",
    "                                              'a cknowledgment',\n",
    "                                              'c ontents', \n",
    "                                              't able of contents', \n",
    "                                              'l ist of figures', \n",
    "                                              'l ist of abbreviations', \n",
    "                                              'a cknowledgements', \n",
    "                                              'r eferences',\n",
    "                                              'dankwoord',\n",
    "                                              'nawoord', \n",
    "                                              'acknowledgment',\n",
    "                                              'contents', \n",
    "                                              'table of contents', \n",
    "                                              'list of figures', \n",
    "                                              'list of abbreviations', \n",
    "                                              'acknowledgements', \n",
    "                                              'references',\n",
    "                                              's ummary', \n",
    "                                              'summary',\n",
    "                                              'english summary']):\n",
    "            capture = False\n",
    "        elif section_num != init_section_num:\n",
    "            capture = False\n",
    "        \n",
    "        if capture:\n",
    "            scount += 1\n",
    "            samenvatting.append(page)\n",
    "        if scount >= max_scount:\n",
    "            break\n",
    "    summary = []\n",
    "    capture = False\n",
    "    scount = 0\n",
    "    for page in Texts:\n",
    "        page = re_numbers_at_start_of_sentence_plus.sub(r'', page)\n",
    "        if any((x in page.lower()[:60]) | (x in page.lower()[-60:]) for x in ['s ummary', 'summary', 'english summary', 'summery']):\n",
    "            capture = True\n",
    "            init_section_num =  section_num\n",
    "            scount += 1\n",
    "        elif any((x in page.lower()[:60]) | (x in page.lower()[-60:]) for x in ['d ankwoord', \n",
    "                                              'na woord',\n",
    "                                              'a cknowledgment',\n",
    "                                              'c ontents', \n",
    "                                              't able of contents', \n",
    "                                              'l ist of figures', \n",
    "                                              'l ist of abbreviations', \n",
    "                                              'a cknowledgements', \n",
    "                                              'r eferences',\n",
    "                                              'dankwoord',\n",
    "                                              'nawoord', \n",
    "                                              'acknowledgment',\n",
    "                                              'contents', \n",
    "                                              'table of contents', \n",
    "                                              'list of figures', \n",
    "                                              'list of abbreviations', \n",
    "                                              'acknowledgements', \n",
    "                                              'references',\n",
    "                                              's amenvatting', 'samenvatting', \n",
    "                                              'nederlandse samenvatting', \n",
    "                                              'd utch summary',\n",
    "                                              'dutch summary',\n",
    "                                              'n ederlandse samenvatting']):\n",
    "            capture = False\n",
    "        elif section_num != init_section_num:\n",
    "            capture = False\n",
    "            \n",
    "        if capture:\n",
    "            scount += 1\n",
    "            summary.append(page)\n",
    "        if scount >= max_scount:\n",
    "            break\n",
    "    # remove numbers of the start of sentences\n",
    "    # remove multiple newlines\n",
    "    # remove empty lines\n",
    "    # remove multiple spaces\n",
    "    # remove lines with only numbers\n",
    "\n",
    "    summary = [re_numbers_at_start_of_sentence.sub(r'', s) for s in summary]\n",
    "    summary = [re_empty_lines_start.sub(r'', s) for s in summary]\n",
    "    summary = [re_empty_lines_end.sub(r'', s) for s in summary]\n",
    "    summary = [re_empty_lines.sub(r'', s) for s in summary]\n",
    "    summary = [re_lines_with_only_numbers.sub(r'', s) for s in summary]\n",
    "    summary = [re_numbers_at_start_of_string.sub(r'\\n', s) for s in summary]\n",
    "\n",
    "    samenvatting = [re_numbers_at_start_of_sentence.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_empty_lines_start.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_empty_lines_end.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_empty_lines.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_lines_with_only_numbers.sub(r'', s) for s in samenvatting]\n",
    "    samenvatting = [re_numbers_at_start_of_string.sub(r'\\n', s) for s in samenvatting]\n",
    "    return '\\n'.join(summary), '\\n'.join(samenvatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(Text: List[str], min_words: int=100) -> List[str]:\n",
    "    Text = [ftfy.fix_text(t) for t in Text]\n",
    "\n",
    "    Text = [t for t in Text if len(t.split())>50]\n",
    "    Text = [re_numbers_at_start_of_sentence.sub('', t) for t in Text]\n",
    "    Text = [re_numbers_at_start_of_string.sub('', t) for t in Text]\n",
    "    Text = [re_lines_with_only_numbers.sub('', t) for t in Text]\n",
    "    Text = [re_multiple_newlines.sub('\\n', t) for t in Text]\n",
    "    Text = [re_empty_lines.sub('\\n', t) for t in Text]\n",
    "    Text = [re_empty_lines_start.sub('', t) for t in Text]\n",
    "    Text = [re_empty_lines_end.sub('', t) for t in Text]\n",
    "    Text = [re_multiple_spaces.sub(' ', t) for t in Text]\n",
    "    Text = [t for t in Text if len(t.split())>50]\n",
    "\n",
    "    # ignore references\n",
    "    reference_phrases = ['references', 'literature', 'bibliography', 'referenties', 'literatuurlijst']\n",
    "    Text = [t for t in Text if not any(reference_phrase in t.lower() for reference_phrase in reference_phrases)]\n",
    "    # ignore lime that start with numbers after the linebreak or have \"doi:10\" in them\n",
    "    # scan the page for lines that start with numbers after a linebreak\n",
    "    _TEXT = []\n",
    "    for page in Text:\n",
    "        lines = page.split('\\n')\n",
    "        __page= []\n",
    "        for line in lines:\n",
    "            if (not re.search(r'^\\d+', line)) and ('doi:10' not in line.lower()):\n",
    "                __page.append(line)\n",
    "        _TEXT.append('\\n'.join(__page))\n",
    "    Text = _TEXT\n",
    "\n",
    "    # ignore list of figures\n",
    "    figure_phrases = ['list of figures', 'lijst van figuren']\n",
    "    Text = [t for t in Text if not any(figure_phrase in t.lower() for figure_phrase in figure_phrases)]\n",
    "\n",
    "    # ignore list of abbreviations\n",
    "    abbreviation_phrases = ['list of abbreviations', 'lijst van afkortingen']\n",
    "    Text = [t for t in Text if not any(abbreviation_phrase in t.lower() for abbreviation_phrase in abbreviation_phrases)]\n",
    "\n",
    "    # ignore copyright page\n",
    "    copyright_phrases = ['all rights reserved', 'no part of this publication may be reproduced', 'copyright', 'uitgeverij']\n",
    "    Text = [t for t in Text if not any(copyright_phrase in t.lower() for copyright_phrase in copyright_phrases)]\n",
    "\n",
    "    phd_phrases = ['volgens besluit van het college voor promoties', 'de graad van doctor aan']\n",
    "    Text = [t for t in Text if not any(phd_phrase in t.lower() for phd_phrase in phd_phrases)]\n",
    "\n",
    "    # ignore table of contents\n",
    "    toc_phrases = ['inhoudsopgave', 'table of contents']\n",
    "    Text = [t for t in Text if not any(toc_phrase in t.lower() for toc_phrase in toc_phrases)]\n",
    "    # ignore if multiple sentences in a page start with \"chapter \\d\"\n",
    "    chapter_phrases = ['chapter ', 'hoofdstuk ']\n",
    "    Text = [t for t in Text if sum(t.lower().count(chapter_phrase) for chapter_phrase in chapter_phrases)<2]\n",
    "\n",
    "    # ignore acknowledgements\n",
    "    acknowledgement_phrases = ['acknowledgements', 'acknowledgements', 'dankwoord', 'dankbetuiging']\n",
    "    Text = [t for t in Text if not any(acknowledgement_phrase in t.lower() for acknowledgement_phrase in acknowledgement_phrases)]\n",
    "\n",
    "    # ignore list of publications\n",
    "    publication_phrases = ['list of publications', 'lijst van publicaties', 'bibliography', 'bibliografie']\n",
    "    Text = [t for t in Text if not any(publication_phrase in t.lower() for publication_phrase in publication_phrases)]\n",
    "\n",
    "\n",
    "    Text = [t for t in Text if len(t.split())>25]\n",
    "\n",
    "    TextNumWords = [len(t.split()) for t in Text]\n",
    "    \n",
    "\n",
    "    return Text, TextNumWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "\n",
    "ListOfTexts = []    \n",
    "ListOfSummaries = []\n",
    "ListOfNumWordLists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _File in tqdm(Files[len(ListOfNumWordLists):]):\n",
    "    _path = os.path.join(pdf_path, _File)\n",
    "    try:\n",
    "        Text, error = pdf_to_text(_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {_File}: {e}\")\n",
    "        continue\n",
    "    SummaryEnglish, SummaryDutch = extract_summary(Text)\n",
    "    CleanedText, NumWordList = extractor(Text, min_words=25)\n",
    "\n",
    "    for k, page in enumerate(CleanedText):\n",
    "        ListOfTexts.append({\n",
    "            'institute': institute,\n",
    "            'file': _File,\n",
    "            'pseudo_pagenum': k,\n",
    "            'text': page\n",
    "        })\n",
    "    ListOfSummaries.append({\n",
    "        'institute': institute,\n",
    "        'file': _File,\n",
    "        'summary_english': SummaryEnglish,\n",
    "        'summary_dutch': SummaryDutch\n",
    "    })\n",
    "    ListOfNumWordLists.append({\n",
    "        'institute': institute,\n",
    "        'file': _File,\n",
    "        'num_words_per_page': NumWordList\n",
    "    })          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55168759"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([_r for r in ListOfNumWordLists for _r in r['num_words_per_page']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 2219 theses, 1153 have summaries and 1066 do not have summaries.\n"
     ]
    }
   ],
   "source": [
    "ListOfSummariesWith = [d for d in ListOfSummaries if (d['summary_english']!='') or (d['summary_dutch']!='')]\n",
    "with_sum = len(ListOfSummariesWith)\n",
    "ListOfSummariesWithout = [d for d in ListOfSummaries if (d['summary_english']=='') and (d['summary_dutch']=='')]\n",
    "without_sum = len(ListOfSummariesWithout)\n",
    "print(f'Out of {len(ListOfSummaries)} theses, {with_sum} have summaries and {without_sum} do not have summaries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PRETRAINING/PhDTheses/{institute}_texts.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in ListOfTexts:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same for the summaries\n",
    "with open(f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PRETRAINING/PhDTheses/{institute}_summaries.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in ListOfSummariesWith:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubscience-GFzZkKDp-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
