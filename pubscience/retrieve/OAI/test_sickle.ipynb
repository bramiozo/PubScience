{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import lxml\n",
    "import bs4\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import oai\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'UVA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_topiclist = ['athero', \n",
    "                        'plaque', \n",
    "                        'cardiovascular',\n",
    "                        'cardiogram', \n",
    "                        'cardiology', \n",
    "                        'cardiologie',\n",
    "                        'hartvaten',\n",
    "                        'klinsch',\n",
    "                        'medische',\n",
    "                        'hartvaat',\n",
    "                        'heart', \n",
    "                        'vascular',\n",
    "                        'angiogram', \n",
    "                        'cardiologie', \n",
    "                        'hartziekte', \n",
    "                        'vaatziekte',\n",
    "                        'medicine',\n",
    "                        'disease', \n",
    "                        'medical', \n",
    "                        'therapy',\n",
    "                        'therapeutic',\n",
    "                        'diagnosic',\n",
    "                        'clinical',\n",
    "                        'surgical', \n",
    "                        'metabolic',\n",
    "                        'myocard']\n",
    "\n",
    "accepted_topiclist = ['recht', 'wetten', 'juridisch',\n",
    "                        'rechtspraak', 'verordering', 'wetgeving',\n",
    "                        'richtlijn',\n",
    "                        'regelgeving',\n",
    "                        'wetboek',\n",
    "                        'wetboek', 'jurisprudentie', \n",
    "                        'precedent', 'wet bibop',\n",
    "                        'wetboek van strafrecht',\n",
    "                        'wetboek van strafvordering']\n",
    "\n",
    "accepted_dtypes = ['doctoral', 'book', 'article']\n",
    "accepted_languages = ['nl','nld','dut'] \n",
    "\n",
    "\n",
    "base_url =   oai.sources[source]['link'] #'https://repository.ubn.ru.nl/oai/openaire'  https://scholarlypublications.universiteitleiden.nl/oai2, http://dspace.library.uu.nl/oai/dissertation\n",
    "pdf_path = f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses/{source}/'\n",
    "pdf_path = f'/media/koekiemonster/DATA-FAST/text_data/pubscience/PDF/LAW/RUG' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIRE_institutes = ['VU', 'UVA', 'Maastricht',\n",
    "                       'Tilburg', 'RUG', 'UTwente', \n",
    "                       'TUE', 'UU', 'Erasmus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: publications:withFiles contains keyword\n"
     ]
    }
   ],
   "source": [
    "keywords = ['clinical', 'medisch', 'medical', 'dissertation', 'umc', 'medicine',\n",
    "            'diss', 'phd', 'thesis', 'doctorate', 'dissertatie',\n",
    "            'doctoraat', 'proefschrift']\n",
    "if source in OpenAIRE_institutes:\n",
    "    keywords = keywords + ['publications:withfiles']\n",
    "\n",
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    #print(key,val)\n",
    "    if any([c in val.lower() for c in keywords]) | any([c in key.lower() for c in keywords]):\n",
    "        print(f'Set: {key} contains keyword')\n",
    "        Sets_to_mine.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining from set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62283it [30:06, 34.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# get records \n",
    "# Beware: this takes a long time.\n",
    "from collections import defaultdict\n",
    "records_lists = defaultdict(list)\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    #if set_to_mine in ['com_1874_298213']:\n",
    "    #    continue\n",
    "    print(f\"Mining from set: {set_to_mine}\")\n",
    "    try:\n",
    "        records = sickler.ListRecords(metadataPrefix='oai_dc', \n",
    "                                    ignore_deleted=True, \n",
    "                                    set=set_to_mine) # dissertation com_1874_298213\n",
    "        for record in tqdm(records):\n",
    "            records_lists[set_to_mine].append(record)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['D4.4 Prototype of the system for enhanced services recommendation'],\n",
       " 'creator': ['Sargolzaei, M.', 'Shafahi, M.', 'Afsarmanesh, H.'],\n",
       " 'description': ['This deliverable addresses the prototypes of two tools that are implemented as a part of the PSS sub-system in the GloNet system, namely the Service Specification Tool (SST) and Product/Service Discovery and Recommendation (PSDR) tool. The designs of these tools are represented in the deliverable D4.3. These two tools, as well as the Product Specification Tool (PST) represented in D4.1 and D4.2, are well integrated as PSS (Product and Service Specification), supporting proper specification of different aspects of the complex products. The PSS sub-system provides the set of needed mechanisms for specification, registration, discovery, and recommended ranking of sub-products and business services, as well as the composition of business services and assisting with the enhancement for products with business services. As such, this deliverable addresses the implementation aspects related to the service-enhanced product specification and recommendation, while the complete design of the functionalities for this sub-system is provided in deliverable D4.3. Moreover, a set of examples of using the SST and PSDR tools, with some screenshots from the PSS are represented.'],\n",
       " 'date': ['2014-09'],\n",
       " 'type': ['book'],\n",
       " 'format': ['application/pdf'],\n",
       " 'identifier': ['https://dare.uva.nl/personal/pure/en/publications/d44-prototype-of-the-system-for-enhanced-services-recommendation(39be0f8c-24d1-4547-a56f-fa22e322f663).html',\n",
       "  'https://doi.org/10.13140/RG.2.1.3697.4242',\n",
       "  '11245/1.447131',\n",
       "  'https://pure.uva.nl/ws/files/47020744/710492.pdf',\n",
       "  'https://cordis.europa.eu/project/id/285273'],\n",
       " 'source': ['Sargolzaei , M , Shafahi , M & Afsarmanesh , H 2014 , D4.4 Prototype of the system for enhanced services recommendation . Karlsruhe . https://doi.org/10.13140/RG.2.1.3697.4242'],\n",
       " 'language': ['eng'],\n",
       " 'rights': ['info:eu-repo/semantics/openAccess']}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_lists['publications:withFiles'][0].get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADD SEMANTIC SEARCH FOR LEGAL TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62283/62283 [00:09<00:00, 6424.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1189 relevant records in set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_records_lists = defaultdict(list)\n",
    "excluded_records_lists = defaultdict(list)\n",
    "\n",
    "cond_list = []\n",
    "\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    relevant_counter = 0\n",
    "    for r in tqdm(records_lists[set_to_mine]):\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        TOPIC = False\n",
    "        PDF = False\n",
    "        DOCTORATE = True if 'dissertation' in set_to_mine.lower() else False\n",
    "        EMBARGO = False\n",
    "        LANG = False\n",
    "        \n",
    "        try:\n",
    "            lang = meta['language'][0]\n",
    "        except KeyError:\n",
    "            lang = 'en'\n",
    "            \n",
    "        if lang in accepted_languages:\n",
    "            LANG = True\n",
    "        \n",
    "\n",
    "        if source in ['Radboud']: \n",
    "            PDF=True\n",
    "        \n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['subject'] for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['title'] for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['description'] for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass                \n",
    "            \n",
    "        try:\n",
    "            if ('pdf' in meta['format'][0].lower()):\n",
    "                PDF = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if any([t in meta['type'][0].lower() for t in accepted_dtypes]):\n",
    "                DOCTORATE = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if ('embargo' in meta['rights'][0].lower()) |\\\n",
    "                    ('restricted' in meta['rights'][0].lower()):\n",
    "                EMBARGO = True\n",
    "        except:\n",
    "            pass  \n",
    "        \n",
    "        cond_list.append({'Topic': TOPIC, \n",
    "                          'PDF': PDF, \n",
    "                          'Doctorate': DOCTORATE, \n",
    "                          'Embargo': EMBARGO,\n",
    "                          'LANG': LANG}\n",
    "                         )\n",
    "        \n",
    "        if TOPIC & PDF & DOCTORATE & LANG & ~EMBARGO:\n",
    "            relevant_counter += 1\n",
    "            filtered_records_lists[set_to_mine].append(r)\n",
    "        else:\n",
    "            excluded_records_lists[set_to_mine].append(r)\n",
    "\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_counts = defaultdict(int)\n",
    "for r in excluded_records_lists['publications:withFiles']:\n",
    "    try:\n",
    "        types_counts[r.metadata['type'][0]] += 1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'book': 3120,\n",
       "             'article': 38087,\n",
       "             'bookPart': 3904,\n",
       "             'doctoralThesis': 10366,\n",
       "             'contributionToPeriodical': 3263,\n",
       "             'workingPaper': 1314,\n",
       "             'other': 895,\n",
       "             'conferenceObject': 145})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_df = pd.DataFrame(cond_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic         2015\n",
       "PDF          60604\n",
       "Doctorate    56666\n",
       "Embargo      10549\n",
       "LANG          6897\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conds_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of theses with PDF: 55094\n"
     ]
    }
   ],
   "source": [
    "tot_theses_with_pdf = (conds_df.Doctorate & conds_df.PDF).sum()\n",
    "print(f\"Total number of theses with PDF: {tot_theses_with_pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1189 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1189/1189 [00:00<00:00, 12698.17it/s]\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "\n",
    "for set_to_mine in filtered_records_lists.keys():\n",
    "    for record in tqdm(filtered_records_lists[set_to_mine]):\n",
    "        META = record.get_metadata()\n",
    "        try:\n",
    "            List_of_identifiers = META['identifier'] if 'identifier' in META.keys() else ['']\n",
    "            Title = META['title'] if 'title' in META.keys() else ['']\n",
    "            Description = META['description'] if 'description' in META.keys() else ['']\n",
    "            Date = META['date'] if 'date' in META.keys() else ['']\n",
    "            Language = META['language'] if 'language' in META.keys() else ['']       \n",
    "            Creator = META['creator'] if 'creator' in META.keys() else ['']\n",
    "            \n",
    "            \n",
    "            List_of_identifiers = [id for id in List_of_identifiers if id is not None]\n",
    "            \n",
    "            if len(List_of_identifiers)>0:\n",
    "                link_list.append({'Set': set_to_mine, \n",
    "                                'Link': List_of_identifiers, \n",
    "                                'Title': Title,\n",
    "                                'Description': Description,\n",
    "                                'Date': Date ,\n",
    "                                'Language': Language,\n",
    "                                'Creator': Creator,\n",
    "                                'Publisher': META.get('publisher'),\n",
    "                                }\n",
    "                                )\n",
    "            else:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            error_list.append(f\"Error parsing {e}: {META}: \")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {None: 1178})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_counts = defaultdict(int)\n",
    "for r in link_list:\n",
    "    try:\n",
    "        publisher_counts[r['Publisher']] += 1\n",
    "    except:\n",
    "        pass\n",
    "publisher_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Set': 'publications:withFiles',\n",
       " 'Link': ['https://dare.uva.nl/personal/pure/en/publications/billijke-vergoeding-in-recht-en-economie(1a2cc12d-81f9-4783-92bc-910f1837ec5a).html',\n",
       "  '11245/1.501089',\n",
       "  'https://pure.uva.nl/ws/files/2682758/170178_2015_Billijke_vergoeding_in_recht_en_economie_Poort_AMI_6_2015_p_1_5.pdf'],\n",
       " 'Title': ['Billijke vergoeding in recht en economie'],\n",
       " 'Description': [\"Op 2 oktober 2013 wees de Rechtbank Den Haag vonnis in de zaak van SENA tegen 25 organisatoren van 'dance events' over de billijke vergoeding die zij aan SENA verschuldigd zijn voor het draaien van muziekopnames. Hoger beroep dient bij het Haagse gerechtshof, dat de zaak naar verwachting voor advies zal verwijzen naar de Geschillencommissie Auteursrecht. Niet eerder liet een Nederlandse rechter zich expliciet uit over de hoogte van de billijke vergoeding en nu de billijke vergoeding voor auteurs ook in het nieuwe auteurscontractenrecht een centrale rol speelt, rijst de vraag hoe zo'n vergoeding kan worden bepaald. Economen kunnen wel wat zeggen over welvaartseffecten van vergoedingen, maar rekenen vragen over billijkheid tot het domein van politiek en recht. ook in het recht blijken echter nauwelijks aanknopingspunten te zijn om dit concept concreet te maken.\"],\n",
       " 'Date': ['2015'],\n",
       " 'Language': ['nld'],\n",
       " 'Creator': ['Poort, J.P.'],\n",
       " 'Publisher': None}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs_Leiden(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        # random number between 0.5 and 2.5 seconds\n",
    "        rndSleep = round(random.uniform(1, 5), 2)\n",
    "        sleep(rndSleep)\n",
    "        soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "        found, dsfound, esfound = False, False, False\n",
    "        for _res in soup.findAll('li', {'class':'ubl-file-view'}):\n",
    "            if _res.a is not None:\n",
    "                if _res.a.contents[0].strip().lower() == 'full text':\n",
    "                    _pdfdir = _res.a['href']\n",
    "                    found = True\n",
    "                elif _res.a.contents[0].strip().lower() == 'summary in dutch':\n",
    "                    _dutch_summary = _res.a['href']\n",
    "                    dsfound = True\n",
    "                elif _res.a.contents[0].strip().lower() == 'summary in english':\n",
    "                    _english_summary = _res.a['href']\n",
    "                    esfound = True\n",
    "\n",
    "        linkPdfAlt = f\"https://scholarlypublications.universiteitleiden.nl{_pdfdir}\" if found else None\n",
    "        DutchSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_dutch_summary}\" if dsfound else None\n",
    "        EnglishSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_english_summary}\" if esfound else None\n",
    "        \n",
    "        return linkPdfAlt, DutchSummaryLink, EnglishSummaryLink, r.status_code\n",
    "    except Exception as e:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_links(links, institute=None):\n",
    "    pdf_links = []\n",
    "    if institute in ['VU', 'UVA', 'UTwente']:\n",
    "        inclusion_terms = [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in ['Radboud']:\n",
    "        inclusion_terms = [r'handle', r'bitstream']\n",
    "    elif institute in ['Maastricht']:\n",
    "        inclusion_terms = [r'ASSET1', r'c[0-9]{3,4}\\.pdf']\n",
    "    elif institute in ['Tilburg']:\n",
    "        inclusion_terms = [r'\\.pdf']\n",
    "    elif institute in ['RUG']:\n",
    "        inclusion_terms = [r'summ\\.pdf', r'summary\\.pdf',  r'samenv\\.pdf', r'samenvat\\.pdf', r'[ch][0-9]{1,2}\\.pdf', \n",
    "                           r'thesis\\.pdf', r'proefschrift\\.pdf', r'dissertation\\.pdf', r'dissertatie\\.pdf']\n",
    "    elif institute in ['TUE']:\n",
    "        inclusion_terms = [r'summ\\.pdf', r'summary\\.pdf',  r'samenv\\.pdf', r'samenvat\\.pdf', r'[ch][0-9]{1,2}\\.pdf', \n",
    "                           r'thesis\\.pdf', r'proefschrift\\.pdf', r'dissertation\\.pdf', r'dissertatie\\.pdf']\n",
    "        inclusion_terms = inclusion_terms + [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in ['UU']:\n",
    "        inclusion_terms = [r'dspace\\.library\\.uu\\.nl']\n",
    "    elif institute in ['Leiden']:\n",
    "        inclusion_terms = [r'handle']\n",
    "    elif institute in ['Erasmus']:\n",
    "        inclusion_terms = [r'files']\n",
    "    else:\n",
    "        raise ValueError(f'Institute {institute} not recognized')\n",
    "    \n",
    "    inclusion_terms = [re.compile(rs) for rs in inclusion_terms]\n",
    "    \n",
    "    if institute in ['UU']:\n",
    "        _pdf_links = []\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')):\n",
    "                baselink = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "                _pdf_links.append(baselink + '/full.pdf')\n",
    "        pdf_links = _pdf_links\n",
    "    elif institute in ['Leiden']:\n",
    "        _pdf_links = []\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & (any([t.search(link) is not None for t in inclusion_terms])):\n",
    "                main, dutch_summ, engl_summ, return_code = get_pdfs_Leiden(link)\n",
    "                if return_code == 429:\n",
    "                    print(\"Too many requests. Waiting 60 seconds\")\n",
    "                    sleep(60)\n",
    "                else:\n",
    "                    _pdf_links.extend([main, dutch_summ, engl_summ])\n",
    "        pdf_links = [l for l in _pdf_links if l is not None]\n",
    "    else:\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & \\\n",
    "                    (link.lower().endswith('.pdf')):\n",
    "                if any([t.search(link) is not None for t in inclusion_terms]):\n",
    "                    pdf_links.append(link)        \n",
    "\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1189/1189 [00:00<00:00, 37966.89it/s]\n"
     ]
    }
   ],
   "source": [
    "link_pdf_list = []\n",
    "for l in tqdm(link_list):\n",
    "    links = extract_pdf_links(l['Link'], institute=source)\n",
    "    tmp = []\n",
    "    for _l in links:\n",
    "        Creator = l['Creator'][0] if l['Creator'][0] is not None else 'Unknown'\n",
    "        Date = l['Date'][0] if l['Date'][0] is not None else 'Unknown'\n",
    "        \n",
    "        pdfPath = _l.split(\"/\")[-1].replace(\"%20\", \"_\").rstrip('.pdf')\n",
    "        pdfPath = source + \"_\" + Creator + \"_\" + Date + \"_\" + pdfPath\n",
    "        pdfPath = pdfPath.replace(\",\", \"\")\n",
    "        pdfPath = pdfPath.replace(\":\", \"\")\n",
    "        pdfPath = pdfPath.replace(\".\", \"\")\n",
    "        pdfPath = pdfPath.replace(\" \", \"\")\n",
    "        pdfPath = os.path.join(pdf_path, pdfPath+\".pdf\")\n",
    "        tmp.append((_l, pdfPath))\n",
    "    if len(tmp)==0:\n",
    "        continue\n",
    "    l['pdf_links'] = tmp\n",
    "    link_pdf_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_pdf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries, based on the title\n",
    "title_set = set()\n",
    "unique_link_list = []\n",
    "for el in link_pdf_list:\n",
    "    try:\n",
    "        title = el['Title'][0]\n",
    "        if title not in title_set:\n",
    "            unique_link_list.append(el)\n",
    "            title_set.add(title)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
    "\n",
    "# add sleep\n",
    "def pdf_writer(pdf_url, _pdf_path):\n",
    "    r = requests.get(pdf_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for i,chunk in enumerate(r.iter_content(chunk_size=1024)): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return True, 200\n",
    "    else:\n",
    "        return False, r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [07:43<00:00,  8.91s/it]\n"
     ]
    }
   ],
   "source": [
    "if source=='Radboud':\n",
    "    filelist = os.listdir(pdf_path)\n",
    "    pre_list = [f.split(\"_\")[-1] for f in filelist]\n",
    "if source=='UU':\n",
    "    filelist = os.listdir(pdf_path)\n",
    "    pre_list = [f.split(\".\")[-2] for f in filelist]\n",
    "\n",
    "pdf_error_list = []\n",
    "skipped_list = []\n",
    "src_list = []\n",
    "rcode_list = []\n",
    "success_list = []\n",
    "extract_full_text = True\n",
    "for link in tqdm(unique_link_list):\n",
    "    for lt in link['pdf_links']:\n",
    "        _pdf_path = lt[1]\n",
    "        pdf_url = lt[0]\n",
    "        return_code = None\n",
    "\n",
    "        if source=='Radboud':\n",
    "            if _pdf_path.split(\"_\")[-1] in pre_list:\n",
    "                skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "                continue\n",
    "        if source=='UU':\n",
    "            if link['pdf_links'][0][0].split(\"/\")[-2] in pre_list:\n",
    "                skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "                continue\n",
    "            \n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "        else:\n",
    "            # try to download the pdf\n",
    "            try:\n",
    "                write_file, return_code = pdf_writer(pdf_url, _pdf_path)\n",
    "                success_list.append(f'Pdf downloaded: {_pdf_path}')\n",
    "            except Exception as e:\n",
    "                pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')  \n",
    "\n",
    "            if return_code == 429:\n",
    "                print(f'Rate limit exceeded...sleeping 30 seconds')\n",
    "                sleep(30)\n",
    "                continue\n",
    "            else:\n",
    "                sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 errors while downloading pdfs\n",
      "Skipped 1 pdfs because they already existed\n"
     ]
    }
   ],
   "source": [
    "print(f'Found {len(pdf_error_list)} errors while downloading pdfs')\n",
    "print(f'Skipped {len(skipped_list)} pdfs because they already existed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
