{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = ['cardiovascular', 'cardiogram', 'cardiology', 'heart', 'vascular']\n",
    "base_url = 'http://dspace.library.uu.nl/oai/dissertation'\n",
    "pdf_path = '//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: dissertation contains diss\n",
      "Set: com_1874_298213 contains umcu\n",
      "Set: col_1874_298214 contains umcu\n"
     ]
    }
   ],
   "source": [
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    if 'umc' in val.lower():\n",
    "        print(f'Set: {key} contains umcu')\n",
    "        Sets_to_mine.append(key)\n",
    "    elif 'diss' in val.lower():\n",
    "        print(f'Set: {key} contains diss')\n",
    "        Sets_to_mine.append(key)\n",
    "    elif 'phd' in val.lower():\n",
    "        print(f'Set: {key} contains phd') \n",
    "        Sets_to_mine.append(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10398it [01:52, 92.58it/s]0<?, ?it/s]\n",
      " 33%|███▎      | 1/3 [01:52<03:44, 112.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 993 relevant records in set: dissertation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3463it [00:28, 122.49it/s]\n",
      " 67%|██████▋   | 2/3 [02:20<01:02, 62.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 796 relevant records in set: com_1874_298213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3463it [00:28, 123.23it/s]\n",
      "100%|██████████| 3/3 [02:49<00:00, 56.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 796 relevant records in set: col_1874_298214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "for set_to_mine in tqdm(Sets_to_mine):\n",
    "    records = sickler.ListRecords(metadataPrefix='oai_dc', ignore_deleted=True, set=set_to_mine) # dissertation com_1874_298213\n",
    "\n",
    "    relevant_counter = 0\n",
    "    for r in tqdm(records):\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        try:\n",
    "            if any([t in subj for subj in meta['subject'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            if any([t in subj for subj in meta['description'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass          \n",
    "        \n",
    "        if relevant:\n",
    "            relevant_counter += 1\n",
    "            link = meta['identifier'][0]\n",
    "            linkPdf = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "            linkPdf = linkPdf + '/full.pdf'\n",
    "            try:\n",
    "                link_list.append({'Set':set_to_mine, \n",
    "                                'Link': link, \n",
    "                                'PdfLink': linkPdf,\n",
    "                                'Title': meta['title'][0],\n",
    "                                'Description': meta['description'][0],\n",
    "                                'Date': meta['date'][0],\n",
    "                                'Language': meta['language'][0],\n",
    "                                }\n",
    "                                )\n",
    "            except Exception as e:\n",
    "                link_list.append({'Set':set_to_mine, \n",
    "                                'Link': link, \n",
    "                                'PdfLink': linkPdf,\n",
    "                                'Title': \"whoopsie\",\n",
    "                                'Description': \"daisey\",\n",
    "                                'Date': \"in the present\",\n",
    "                                'Language': \"klingon?\",\n",
    "                                }\n",
    "                                )\n",
    "                error_list.append(f'Error: {e} for link: {link}')\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 138/2585 [04:43<1:11:30,  1.75s/it]"
     ]
    }
   ],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# add sleep\n",
    "from time import sleep\n",
    "\n",
    "pdf_error_list = []\n",
    "for link in tqdm(link_list):\n",
    "    pdf_url = link['PdfLink']\n",
    "    pdf_name = link['Link'].split('/')[-1] + '.pdf'\n",
    "    _pdf_path = os.path.join(pdf_path, pdf_name)\n",
    "    \n",
    "    # try to download the pdf\n",
    "    try:\n",
    "        r = requests.get(pdf_url, stream=True)\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "    except Exception as e:\n",
    "        pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')\n",
    "        \n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
