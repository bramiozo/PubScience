{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import lxml\n",
    "import bs4\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import oai\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'UVA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_topiclist = ['athero', \n",
    "                        'plaque', \n",
    "                        'cardiovascular',\n",
    "                        'cardiogram', \n",
    "                        'cardiology', \n",
    "                        'cardiologie',\n",
    "                        'hartvaten',\n",
    "                        'klinsch',\n",
    "                        'medische',\n",
    "                        'hartvaat',\n",
    "                        'heart', \n",
    "                        'vascular',\n",
    "                        'angiogram', \n",
    "                        'cardiologie', \n",
    "                        'hartziekte', \n",
    "                        'vaatziekte',\n",
    "                        'medicine',\n",
    "                        'disease', \n",
    "                        'medical', \n",
    "                        'therapy',\n",
    "                        'therapeutic',\n",
    "                        'diagnosic',\n",
    "                        'clinical',\n",
    "                        'surgical', \n",
    "                        'metabolic',\n",
    "                        'myocard']\n",
    "\n",
    "accepted_topiclist = ['recht', 'wetten', 'juridisch',\n",
    "                        'rechtspraak', 'verordering', 'wetgeving',\n",
    "                        'richtlijn',\n",
    "                        'regelgeving',\n",
    "                        'wetboek',\n",
    "                        'wetboek', 'jurisprudentie', \n",
    "                        'precedent', 'wet bibop',\n",
    "                        'wetboek van strafrecht',\n",
    "                        'wetboek van strafvordering']\n",
    "\n",
    "accepted_dtypes = ['doctoral', 'book', 'article']\n",
    "accepted_languages = ['nl','nld','dut', 'und'] \n",
    "\n",
    "\n",
    "base_url =   oai.sources[source]['link'] #'https://repository.ubn.ru.nl/oai/openaire'  https://scholarlypublications.universiteitleiden.nl/oai2, http://dspace.library.uu.nl/oai/dissertation\n",
    "pdf_path = f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses/{source}/'\n",
    "pdf_path = f'/media/koekiemonster/DATA-FAST/text_data/pubscience/PDF/LAW/RUG' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIRE_institutes = ['VU', 'UVA', 'Maastricht',\n",
    "                       'Tilburg', 'RUG', 'UTwente', \n",
    "                       'TUE', 'UU', 'Erasmus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: publications:withFiles contains keyword\n"
     ]
    }
   ],
   "source": [
    "keywords = ['clinical', 'medisch', 'medical', 'dissertation', 'umc', 'medicine',\n",
    "            'diss', 'phd', 'thesis', 'doctorate', 'dissertatie',\n",
    "            'doctoraat', 'proefschrift']\n",
    "if source in OpenAIRE_institutes:\n",
    "    keywords = keywords + ['publications:withfiles']\n",
    "\n",
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    #print(key,val)\n",
    "    if any([c in val.lower() for c in keywords]) | any([c in key.lower() for c in keywords]):\n",
    "        print(f'Set: {key} contains keyword')\n",
    "        Sets_to_mine.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining from set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62283it [30:06, 34.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# get records \n",
    "# Beware: this takes a long time.\n",
    "from collections import defaultdict\n",
    "records_lists = defaultdict(list)\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    #if set_to_mine in ['com_1874_298213']:\n",
    "    #    continue\n",
    "    print(f\"Mining from set: {set_to_mine}\")\n",
    "    try:\n",
    "        records = sickler.ListRecords(metadataPrefix='oai_dc', \n",
    "                                    ignore_deleted=True, \n",
    "                                    set=set_to_mine) # dissertation com_1874_298213\n",
    "        for record in tqdm(records):\n",
    "            records_lists[set_to_mine].append(record)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['D4.4 Prototype of the system for enhanced services recommendation'],\n",
       " 'creator': ['Sargolzaei, M.', 'Shafahi, M.', 'Afsarmanesh, H.'],\n",
       " 'description': ['This deliverable addresses the prototypes of two tools that are implemented as a part of the PSS sub-system in the GloNet system, namely the Service Specification Tool (SST) and Product/Service Discovery and Recommendation (PSDR) tool. The designs of these tools are represented in the deliverable D4.3. These two tools, as well as the Product Specification Tool (PST) represented in D4.1 and D4.2, are well integrated as PSS (Product and Service Specification), supporting proper specification of different aspects of the complex products. The PSS sub-system provides the set of needed mechanisms for specification, registration, discovery, and recommended ranking of sub-products and business services, as well as the composition of business services and assisting with the enhancement for products with business services. As such, this deliverable addresses the implementation aspects related to the service-enhanced product specification and recommendation, while the complete design of the functionalities for this sub-system is provided in deliverable D4.3. Moreover, a set of examples of using the SST and PSDR tools, with some screenshots from the PSS are represented.'],\n",
       " 'date': ['2014-09'],\n",
       " 'type': ['book'],\n",
       " 'format': ['application/pdf'],\n",
       " 'identifier': ['https://dare.uva.nl/personal/pure/en/publications/d44-prototype-of-the-system-for-enhanced-services-recommendation(39be0f8c-24d1-4547-a56f-fa22e322f663).html',\n",
       "  'https://doi.org/10.13140/RG.2.1.3697.4242',\n",
       "  '11245/1.447131',\n",
       "  'https://pure.uva.nl/ws/files/47020744/710492.pdf',\n",
       "  'https://cordis.europa.eu/project/id/285273'],\n",
       " 'source': ['Sargolzaei , M , Shafahi , M & Afsarmanesh , H 2014 , D4.4 Prototype of the system for enhanced services recommendation . Karlsruhe . https://doi.org/10.13140/RG.2.1.3697.4242'],\n",
       " 'language': ['eng'],\n",
       " 'rights': ['info:eu-repo/semantics/openAccess']}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_lists['publications:withFiles'][0].get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADD SEMANTIC SEARCH FOR LEGAL TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62283/62283 [00:09<00:00, 6536.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 relevant records in set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_records_lists = defaultdict(list)\n",
    "excluded_records_lists = defaultdict(list)\n",
    "\n",
    "cond_list = []\n",
    "\n",
    "RAW_MINE_RESULTS = []\n",
    "\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    relevant_counter = 0\n",
    "    for r in tqdm(records_lists[set_to_mine]):\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        TOPIC = False\n",
    "        PDF = False\n",
    "        DOCTORATE = True if 'dissertation' in set_to_mine.lower() else False\n",
    "        EMBARGO = False\n",
    "        LANG = False\n",
    "        \n",
    "        try:\n",
    "            _lang = meta['language'][0]\n",
    "        except KeyError:\n",
    "            _lang = 'unknown'\n",
    "            \n",
    "        if _lang in accepted_languages:\n",
    "            LANG = True\n",
    "        \n",
    "\n",
    "        if source in ['Radboud']: \n",
    "            PDF=True\n",
    "        \n",
    "        try:\n",
    "            _subj = \",\".join(meta['subject'])\n",
    "            if any([t in _subj.lower() for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            _subj = \"\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            _title = meta['title']\n",
    "            if any([t in subj.lower() for subj in _title for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            _title = \"\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            _description = meta['description']\n",
    "            if any([t in subj.lower() for subj in _description for t in accepted_topiclist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            _description = \"\"\n",
    "            pass                        \n",
    "        \n",
    "        try:\n",
    "            _format = meta['format'][0].lower()\n",
    "            if ('pdf' in _format):\n",
    "                PDF = True\n",
    "        except:\n",
    "            _format = \"\"\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            _type = meta['type'][0].lower()\n",
    "            if any([t in +_type for t in accepted_dtypes]):\n",
    "                DOCTORATE = True\n",
    "        except:\n",
    "            _type = \"\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            _rights = meta['rights'][0].lower()\n",
    "            if ('embargo' in _rights) |\\\n",
    "                    ('restricted' in _rights):\n",
    "                EMBARGO = True\n",
    "        except:\n",
    "            _rights = \"\"\n",
    "            pass  \n",
    "        \n",
    "        try:\n",
    "            _date = meta['date'][0].lower()\n",
    "        except:\n",
    "            _date = \"\"\n",
    "            pass\n",
    "        \n",
    "        RAW_MINE_RESULTS.append(\n",
    "                {\n",
    "                'LANG': _lang,\n",
    "                'TOPIC': _subj,\n",
    "                'TITLE': _title,\n",
    "                'DESCRIPTION': _description,            \n",
    "                'FORMAT': _format,\n",
    "                'TYPE': _type,\n",
    "                'RIGHTS': _rights,\n",
    "                'DATE': _date,\n",
    "                'TOPIC_OF_INTEREST': TOPIC,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        cond_list.append({'Topic': TOPIC, \n",
    "                          'PDF': PDF, \n",
    "                          'Doctorate': DOCTORATE, \n",
    "                          'Embargo': EMBARGO,\n",
    "                          'LANG': LANG}\n",
    "                         )\n",
    "        \n",
    "        if TOPIC & PDF & DOCTORATE & LANG & ~EMBARGO:\n",
    "            relevant_counter += 1\n",
    "            filtered_records_lists[set_to_mine].append(r)\n",
    "        else:\n",
    "            excluded_records_lists[set_to_mine].append(r)\n",
    "\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of torch._utils failed: Traceback (most recent call last):\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/lib/python3.10/importlib/__init__.py\", line 159, in reload\n",
      "    raise ImportError(msg.format(parent_name),\n",
      "ImportError: parent 'torch' not in sys.modules\n",
      "]\n",
      "[autoreload of torch._utils_internal failed: Traceback (most recent call last):\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/lib/python3.10/importlib/__init__.py\", line 159, in reload\n",
      "    raise ImportError(msg.format(parent_name),\n",
      "ImportError: parent 'torch' not in sys.modules\n",
      "]\n",
      "[autoreload of torch.version failed: Traceback (most recent call last):\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/lib/python3.10/importlib/__init__.py\", line 159, in reload\n",
      "    raise ImportError(msg.format(parent_name),\n",
      "ImportError: parent 'torch' not in sys.modules\n",
      "]\n",
      "[autoreload of torch.torch_version failed: Traceback (most recent call last):\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/lib/python3.10/importlib/__init__.py\", line 159, in reload\n",
      "    raise ImportError(msg.format(parent_name),\n",
      "ImportError: parent 'torch' not in sys.modules\n",
      "]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.linalg' has no attribute '_umath_linalg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load SBERT model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      3\u001b[0m SBERT \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparaphrase-multilingual-MiniLM-L12-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# ('paraphrase-multilingual-mpnet-base-v2')\u001b[39;00m\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/sentence_transformers/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/sentence_transformers/datasets/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInputExample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtreebank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TreebankWordDetokenizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/nltk/__init__.py:133\u001b[0m\n\u001b[1;32m    125\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mPopen \u001b[38;5;241m=\u001b[39m _fake_Popen\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/nltk/collocations.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_itertools\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     38\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     39\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     40\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspearman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/nltk/metrics/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magreement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotationTask\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massociation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     20\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     21\u001b[0m     NgramAssocMeasures,\n\u001b[1;32m     22\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     23\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfusionmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     binary_distance,\n\u001b[1;32m     28\u001b[0m     custom_distance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     presence,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/nltk/metrics/association.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m _SMALL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-20\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fisher_exact\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfisher_exact\u001b[39m(\u001b[38;5;241m*\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs):\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/scipy/stats/__init__.py:608\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m \n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/scipy/stats/_stats_py.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/numpy/testing/__init__.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestCase\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extbuild\n",
      "File \u001b[0;32m/media/koekiemonster/home/bramiozo/.pyenv/versions/3.10.4/envs/nlp/lib/python3.10/site-packages/numpy/testing/_private/utils.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m IS_PYSTON \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(sys, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyston_version_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m HAS_REFCOUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(sys, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetrefcount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IS_PYSTON\n\u001b[0;32m---> 57\u001b[0m HAS_LAPACK64 \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241m.\u001b[39m_ilp64\n\u001b[1;32m     59\u001b[0m _OLD_PROMOTION \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: np\u001b[38;5;241m.\u001b[39m_get_promotion_state() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegacy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     61\u001b[0m IS_MUSL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.linalg' has no attribute '_umath_linalg'"
     ]
    }
   ],
   "source": [
    "# load SBERT model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "SBERT = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')# ('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eng', 43509),\n",
       " ('und', 10783),\n",
       " ('nld', 6897),\n",
       " ('deu', 284),\n",
       " ('fra', 191),\n",
       " ('spa', 191),\n",
       " ('ita', 98),\n",
       " ('rus', 77),\n",
       " ('dan', 45),\n",
       " ('por', 39)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lang_dict.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_counts = defaultdict(int)\n",
    "for r in excluded_records_lists['publications:withFiles']:\n",
    "    try:\n",
    "        types_counts[r.metadata['type'][0]] += 1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'book': 3120,\n",
       "             'article': 38087,\n",
       "             'bookPart': 3904,\n",
       "             'doctoralThesis': 10366,\n",
       "             'contributionToPeriodical': 3263,\n",
       "             'workingPaper': 1314,\n",
       "             'other': 895,\n",
       "             'conferenceObject': 145})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_df = pd.DataFrame(cond_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic         2015\n",
       "PDF          60604\n",
       "Doctorate    56666\n",
       "Embargo      10549\n",
       "LANG          6897\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conds_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of theses with PDF: 55094\n"
     ]
    }
   ],
   "source": [
    "tot_theses_with_pdf = (conds_df.Doctorate & conds_df.PDF).sum()\n",
    "print(f\"Total number of theses with PDF: {tot_theses_with_pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1189 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1189/1189 [00:00<00:00, 12698.17it/s]\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "\n",
    "for set_to_mine in filtered_records_lists.keys():\n",
    "    for record in tqdm(filtered_records_lists[set_to_mine]):\n",
    "        META = record.get_metadata()\n",
    "        try:\n",
    "            List_of_identifiers = META['identifier'] if 'identifier' in META.keys() else ['']\n",
    "            Title = META['title'] if 'title' in META.keys() else ['']\n",
    "            Description = META['description'] if 'description' in META.keys() else ['']\n",
    "            Date = META['date'] if 'date' in META.keys() else ['']\n",
    "            Language = META['language'] if 'language' in META.keys() else ['']       \n",
    "            Creator = META['creator'] if 'creator' in META.keys() else ['']\n",
    "            \n",
    "            \n",
    "            List_of_identifiers = [id for id in List_of_identifiers if id is not None]\n",
    "            \n",
    "            if len(List_of_identifiers)>0:\n",
    "                link_list.append({'Set': set_to_mine, \n",
    "                                'Link': List_of_identifiers, \n",
    "                                'Title': Title,\n",
    "                                'Description': Description,\n",
    "                                'Date': Date ,\n",
    "                                'Language': Language,\n",
    "                                'Creator': Creator,\n",
    "                                'Publisher': META.get('publisher'),\n",
    "                                }\n",
    "                                )\n",
    "            else:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            error_list.append(f\"Error parsing {e}: {META}: \")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {None: 1178})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_counts = defaultdict(int)\n",
    "for r in link_list:\n",
    "    try:\n",
    "        publisher_counts[r['Publisher']] += 1\n",
    "    except:\n",
    "        pass\n",
    "publisher_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Set': 'publications:withFiles',\n",
       " 'Link': ['https://dare.uva.nl/personal/pure/en/publications/billijke-vergoeding-in-recht-en-economie(1a2cc12d-81f9-4783-92bc-910f1837ec5a).html',\n",
       "  '11245/1.501089',\n",
       "  'https://pure.uva.nl/ws/files/2682758/170178_2015_Billijke_vergoeding_in_recht_en_economie_Poort_AMI_6_2015_p_1_5.pdf'],\n",
       " 'Title': ['Billijke vergoeding in recht en economie'],\n",
       " 'Description': [\"Op 2 oktober 2013 wees de Rechtbank Den Haag vonnis in de zaak van SENA tegen 25 organisatoren van 'dance events' over de billijke vergoeding die zij aan SENA verschuldigd zijn voor het draaien van muziekopnames. Hoger beroep dient bij het Haagse gerechtshof, dat de zaak naar verwachting voor advies zal verwijzen naar de Geschillencommissie Auteursrecht. Niet eerder liet een Nederlandse rechter zich expliciet uit over de hoogte van de billijke vergoeding en nu de billijke vergoeding voor auteurs ook in het nieuwe auteurscontractenrecht een centrale rol speelt, rijst de vraag hoe zo'n vergoeding kan worden bepaald. Economen kunnen wel wat zeggen over welvaartseffecten van vergoedingen, maar rekenen vragen over billijkheid tot het domein van politiek en recht. ook in het recht blijken echter nauwelijks aanknopingspunten te zijn om dit concept concreet te maken.\"],\n",
       " 'Date': ['2015'],\n",
       " 'Language': ['nld'],\n",
       " 'Creator': ['Poort, J.P.'],\n",
       " 'Publisher': None}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs_Leiden(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        # random number between 0.5 and 2.5 seconds\n",
    "        rndSleep = round(random.uniform(1, 5), 2)\n",
    "        sleep(rndSleep)\n",
    "        soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "        found, dsfound, esfound = False, False, False\n",
    "        for _res in soup.findAll('li', {'class':'ubl-file-view'}):\n",
    "            if _res.a is not None:\n",
    "                if _res.a.contents[0].strip().lower() == 'full text':\n",
    "                    _pdfdir = _res.a['href']\n",
    "                    found = True\n",
    "                elif _res.a.contents[0].strip().lower() == 'summary in dutch':\n",
    "                    _dutch_summary = _res.a['href']\n",
    "                    dsfound = True\n",
    "                elif _res.a.contents[0].strip().lower() == 'summary in english':\n",
    "                    _english_summary = _res.a['href']\n",
    "                    esfound = True\n",
    "\n",
    "        linkPdfAlt = f\"https://scholarlypublications.universiteitleiden.nl{_pdfdir}\" if found else None\n",
    "        DutchSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_dutch_summary}\" if dsfound else None\n",
    "        EnglishSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_english_summary}\" if esfound else None\n",
    "        \n",
    "        return linkPdfAlt, DutchSummaryLink, EnglishSummaryLink, r.status_code\n",
    "    except Exception as e:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_links(links, institute=None):\n",
    "    pdf_links = []\n",
    "    if institute in ['VU', 'UVA', 'UTwente']:\n",
    "        inclusion_terms = [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in ['Radboud']:\n",
    "        inclusion_terms = [r'handle', r'bitstream']\n",
    "    elif institute in ['Maastricht']:\n",
    "        inclusion_terms = [r'ASSET1', r'c[0-9]{3,4}\\.pdf']\n",
    "    elif institute in ['Tilburg']:\n",
    "        inclusion_terms = [r'\\.pdf']\n",
    "    elif institute in ['RUG']:\n",
    "        inclusion_terms = [r'summ\\.pdf', r'summary\\.pdf',  r'samenv\\.pdf', r'samenvat\\.pdf', r'[ch][0-9]{1,2}\\.pdf', \n",
    "                           r'thesis\\.pdf', r'proefschrift\\.pdf', r'dissertation\\.pdf', r'dissertatie\\.pdf']\n",
    "    elif institute in ['TUE']:\n",
    "        inclusion_terms = [r'summ\\.pdf', r'summary\\.pdf',  r'samenv\\.pdf', r'samenvat\\.pdf', r'[ch][0-9]{1,2}\\.pdf', \n",
    "                           r'thesis\\.pdf', r'proefschrift\\.pdf', r'dissertation\\.pdf', r'dissertatie\\.pdf']\n",
    "        inclusion_terms = inclusion_terms + [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in ['UU']:\n",
    "        inclusion_terms = [r'dspace\\.library\\.uu\\.nl']\n",
    "    elif institute in ['Leiden']:\n",
    "        inclusion_terms = [r'handle']\n",
    "    elif institute in ['Erasmus']:\n",
    "        inclusion_terms = [r'files']\n",
    "    else:\n",
    "        raise ValueError(f'Institute {institute} not recognized')\n",
    "    \n",
    "    inclusion_terms = [re.compile(rs) for rs in inclusion_terms]\n",
    "    \n",
    "    if institute in ['UU']:\n",
    "        _pdf_links = []\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')):\n",
    "                baselink = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "                _pdf_links.append(baselink + '/full.pdf')\n",
    "        pdf_links = _pdf_links\n",
    "    elif institute in ['Leiden']:\n",
    "        _pdf_links = []\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & (any([t.search(link) is not None for t in inclusion_terms])):\n",
    "                main, dutch_summ, engl_summ, return_code = get_pdfs_Leiden(link)\n",
    "                if return_code == 429:\n",
    "                    print(\"Too many requests. Waiting 60 seconds\")\n",
    "                    sleep(60)\n",
    "                else:\n",
    "                    _pdf_links.extend([main, dutch_summ, engl_summ])\n",
    "        pdf_links = [l for l in _pdf_links if l is not None]\n",
    "    else:\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & \\\n",
    "                    (link.lower().endswith('.pdf')):\n",
    "                if any([t.search(link) is not None for t in inclusion_terms]):\n",
    "                    pdf_links.append(link)        \n",
    "\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1189/1189 [00:00<00:00, 37966.89it/s]\n"
     ]
    }
   ],
   "source": [
    "link_pdf_list = []\n",
    "for l in tqdm(link_list):\n",
    "    links = extract_pdf_links(l['Link'], institute=source)\n",
    "    tmp = []\n",
    "    for _l in links:\n",
    "        Creator = l['Creator'][0] if l['Creator'][0] is not None else 'Unknown'\n",
    "        Date = l['Date'][0] if l['Date'][0] is not None else 'Unknown'\n",
    "        \n",
    "        pdfPath = _l.split(\"/\")[-1].replace(\"%20\", \"_\").rstrip('.pdf')\n",
    "        pdfPath = source + \"_\" + Creator + \"_\" + Date + \"_\" + pdfPath\n",
    "        pdfPath = pdfPath.replace(\",\", \"\")\n",
    "        pdfPath = pdfPath.replace(\":\", \"\")\n",
    "        pdfPath = pdfPath.replace(\".\", \"\")\n",
    "        pdfPath = pdfPath.replace(\" \", \"\")\n",
    "        pdfPath = os.path.join(pdf_path, pdfPath+\".pdf\")\n",
    "        tmp.append((_l, pdfPath))\n",
    "    if len(tmp)==0:\n",
    "        continue\n",
    "    l['pdf_links'] = tmp\n",
    "    link_pdf_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_pdf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries, based on the title\n",
    "title_set = set()\n",
    "unique_link_list = []\n",
    "for el in link_pdf_list:\n",
    "    try:\n",
    "        title = el['Title'][0]\n",
    "        if title not in title_set:\n",
    "            unique_link_list.append(el)\n",
    "            title_set.add(title)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
    "\n",
    "# add sleep\n",
    "def pdf_writer(pdf_url, _pdf_path):\n",
    "    r = requests.get(pdf_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for i,chunk in enumerate(r.iter_content(chunk_size=1024)): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return True, 200\n",
    "    else:\n",
    "        return False, r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [07:43<00:00,  8.91s/it]\n"
     ]
    }
   ],
   "source": [
    "if source=='Radboud':\n",
    "    filelist = os.listdir(pdf_path)\n",
    "    pre_list = [f.split(\"_\")[-1] for f in filelist]\n",
    "if source=='UU':\n",
    "    filelist = os.listdir(pdf_path)\n",
    "    pre_list = [f.split(\".\")[-2] for f in filelist]\n",
    "\n",
    "pdf_error_list = []\n",
    "skipped_list = []\n",
    "src_list = []\n",
    "rcode_list = []\n",
    "success_list = []\n",
    "extract_full_text = True\n",
    "for link in tqdm(unique_link_list):\n",
    "    for lt in link['pdf_links']:\n",
    "        _pdf_path = lt[1]\n",
    "        pdf_url = lt[0]\n",
    "        return_code = None\n",
    "\n",
    "        if source=='Radboud':\n",
    "            if _pdf_path.split(\"_\")[-1] in pre_list:\n",
    "                skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "                continue\n",
    "        if source=='UU':\n",
    "            if link['pdf_links'][0][0].split(\"/\")[-2] in pre_list:\n",
    "                skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "                continue\n",
    "            \n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "        else:\n",
    "            # try to download the pdf\n",
    "            try:\n",
    "                write_file, return_code = pdf_writer(pdf_url, _pdf_path)\n",
    "                success_list.append(f'Pdf downloaded: {_pdf_path}')\n",
    "            except Exception as e:\n",
    "                pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')  \n",
    "\n",
    "            if return_code == 429:\n",
    "                print(f'Rate limit exceeded...sleeping 30 seconds')\n",
    "                sleep(30)\n",
    "                continue\n",
    "            else:\n",
    "                sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 errors while downloading pdfs\n",
      "Skipped 1 pdfs because they already existed\n"
     ]
    }
   ],
   "source": [
    "print(f'Found {len(pdf_error_list)} errors while downloading pdfs')\n",
    "print(f'Skipped {len(skipped_list)} pdfs because they already existed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
