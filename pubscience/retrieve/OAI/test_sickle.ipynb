{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "import lxml\n",
    "import bs4\n",
    "import random\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = ['cardiovascular', 'cardiogram', 'cardiology', 'heart', 'vascular']\n",
    "institute = 'LUMC'\n",
    "base_url =  'https://oai.narcis.nl/oai2' # https://scholarlypublications.universiteitleiden.nl/oai2, http://dspace.library.uu.nl/oai/dissertation 'https://pure.uvt.nl/ws/oai?metadataPrefix=oai_dc'\n",
    "pdf_path = '//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    if any([c in val.lower() for c in ['diss', 'phd', 'thesis']]):\n",
    "        print(f'Set: {key} contains diss')\n",
    "        Sets_to_mine.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sets_to_mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get records \n",
    "from collections import defaultdict\n",
    "records_lists = defaultdict(list)\n",
    "for set_to_mine in tqdm(Sets_to_mine):\n",
    "    records = sickler.ListRecords(metadataPrefix='oai_dc', ignore_deleted=True, set=set_to_mine) # dissertation com_1874_298213\n",
    "    for record in records:\n",
    "        records_lists[set_to_mine].append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    relevant_counter = 0\n",
    "    for r in tqdm(records_lists[set_to_mine]):\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        try:\n",
    "            if any([t in subj for subj in meta['subject'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            if any([t in subj for subj in meta['description'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if 'embargo' in meta['rights'][0].lower():\n",
    "                relevant = False\n",
    "        except:\n",
    "            pass   \n",
    "        \n",
    "        if relevant:\n",
    "            relevant_counter += 1\n",
    "            if institute == 'LUMC':\n",
    "                try:\n",
    "                    link = meta['identifier'][-1] \n",
    "                    # identify first url in list\n",
    "                    found_link = False\n",
    "                    for l in meta['identifier']:\n",
    "                        if ('http:' in l) or ('https:' in l):\n",
    "                            link = l\n",
    "                            found_link = True\n",
    "                            break\n",
    "                    if not found_link:\n",
    "                        error_list.append(f'No link found for {meta[\"identifier\"]}')  \n",
    "\n",
    "                    doc_id = link.split('/')[-1]\n",
    "                    doc_id_int = int(doc_id)+2\n",
    "\n",
    "                    link = f\"https://scholarlypublications.universiteitleiden.nl/handle/1887/{doc_id}\"\n",
    "                    linkPdf = f\"https://scholarlypublications.universiteitleiden.nl/access/item%3A{doc_id_int}/download\"\n",
    "\n",
    "                    # extract through link url. The directory can be found in <li class='ubl-file-download'> <a href='...'>\n",
    "                    # only if <a href in ubl-file-view is \"full\"\n",
    "                    r = requests.get(link)\n",
    "                    # random number between 0.5 and 2.5 seconds\n",
    "                    rndSleep = round(random.uniform(0.5, 2.5), 2)\n",
    "                    sleep(rndSleep)\n",
    "                    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "                    found, dsfound, esfound = False, False, False\n",
    "                    for _res in soup.findAll('li', {'class':'ubl-file-view'}):\n",
    "                        if _res.a is not None:\n",
    "                            if _res.a.contents[0].strip().lower() == 'full text':\n",
    "                                _pdfdir = _res.a['href']\n",
    "                                found = True\n",
    "                            elif _res.a.contents[0].strip().lower() == 'summary in dutch':\n",
    "                                _dutch_summary = _res.a['href']\n",
    "                                dsfound = True\n",
    "                            elif _res.a.contents[0].strip().lower() == 'summary in english':\n",
    "                                _english_summary = _res.a['href']\n",
    "                                esfound = True\n",
    "\n",
    "                    linkPdfAlt = f\"https://scholarlypublications.universiteitleiden.nl{_pdfdir}\" if found else None\n",
    "                    DutchSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_dutch_summary}\" if dsfound else None\n",
    "                    EnglishSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_english_summary}\" if esfound else None\n",
    "                except Exception as e:\n",
    "                    error_list.append(f'Error: {e} for link: {link}, with meta data: {meta[\"identifier\"]}')\n",
    "                    pass\n",
    "                    #raise ValueError(f'Could not find pdf link for {link}, with error raised: {e}')\n",
    "            elif institute == 'UU':                \n",
    "                link = meta['identifier'][0]\n",
    "                baselink = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "                linkPdf = baselink + '/full.pdf'\n",
    "\n",
    "                try:\n",
    "                    linkPdfAlt = baselink +'/'+meta['creator'][0].split(',')[0].lower()+'.pdf'\n",
    "                except:\n",
    "                    linkPdfAlt = None\n",
    "                \n",
    "                DutchSummaryLink = None\n",
    "                EnglishSummaryLink = None\n",
    "\n",
    "            try:\n",
    "                link_list.append({'Set':set_to_mine, \n",
    "                                'Link': link, \n",
    "                                'PdfLink': linkPdf,\n",
    "                                'PdfLinkAlt': linkPdfAlt,\n",
    "                                'DutchSummaryLink': DutchSummaryLink,\n",
    "                                'EnglishSummaryLink': EnglishSummaryLink,\n",
    "                                'Title': meta['title'][0],\n",
    "                                'Description': meta['description'][0],\n",
    "                                'Date': meta['date'][0],\n",
    "                                'Language': meta['language'][0],\n",
    "                                }\n",
    "                                )\n",
    "            except Exception as e:\n",
    "                link_list.append({'Set':set_to_mine, \n",
    "                                'Link': link, \n",
    "                                'PdfLink': linkPdf,                                \n",
    "                                'PdfLinkAlt': linkPdfAlt,\n",
    "                                'DutchSummaryLink': None,\n",
    "                                'EnglishSummaryLink': None,\n",
    "                                'Title': None,\n",
    "                                'Description': None,\n",
    "                                'Date': None,\n",
    "                                'Language': None,\n",
    "                                }\n",
    "                                )\n",
    "                error_list.append(f'Error: {e} for link: {link}')\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries, based on the title\n",
    "title_set = set()\n",
    "unique_link_list = []\n",
    "for el in link_list:\n",
    "    try:\n",
    "        title = el['Title']\n",
    "        if title not in title_set:\n",
    "            unique_link_list.append(el)\n",
    "            title_set.add(title)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_link_list), len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
    "\n",
    "# add sleep\n",
    "\n",
    "def pdf_writer(pdf_url, _pdf_path):\n",
    "    r = requests.get(pdf_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for i,chunk in enumerate(r.iter_content(chunk_size=1024)): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return True\n",
    "    else:\n",
    "        return False \n",
    "\n",
    "pdf_error_list = []\n",
    "skipped_list = []\n",
    "src_list = []\n",
    "rcode_list = []\n",
    "extract_full_text = True\n",
    "for link in tqdm(unique_link_list):\n",
    "    # MAIN PDF\n",
    "    pdf_name = link['Link'].split('/')[-1] + '.pdf'\n",
    "    _pdf_path = os.path.join(pdf_path, pdf_name)\n",
    "        \n",
    "    if extract_full_text:\n",
    "        pdf_url = link['PdfLink']\n",
    "        # check if pdf already exists\n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "        else:\n",
    "            # try to download the pdf\n",
    "            try:\n",
    "                write_file = pdf_writer(pdf_url, _pdf_path)\n",
    "                src_list.append(f'Pdf downloaded from original link: {pdf_url}')            \n",
    "                \n",
    "                if not write_file:\n",
    "                    pdf_url = link['PdfLinkAlt']\n",
    "                    write_file = pdf_writer(pdf_url, _pdf_path)\n",
    "\n",
    "                    if write_file:                \n",
    "                        src_list.append(f'Pdf downloaded from alternative link: {pdf_url}')\n",
    "                    else:\n",
    "                        pdf_error_list.append(f'Error: No success for both : {link[\"PdfLinkAlt\"]} and {link[\"PdfLink\"]}')\n",
    "            \n",
    "            except Exception as e:\n",
    "                pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')  \n",
    "            sleep(1)\n",
    "\n",
    "    if link['DutchSummaryLink'] is not None:\n",
    "        pdf_url = link['DutchSummaryLink']\n",
    "        _pdf_path_ds = _pdf_path.replace('.pdf', '_dutch_summary.pdf')\n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path_ds}')\n",
    "        else:\n",
    "            write_file = pdf_writer(pdf_url, _pdf_path_ds)\n",
    "            \n",
    "            if not write_file:\n",
    "                pdf_error_list.append(f'Error: No success for dutch summary: {pdf_url}')\n",
    "\n",
    "    if link['EnglishSummaryLink'] is not None:\n",
    "        pdf_url = link['EnglishSummaryLink']\n",
    "        _pdf_path_es = _pdf_path.replace('.pdf', '_english_summary.pdf')\n",
    "        if os.path.isfile(_pdf_path_es):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path_es}')\n",
    "        else:\n",
    "            write_file = pdf_writer(pdf_url, _pdf_path_es)\n",
    "            if not write_file:\n",
    "                pdf_error_list.append(f'Error: No success for english summary: {pdf_url}')    \n",
    "    \n",
    "print(f'Found {len(pdf_error_list)} errors while downloading pdfs')\n",
    "print(f'Skipped {len(skipped_list)} pdfs because they already existed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract PDF text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pdf and extract text\n",
    "# We want to ignore all the decorum and only extract the text (i.e. no page number etc.)\n",
    "import pytesseract\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path, backend='pypdf'):\n",
    "    '''Extract text from pdf documents\n",
    "        Source: https://towardsdatascience.com/pdf-preprocessing-with-python-19829752af9f\n",
    "    '''\n",
    "\n",
    "    if backend=='pdfminer':\n",
    "        manager = PDFResourceManager()\n",
    "        retstr = StringIO()\n",
    "        layout = LAParams(all_texts=False, detect_vertical=True)\n",
    "        device = TextConverter(manager, retstr, laparams=layout)\n",
    "        interpreter = PDFPageInterpreter(manager, device)\n",
    "        with open(path, 'rb') as filepath:\n",
    "            for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "                interpreter.process_page(page)\n",
    "        text = retstr.getvalue()\n",
    "        device.close()\n",
    "        retstr.close()\n",
    "        return text\n",
    "    elif backend=='pypdf':\n",
    "        reader = PdfReader(path)\n",
    "        return [p.extract_text(0) for p in reader.pages]\n",
    "    elif backend=='pytesseract':\n",
    "        return pytesseract.image_to_string(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_File = Files[245]\n",
    "Text = pdf_to_text(os.path.join(pdf_path, _File), backend='pypdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Summary from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nums_samenvatting = [i for i,p in enumerate(Text) if 'samenvatting' in p.lower()]\n",
    "page_nums_dankwoord = [i for i,p in enumerate(Text) if any([t in p.lower() for t in ['dankwoord', 'nawoord']])]\n",
    "page_nums_empty = [i for i,p in enumerate(Text) if p.strip()=='']\n",
    "page_nums_ToC = [i for i,p in enumerate(Text) if 'content' in p.lower()]\n",
    "print(page_nums_samenvatting, page_nums_dankwoord, page_nums_empty, page_nums_ToC, _File)\n",
    "# extract the text from the pdfs based on the page_nums:\n",
    "# if there is a page number, we assume that text on that page is relevant\n",
    "\n",
    "# then we want to find the delimiters for the different sections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samenvatting_page(txts, page_nums_ToC):\n",
    "    ToC_num = min([n for n in page_nums_ToC if n>1])\n",
    "    ToC_page = txts[ToC_num]\n",
    "    return int(re.findall(r'Samenvatting[\\s\\t]+(\\d+)', ToC_page))\n",
    "\n",
    "# first element in ToC, after the samenvatting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate the remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all text starting from the first introduction header, after the table of contents.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
