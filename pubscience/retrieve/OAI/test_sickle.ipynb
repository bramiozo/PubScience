{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import lxml\n",
    "import bs4\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import oai\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'Tilburg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = ['athero', \n",
    "         'plaque', \n",
    "         'cardiovascular',\n",
    "         'cardiogram', \n",
    "         'cardiology', \n",
    "         'cardiologie',\n",
    "         'hartvaten',\n",
    "         'klinsch',\n",
    "         'medische',\n",
    "         'hartvaat',\n",
    "         'heart', \n",
    "         'vascular',\n",
    "         'angiogram', \n",
    "         'cardiologie', \n",
    "         'hartziekte', \n",
    "         'vaatziekte',\n",
    "         'medicine',\n",
    "         'disease', \n",
    "         'medical', \n",
    "         'therapy',\n",
    "         'therapeutic',\n",
    "         'diagnosic',\n",
    "         'clinical',\n",
    "         'surgical', \n",
    "         'metabolic',\n",
    "         'myocard',]\n",
    "\n",
    "tlist = ['recht', 'wetten', 'juridisch', 'wetboek', 'jurisprudentie', 'precedent', 'wet bibop', 'wetboek van strafrecht', 'wetboek van strafvordering']\n",
    "\n",
    "base_url =   oai.sources[source]['link'] #'https://repository.ubn.ru.nl/oai/openaire'  https://scholarlypublications.universiteitleiden.nl/oai2, http://dspace.library.uu.nl/oai/dissertation\n",
    "pdf_path = f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses/{source}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIRE_institutes = ['VU', 'UVA', 'Maastricht', 'Tilburg', 'RUG', 'UTwente', 'TUE', 'UU', 'Erasmus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: publications:withFiles contains keyword\n"
     ]
    }
   ],
   "source": [
    "keywords = ['clinical', 'medisch', 'medical', 'dissertation', 'umc', 'medicine',\n",
    "            'diss', 'phd', 'thesis', 'doctorate', 'dissertatie',\n",
    "            'doctoraat', 'proefschrift']\n",
    "if source in OpenAIRE_institutes:\n",
    "    keywords = keywords + ['publications:withfiles']\n",
    "\n",
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    #print(key,val)\n",
    "    if any([c in val.lower() for c in keywords]) | any([c in key.lower() for c in keywords]):\n",
    "        print(f'Set: {key} contains keyword')\n",
    "        Sets_to_mine.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining from set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24925it [09:42, 42.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# get records \n",
    "# Beware: this takes a long time.\n",
    "from collections import defaultdict\n",
    "records_lists = defaultdict(list)\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    #if set_to_mine in ['com_1874_298213']:\n",
    "    #    continue\n",
    "    print(f\"Mining from set: {set_to_mine}\")\n",
    "    try:\n",
    "        records = sickler.ListRecords(metadataPrefix='oai_dc', \n",
    "                                    ignore_deleted=True, \n",
    "                                    set=set_to_mine) # dissertation com_1874_298213\n",
    "        for record in tqdm(records):\n",
    "            records_lists[set_to_mine].append(record)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24925/24925 [00:02<00:00, 11754.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 359 relevant records in set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_records_lists = defaultdict(list)\n",
    "excluded_records_lists = defaultdict(list)\n",
    "\n",
    "cond_list = []\n",
    "\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    relevant_counter = 0\n",
    "    for r in tqdm(records_lists[set_to_mine]):\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        TOPIC = False\n",
    "        PDF = False\n",
    "        DOCTORATE = True if 'dissertation' in set_to_mine.lower() else False\n",
    "        EMBARGO = False\n",
    "\n",
    "        if source in ['Radboud']: \n",
    "            PDF=True\n",
    "        \n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['subject'] for t in tlist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['title'] for t in tlist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['description'] for t in tlist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass                \n",
    "            \n",
    "        try:\n",
    "            if ('pdf' in meta['format'][0].lower()):\n",
    "                PDF = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if ('doctoral' in meta['type'][0].lower()) |\\\n",
    "                    ('book' in meta['type'][0].lower()):\n",
    "                DOCTORATE = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if ('embargo' in meta['rights'][0].lower()) |\\\n",
    "                    ('restricted' in meta['rights'][0].lower()):\n",
    "                EMBARGO = True\n",
    "        except:\n",
    "            pass  \n",
    "        \n",
    "        cond_list.append({'Topic': TOPIC, \n",
    "                          'PDF': PDF, \n",
    "                          'Doctorate': DOCTORATE, \n",
    "                          'Embargo': EMBARGO}\n",
    "                         )\n",
    "        \n",
    "        if TOPIC & PDF & DOCTORATE & ~EMBARGO:\n",
    "            relevant_counter += 1\n",
    "            filtered_records_lists[set_to_mine].append(r)\n",
    "        else:\n",
    "            excluded_records_lists[set_to_mine].append(r)\n",
    "\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_counts = defaultdict(int)\n",
    "for r in excluded_records_lists['publications:withFiles']:\n",
    "    try:\n",
    "        types_counts[r.metadata['type'][0]] += 1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'article': 12069,\n",
       "             'book': 5644,\n",
       "             'bookPart': 1833,\n",
       "             'other': 362,\n",
       "             'contributionToPeriodical': 706,\n",
       "             'workingPaper': 3864,\n",
       "             'conferenceObject': 88})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_df = pd.DataFrame(cond_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic         1294\n",
       "PDF          24349\n",
       "Doctorate     7836\n",
       "Embargo      10646\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conds_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:00<00:00, 8255.92it/s]\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "\n",
    "for set_to_mine in filtered_records_lists.keys():\n",
    "    for record in tqdm(filtered_records_lists[set_to_mine]):\n",
    "        META = record.get_metadata()\n",
    "        try:\n",
    "            List_of_identifiers = META['identifier'] if 'identifier' in META.keys() else ['']\n",
    "            Title = META['title'] if 'title' in META.keys() else ['']\n",
    "            Description = META['description'] if 'description' in META.keys() else ['']\n",
    "            Date = META['date'] if 'date' in META.keys() else ['']\n",
    "            Language = META['language'] if 'language' in META.keys() else ['']       \n",
    "            Creator = META['creator'] if 'creator' in META.keys() else ['']\n",
    "            \n",
    "            \n",
    "            List_of_identifiers = [id for id in List_of_identifiers if id is not None]\n",
    "            \n",
    "            if len(List_of_identifiers)>0:\n",
    "                link_list.append({'Set': set_to_mine, \n",
    "                                'Link': List_of_identifiers, \n",
    "                                'Title': Title,\n",
    "                                'Description': Description,\n",
    "                                'Date': Date ,\n",
    "                                'Language': Language,\n",
    "                                'Creator': Creator,\n",
    "                                'Publisher': META.get('publisher'),\n",
    "                                }\n",
    "                                )\n",
    "            else:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            error_list.append(f\"Error parsing {e}: {META}: \")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {None: 22})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_counts = defaultdict(int)\n",
    "for r in link_list:\n",
    "    try:\n",
    "        publisher_counts[r['Publisher']] += 1\n",
    "    except:\n",
    "        pass\n",
    "publisher_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Set': 'publications:withFiles',\n",
       " 'Link': ['https://research.tilburguniversity.edu/en/publications/8b5ba7c6-2467-4e66-aa35-fa78fe44858c',\n",
       "  'https://pure.uvt.nl/ws/files/1063325/3240346.pdf',\n",
       "  'urn:ISBN:9789058503558'],\n",
       " 'Title': ['Een rechtsvergelijkend onderzoek naar de positie van consumenten op de Nederlandse en Belgische woningbouwmarkt:Bijdrage aan een mogelijke Europese regulering'],\n",
       " 'Description': [''],\n",
       " 'Date': ['2008'],\n",
       " 'Language': ['nld'],\n",
       " 'Creator': ['Dierikx, M.'],\n",
       " 'Publisher': ['Wolf Legal Publishers (WLP)']}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs_Leiden(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        # random number between 0.5 and 2.5 seconds\n",
    "        rndSleep = round(random.uniform(1, 5), 2)\n",
    "        sleep(rndSleep)\n",
    "        soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "        found, dsfound, esfound = False, False, False\n",
    "        for _res in soup.findAll('li', {'class':'ubl-file-view'}):\n",
    "            if _res.a is not None:\n",
    "                if _res.a.contents[0].strip().lower() == 'full text':\n",
    "                    _pdfdir = _res.a['href']\n",
    "                    found = True\n",
    "                elif _res.a.contents[0].strip().lower() == 'summary in dutch':\n",
    "                    _dutch_summary = _res.a['href']\n",
    "                    dsfound = True\n",
    "                elif _res.a.contents[0].strip().lower() == 'summary in english':\n",
    "                    _english_summary = _res.a['href']\n",
    "                    esfound = True\n",
    "\n",
    "        linkPdfAlt = f\"https://scholarlypublications.universiteitleiden.nl{_pdfdir}\" if found else None\n",
    "        DutchSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_dutch_summary}\" if dsfound else None\n",
    "        EnglishSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_english_summary}\" if esfound else None\n",
    "        \n",
    "        return linkPdfAlt, DutchSummaryLink, EnglishSummaryLink, r.status_code\n",
    "    except Exception as e:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_links(links, institute=None):\n",
    "    pdf_links = []\n",
    "    if institute in ['VU', 'UVA', 'UTwente']:\n",
    "        inclusion_terms = [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in ['Radboud']:\n",
    "        inclusion_terms = [r'handle', r'bitstream']\n",
    "    elif institute in ['Maastricht']:\n",
    "        inclusion_terms = [r'ASSET1', r'c[0-9]{3,4}\\.pdf']\n",
    "    elif institute in ['Tilburg']:\n",
    "        inclusion_terms = [r'\\.pdf']\n",
    "    elif institute in ['RUG']:\n",
    "        inclusion_terms = [r'summ\\.pdf', r'summary\\.pdf',  r'samenv\\.pdf', r'samenvat\\.pdf', r'[ch][0-9]{1,2}\\.pdf', \n",
    "                           r'thesis\\.pdf', r'proefschrift\\.pdf', r'dissertation\\.pdf', r'dissertatie\\.pdf']\n",
    "    elif institute in ['TUE']:\n",
    "        inclusion_terms = [r'summ\\.pdf', r'summary\\.pdf',  r'samenv\\.pdf', r'samenvat\\.pdf', r'[ch][0-9]{1,2}\\.pdf', \n",
    "                           r'thesis\\.pdf', r'proefschrift\\.pdf', r'dissertation\\.pdf', r'dissertatie\\.pdf']\n",
    "        inclusion_terms = inclusion_terms + [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in ['UU']:\n",
    "        inclusion_terms = [r'dspace\\.library\\.uu\\.nl']\n",
    "    elif institute in ['Leiden']:\n",
    "        inclusion_terms = [r'handle']\n",
    "    elif institute in ['Erasmus']:\n",
    "        inclusion_terms = [r'files']\n",
    "    else:\n",
    "        raise ValueError(f'Institute {institute} not recognized')\n",
    "    \n",
    "    inclusion_terms = [re.compile(rs) for rs in inclusion_terms]\n",
    "    \n",
    "    if institute in ['UU']:\n",
    "        _pdf_links = []\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')):\n",
    "                baselink = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "                _pdf_links.append(baselink + '/full.pdf')\n",
    "        pdf_links = _pdf_links\n",
    "    elif institute in ['Leiden']:\n",
    "        _pdf_links = []\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & (any([t.search(link) is not None for t in inclusion_terms])):\n",
    "                main, dutch_summ, engl_summ, return_code = get_pdfs_Leiden(link)\n",
    "                if return_code == 429:\n",
    "                    print(\"Too many requests. Waiting 60 seconds\")\n",
    "                    sleep(60)\n",
    "                else:\n",
    "                    _pdf_links.extend([main, dutch_summ, engl_summ])\n",
    "        pdf_links = [l for l in _pdf_links if l is not None]\n",
    "    else:\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & \\\n",
    "                    (link.lower().endswith('.pdf')):\n",
    "                if any([t.search(link) is not None for t in inclusion_terms]):\n",
    "                    pdf_links.append(link)        \n",
    "\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1119"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1119/1119 [00:00<00:00, 48589.71it/s]\n"
     ]
    }
   ],
   "source": [
    "link_pdf_list = []\n",
    "for l in tqdm(link_list):\n",
    "    links = extract_pdf_links(l['Link'], institute=source)\n",
    "    tmp = []\n",
    "    for _l in links:\n",
    "        Creator = l['Creator'][0] if l['Creator'][0] is not None else 'Unknown'\n",
    "        Date = l['Date'][0] if l['Date'][0] is not None else 'Unknown'\n",
    "        \n",
    "        pdfPath = _l.split(\"/\")[-1].replace(\"%20\", \"_\").rstrip('.pdf')\n",
    "        pdfPath = source + \"_\" + Creator + \"_\" + Date + \"_\" + pdfPath\n",
    "        pdfPath = pdfPath.replace(\",\", \"\")\n",
    "        pdfPath = pdfPath.replace(\":\", \"\")\n",
    "        pdfPath = pdfPath.replace(\".\", \"\")\n",
    "        pdfPath = pdfPath.replace(\" \", \"\")\n",
    "        pdfPath = os.path.join(pdf_path, pdfPath+\".pdf\")\n",
    "        tmp.append((_l, pdfPath))\n",
    "    if len(tmp)==0:\n",
    "        continue\n",
    "    l['pdf_links'] = tmp\n",
    "    link_pdf_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1119"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_pdf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries, based on the title\n",
    "title_set = set()\n",
    "unique_link_list = []\n",
    "for el in link_pdf_list:\n",
    "    try:\n",
    "        title = el['Title'][0]\n",
    "        if title not in title_set:\n",
    "            unique_link_list.append(el)\n",
    "            title_set.add(title)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
    "\n",
    "# add sleep\n",
    "def pdf_writer(pdf_url, _pdf_path):\n",
    "    r = requests.get(pdf_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for i,chunk in enumerate(r.iter_content(chunk_size=1024)): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return True, 200\n",
    "    else:\n",
    "        return False, r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [3:43:14<00:00, 11.98s/it] \n"
     ]
    }
   ],
   "source": [
    "if source=='Radboud':\n",
    "    filelist = os.listdir(pdf_path)\n",
    "    pre_list = [f.split(\"_\")[-1] for f in filelist]\n",
    "if source=='UU':\n",
    "    filelist = os.listdir(pdf_path)\n",
    "    pre_list = [f.split(\".\")[-2] for f in filelist]\n",
    "\n",
    "pdf_error_list = []\n",
    "skipped_list = []\n",
    "src_list = []\n",
    "rcode_list = []\n",
    "success_list = []\n",
    "extract_full_text = True\n",
    "for link in tqdm(unique_link_list):\n",
    "    for lt in link['pdf_links']:\n",
    "        _pdf_path = lt[1]\n",
    "        pdf_url = lt[0]\n",
    "        return_code = None\n",
    "\n",
    "        if source=='Radboud':\n",
    "            if _pdf_path.split(\"_\")[-1] in pre_list:\n",
    "                skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "                continue\n",
    "        if source=='UU':\n",
    "            if link['pdf_links'][0][0].split(\"/\")[-2] in pre_list:\n",
    "                skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "                continue\n",
    "            \n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "        else:\n",
    "            # try to download the pdf\n",
    "            try:\n",
    "                write_file, return_code = pdf_writer(pdf_url, _pdf_path)\n",
    "                success_list.append(f'Pdf downloaded: {_pdf_path}')\n",
    "            except Exception as e:\n",
    "                pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')  \n",
    "\n",
    "            if return_code == 429:\n",
    "                print(f'Rate limit exceeded...sleeping 30 seconds')\n",
    "                sleep(30)\n",
    "                continue\n",
    "            else:\n",
    "                sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 errors while downloading pdfs\n",
      "Skipped 5 pdfs because they already existed\n"
     ]
    }
   ],
   "source": [
    "print(f'Found {len(pdf_error_list)} errors while downloading pdfs')\n",
    "print(f'Skipped {len(skipped_list)} pdfs because they already existed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
