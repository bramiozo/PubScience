{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "import lxml\n",
    "import bs4\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import oai\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'Maastricht'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = ['athero', \n",
    "         'plaque', \n",
    "         'cardiovascular',\n",
    "         'cardiogram', \n",
    "         'cardiology', \n",
    "         'cardiologie',\n",
    "         'hartvaten',\n",
    "         'klinsch',\n",
    "         'medische',\n",
    "         'hartvaat',\n",
    "         'heart', \n",
    "         'vascular',\n",
    "         'angiogram', \n",
    "         'cardiologie', \n",
    "         'hartziekte', \n",
    "         'vaatziekte',\n",
    "         'medicine',\n",
    "         'disease', \n",
    "         'medical', \n",
    "         'therapy',\n",
    "         'therapeutic',\n",
    "         'diagnosic',\n",
    "         'clinical',\n",
    "         'surgical', \n",
    "         'metabolic',\n",
    "         'myocard',]\n",
    "base_url =   oai.sources[source]['link'] #'https://repository.ubn.ru.nl/oai/openaire'  https://scholarlypublications.universiteitleiden.nl/oai2, http://dspace.library.uu.nl/oai/dissertation\n",
    "pdf_path = f'//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses/{source}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIRE_institutes = ['VU', 'UVA', 'Maastricht']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: publications:withFiles contains keyword\n"
     ]
    }
   ],
   "source": [
    "keywords = ['clinical', 'medisch', 'medical', 'dissertation',\n",
    "            'diss', 'phd', 'thesis', 'doctorate', \n",
    "            'doctoraat', 'proefschrift']\n",
    "if source in OpenAIRE_institutes:\n",
    "    keywords = keywords + ['publications:withfiles']\n",
    "\n",
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    if any([c in val.lower() for c in keywords]) | any([c in key.lower() for c in keywords]):\n",
    "        print(f'Set: {key} contains keyword')\n",
    "        Sets_to_mine.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining from set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [15:21<00:00, 921.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# get records \n",
    "from collections import defaultdict\n",
    "records_lists = defaultdict(list)\n",
    "for set_to_mine in tqdm(Sets_to_mine):\n",
    "    print(f\"Mining from set: {set_to_mine}\")\n",
    "    records = sickler.ListRecords(metadataPrefix='oai_dc', \n",
    "                                  ignore_deleted=True, \n",
    "                                  set=set_to_mine) # dissertation com_1874_298213\n",
    "    for record in records:\n",
    "        records_lists[set_to_mine].append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21600/21600 [00:03<00:00, 6280.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1814 relevant records in set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_records_lists = defaultdict(list)\n",
    "excluded_records_lists = defaultdict(list)\n",
    "\n",
    "cond_list = []\n",
    "\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    relevant_counter = 0\n",
    "    for r in tqdm(records_lists[set_to_mine]):\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        TOPIC = False\n",
    "        PDF = False\n",
    "        DOCTORATE = False\n",
    "        EMBARGO = False\n",
    "\n",
    "        if source in ['Radboud']: \n",
    "            PDF=True\n",
    "        \n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['subject'] for t in tlist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['title'] for t in tlist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj.lower() for subj in meta['description'] for t in tlist]):\n",
    "                TOPIC = True\n",
    "        except:\n",
    "            pass                \n",
    "            \n",
    "        try:\n",
    "            if ('pdf' in meta['format'][0].lower()):\n",
    "                PDF = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if ('doctoral' in meta['type'][0].lower()) |\\\n",
    "                    ('book' in meta['type'][0].lower()):\n",
    "                DOCTORATE = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if ('embargo' in meta['rights'][0].lower()) |\\\n",
    "                    ('restricted' in meta['rights'][0].lower()):\n",
    "                EMBARGO = True\n",
    "        except:\n",
    "            pass  \n",
    "        \n",
    "        cond_list.append({'Topic': TOPIC, \n",
    "                          'PDF': PDF, \n",
    "                          'Doctorate': DOCTORATE, \n",
    "                          'Embargo': EMBARGO}\n",
    "                         )\n",
    "        \n",
    "        if TOPIC & PDF & DOCTORATE & ~EMBARGO:\n",
    "            relevant_counter += 1\n",
    "            filtered_records_lists[set_to_mine].append(r)\n",
    "        else:\n",
    "            excluded_records_lists[set_to_mine].append(r)\n",
    "\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_counts = defaultdict(int)\n",
    "for r in excluded_records_lists['publications:withFiles']:\n",
    "    try:\n",
    "        types_counts[r.metadata['type'][0]] += 1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'bookPart': 931,\n",
       "             'article': 9945,\n",
       "             'book': 6050,\n",
       "             'workingPaper': 2464,\n",
       "             'contributionToPeriodical': 309,\n",
       "             'other': 73,\n",
       "             'conferenceObject': 14})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_df = pd.DataFrame(cond_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic         5671\n",
       "PDF          21497\n",
       "Doctorate     8795\n",
       "Embargo        318\n",
       "dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conds_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1814/1814 [00:00<00:00, 15982.43it/s]\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "\n",
    "for set_to_mine in filtered_records_lists.keys():\n",
    "    for record in tqdm(filtered_records_lists[set_to_mine]):\n",
    "        META = record.get_metadata()\n",
    "\n",
    "        try:\n",
    "            List_of_identifiers = META['identifier'] if 'identifier' in META.keys() else ['']\n",
    "            Title = META['title'] if 'title' in META.keys() else ['']\n",
    "            Description = META['description'] if 'description' in META.keys() else ['']\n",
    "            Date = META['date'] if 'date' in META.keys() else ['']\n",
    "            Language = META['language'] if 'language' in META.keys() else ['']       \n",
    "            Creator = META['creator'] if 'creator' in META.keys() else ['']\n",
    "            \n",
    "            \n",
    "            List_of_identifiers = [id for id in List_of_identifiers if id is not None]\n",
    "            \n",
    "            if len(List_of_identifiers)>0:\n",
    "                link_list.append({'Set': set_to_mine, \n",
    "                                'Link': List_of_identifiers, \n",
    "                                'Title': Title,\n",
    "                                'Description': Description,\n",
    "                                'Date': Date ,\n",
    "                                'Language': Language,\n",
    "                                'Creator': Creator\n",
    "                                }\n",
    "                                )\n",
    "            else:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            error_list.append(f\"Error parsing {e}: {META}: \")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 886/886 [00:00<00:00, 8949.35it/s]\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    for r in tqdm(filtered_records_lists[set_to_mine]):\n",
    "        meta = r.get_metadata()\n",
    "        \n",
    "        if relevant:\n",
    "            relevant_counter += 1\n",
    "            if source == 'LUMC':\n",
    "                try:\n",
    "                    link = meta['identifier'][-1] \n",
    "                    # identify first url in list\n",
    "                    found_link = False\n",
    "                    for l in meta['identifier']:\n",
    "                        if ('http:' in l) or ('https:' in l):\n",
    "                            link = l\n",
    "                            found_link = True\n",
    "                            break\n",
    "                    if not found_link:\n",
    "                        error_list.append(f'No link found for {meta[\"identifier\"]}')  \n",
    "\n",
    "                    doc_id = link.split('/')[-1]\n",
    "                    doc_id_int = int(doc_id)+2\n",
    "\n",
    "                    link = f\"https://scholarlypublications.universiteitleiden.nl/handle/1887/{doc_id}\"\n",
    "                    linkPdf = f\"https://scholarlypublications.universiteitleiden.nl/access/item%3A{doc_id_int}/download\"\n",
    "\n",
    "                    # extract through link url. The directory can be found in <li class='ubl-file-download'> <a href='...'>\n",
    "                    # only if <a href in ubl-file-view is \"full\"\n",
    "                    r = requests.get(link)\n",
    "                    # random number between 0.5 and 2.5 seconds\n",
    "                    rndSleep = round(random.uniform(0.5, 2.5), 2)\n",
    "                    sleep(rndSleep)\n",
    "                    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "                    found, dsfound, esfound = False, False, False\n",
    "                    for _res in soup.findAll('li', {'class':'ubl-file-view'}):\n",
    "                        if _res.a is not None:\n",
    "                            if _res.a.contents[0].strip().lower() == 'full text':\n",
    "                                _pdfdir = _res.a['href']\n",
    "                                found = True\n",
    "                            elif _res.a.contents[0].strip().lower() == 'summary in dutch':\n",
    "                                _dutch_summary = _res.a['href']\n",
    "                                dsfound = True\n",
    "                            elif _res.a.contents[0].strip().lower() == 'summary in english':\n",
    "                                _english_summary = _res.a['href']\n",
    "                                esfound = True\n",
    "\n",
    "                    linkPdfAlt = f\"https://scholarlypublications.universiteitleiden.nl{_pdfdir}\" if found else None\n",
    "                    DutchSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_dutch_summary}\" if dsfound else None\n",
    "                    EnglishSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_english_summary}\" if esfound else None\n",
    "                    \n",
    "                    try:\n",
    "                        lang = meta['language'][0]\n",
    "                    except:\n",
    "                        lang = None\n",
    "                except Exception as e:\n",
    "                    error_list.append(f'Error: {e} for link: {link}, with meta data: {meta[\"identifier\"]}')\n",
    "                    pass\n",
    "                    #raise ValueError(f'Could not find pdf link for {link}, with error raised: {e}')\n",
    "            elif source == 'UU':                \n",
    "                link = meta['identifier'][0]\n",
    "                baselink = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "                linkPdf = baselink + '/full.pdf'\n",
    "\n",
    "                try:\n",
    "                    linkPdfAlt = baselink +'/'+meta['creator'][0].split(',')[0].lower()+'.pdf'\n",
    "                except:\n",
    "                    linkPdfAlt = None\n",
    "                \n",
    "                DutchSummaryLink = None\n",
    "                EnglishSummaryLink = None\n",
    "                try:\n",
    "                    lang = meta['language'][0]\n",
    "                except:\n",
    "                    lang = None\n",
    "\n",
    "            elif source == 'Radboud':\n",
    "                try:\n",
    "                    found_link = False\n",
    "                    found_pdf_link = False\n",
    "                    for l in meta['identifier']:\n",
    "                        if l is not None:\n",
    "                            if (('http:' in l) or ('https:' in l)) & (l.split(\"/\")[-1].endswith('.pdf')):\n",
    "                                linkPdf = l\n",
    "                                found_pdf_link = True                            \n",
    "                            elif (('http:' in l) or ('https:' in l)):\n",
    "                                link = l\n",
    "                                found_link = True                           \n",
    "\n",
    "                    if not found_link:                        \n",
    "                        error_list.append(f'No link found for {meta[\"identifier\"]}')  \n",
    "                        continue\n",
    "                    if not found_pdf_link:\n",
    "                        error_list.append(f'No pdf link found for {meta[\"identifier\"]}')\n",
    "                        linkPdf = None\n",
    "\n",
    "                    linkPdfAlt = None\n",
    "                    DutchSummaryLink = None\n",
    "                    EnglishSummaryLink = None\n",
    "                    try:\n",
    "                        lang = meta['language'][0]\n",
    "                    except:\n",
    "                        lang = None\n",
    "                except Exception as e:\n",
    "                    error_list.append(f'Error: {e} for link: {link}, with meta data: {meta[\"identifier\"]}')\n",
    "                    continue\n",
    "                    #raise ValueError(f'Could not find pdf link for {link}, with error raised: {e}')                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstring =\"De BLidieBLa bl\"\n",
    "re_str = re.compile(r'bla', re.IGNORECASE)\n",
    "\n",
    "re_str.search(tstring) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_links(links, institute=None):\n",
    "    pdf_links = []\n",
    "    if institute in [r'VU', r'UVA']:\n",
    "        inclusion_terms = [r'abstract', r'full', r'complete', r'samenvatting', r'summary', r'thesis', r'chapter']\n",
    "    elif institute in [r'Radboud']:\n",
    "        inclusion_terms = [r'handle', r'bitstream']\n",
    "    elif institute in ['Maastricht']:\n",
    "        inclusion_terms = [r'ASSET1', r'c[0-9]{3,4}\\.pdf']\n",
    "    \n",
    "    inclusion_terms = [re.compile(rs) for rs in inclusion_terms]\n",
    "        \n",
    "    for link in links:\n",
    "        if (link.lower().startswith('http')) & \\\n",
    "                (link.lower().endswith('.pdf')):\n",
    "            if any([t.search(link) is not None for t in inclusion_terms]):\n",
    "                pdf_links.append(link)    \n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pdf_list = []\n",
    "for l in link_list:\n",
    "    links = extract_pdf_links(l['Link'], institute=source)\n",
    "    tmp = []\n",
    "    for _l in links:\n",
    "        pdfPath = _l.split(\"/\")[-1].replace(\"%20\", \"_\").rstrip('.pdf')\n",
    "        pdfPath = source + \"_\" + l['Creator'][0] + \"_\" + l['Date'][0] + \"_\" + pdfPath\n",
    "        pdfPath = pdfPath.replace(\",\", \"\")\n",
    "        pdfPath = pdfPath.replace(\":\", \"\")\n",
    "        pdfPath = pdfPath.replace(\".\", \"\")\n",
    "        pdfPath = pdfPath.replace(\" \", \"\")\n",
    "        pdfPath = os.path.join(pdf_path, pdfPath+\".pdf\")\n",
    "        tmp.append((_l, pdfPath))\n",
    "    l['pdf_links'] = tmp\n",
    "    link_pdf_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries, based on the title\n",
    "title_set = set()\n",
    "unique_link_list = []\n",
    "for el in link_pdf_list:\n",
    "    try:\n",
    "        title = el['Title'][0]\n",
    "        if title not in title_set:\n",
    "            unique_link_list.append(el)\n",
    "            title_set.add(title)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
    "\n",
    "# add sleep\n",
    "def pdf_writer(pdf_url, _pdf_path):\n",
    "    r = requests.get(pdf_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for i,chunk in enumerate(r.iter_content(chunk_size=1024)): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return True, 200\n",
    "    else:\n",
    "        return False, r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 573/2408 [1:11:15<2:46:35,  5.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 597/2408 [1:14:48<3:00:43,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 608/2408 [1:16:15<2:52:30,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 647/2408 [1:21:00<2:45:26,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 664/2408 [1:23:12<2:43:09,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 680/2408 [1:25:24<2:45:02,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 733/2408 [1:31:44<2:30:23,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 771/2408 [1:36:39<3:25:14,  7.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mt:\\laupodteam\\AIOS\\Bram\\notebooks\\code_dev\\PubScience\\pubscience\\retrieve\\OAI\\test_sickle.ipynb Cell 21\u001b[0m line \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# try to download the pdf\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         write_file, return_code \u001b[39m=\u001b[39m pdf_writer(pdf_url, _pdf_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         pdf_error_list\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m for link: \u001b[39m\u001b[39m{\u001b[39;00mlink[\u001b[39m\"\u001b[39m\u001b[39mLink\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)  \n",
      "\u001b[1;32mt:\\laupodteam\\AIOS\\Bram\\notebooks\\code_dev\\PubScience\\pubscience\\retrieve\\OAI\\test_sickle.ipynb Cell 21\u001b[0m line \u001b[0;36mpdf_writer\u001b[1;34m(pdf_url, _pdf_path)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i,chunk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m)): \n\u001b[0;32m     <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             \u001b[39mif\u001b[39;00m chunk: \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                 f\u001b[39m.\u001b[39;49mwrite(chunk)\n\u001b[0;32m     <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, \u001b[39m200\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pdf_error_list = []\n",
    "skipped_list = []\n",
    "src_list = []\n",
    "rcode_list = []\n",
    "extract_full_text = True\n",
    "for link in tqdm(unique_link_list):\n",
    "    for lt in link['pdf_links']:\n",
    "        _pdf_path = lt[1]\n",
    "        pdf_url = lt[0]\n",
    "        return_code = None\n",
    "\n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "        else:\n",
    "            # try to download the pdf\n",
    "            try:\n",
    "                write_file, return_code = pdf_writer(pdf_url, _pdf_path)\n",
    "            except Exception as e:\n",
    "                pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')  \n",
    "\n",
    "            if return_code == 429:\n",
    "                print(f'Rate limit exceeded...sleeping 30 seconds')\n",
    "                sleep(30)\n",
    "                continue\n",
    "            else:\n",
    "                sleep(4)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 errors while downloading pdfs\n",
      "Skipped 0 pdfs because they already existed\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "print(f'Found {len(pdf_error_list)} errors while downloading pdfs')\n",
    "print(f'Skipped {len(skipped_list)} pdfs because they already existed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
