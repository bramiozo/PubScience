{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "import lxml\n",
    "import bs4\n",
    "import random\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = ['athero', 'plaque', 'cardiovascular', 'cardiogram', 'cardiology', 'heart', 'vascular', 'angiogram', \n",
    "         'cardiologie', 'hartziekte', 'vaatziekte', 'medical', 'clinical', 'surgical', 'metabolic', 'myocard']\n",
    "institute = 'VU'\n",
    "base_url =   'https://research.vu.nl/ws/oai' #'https://repository.ubn.ru.nl/oai/openaire'  https://scholarlypublications.universiteitleiden.nl/oai2, http://dspace.library.uu.nl/oai/dissertation\n",
    "pdf_path = '//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIRE_institutes = ['VU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['medisch', 'medical', 'diss', 'phd', 'thesis']\n",
    "if institute in OpenAIRE_institutes:\n",
    "    keywords = keywords + ['publications:withfiles']\n",
    "\n",
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    if any([c in val.lower() for c in keywords]) | any([c in key.lower() for c in keywords]):\n",
    "        print(f'Set: {key} contains keyword')\n",
    "        Sets_to_mine.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get records \n",
    "from collections import defaultdict\n",
    "records_lists = defaultdict(list)\n",
    "for set_to_mine in tqdm(Sets_to_mine):\n",
    "    print(f\"Mining from set: {set_to_mine}\")\n",
    "    records = sickler.ListRecords(metadataPrefix='oai_dc', \n",
    "                                  ignore_deleted=True, \n",
    "                                  set=set_to_mine) # dissertation com_1874_298213\n",
    "    for record in records:\n",
    "        records_lists[set_to_mine].append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32671/32671 [00:02<00:00, 12784.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 657 relevant records in set: publications:withFiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_records_lists = defaultdict(list)\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    relevant_counter = 0\n",
    "    for r in tqdm(records_lists[set_to_mine]):\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        try:\n",
    "            if any([t in subj for subj in meta['subject'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj for subj in meta['title'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj for subj in meta['description'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass \n",
    "\n",
    "        try:\n",
    "            if ('pdf' in meta['format'][0].lower()) & relevant:\n",
    "                relevant = True\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if ('doctoral' in meta['type'][0].lower()) & relevant:\n",
    "                relevant = True\n",
    "            else:\n",
    "                relevant = False\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if ('embargo' in meta['rights'][0].lower()) & relevant:\n",
    "                relevant = False\n",
    "        except:\n",
    "            pass  \n",
    "        \n",
    "        if relevant:\n",
    "            relevant_counter += 1\n",
    "            filtered_records_lists[set_to_mine].append(r)\n",
    "\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:00<00:00, 12514.40it/s]\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "\n",
    "for set_to_mine in filtered_records_lists.keys():\n",
    "    for record in tqdm(filtered_records_lists[set_to_mine]):\n",
    "        META = record.get_metadata()\n",
    "\n",
    "        try:\n",
    "            List_of_identifiers = META['identifier'] if 'identifier' in META.keys() else ['']\n",
    "            Title = META['title'] if 'title' in META.keys() else ['']\n",
    "            Description = META['description'] if 'description' in META.keys() else ['']\n",
    "            Date = META['date'] if 'date' in META.keys() else ['']\n",
    "            Language = META['language'] if 'language' in META.keys() else ['']       \n",
    "            Creator = META['creator'] if 'creator' in META.keys() else ['']\n",
    "            link_list.append({'Set': set_to_mine, \n",
    "                            'Link': List_of_identifiers, \n",
    "                            'Title': Title,\n",
    "                            'Description': Description,\n",
    "                            'Date': Date ,\n",
    "                            'Language': Language,\n",
    "                            'Creator': Creator\n",
    "                            }\n",
    "                            )\n",
    "        except Exception as e:\n",
    "            error_list.append(f\"Error parsing {e}: {META}: \")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            if institute == 'LUMC':\n",
    "                try:\n",
    "                    link = meta['identifier'][-1] \n",
    "                    # identify first url in list\n",
    "                    found_link = False\n",
    "                    for l in meta['identifier']:\n",
    "                        if ('http:' in l) or ('https:' in l):\n",
    "                            link = l\n",
    "                            found_link = True\n",
    "                            break\n",
    "                    if not found_link:\n",
    "                        error_list.append(f'No link found for {meta[\"identifier\"]}')  \n",
    "\n",
    "                    doc_id = link.split('/')[-1]\n",
    "                    doc_id_int = int(doc_id)+2\n",
    "\n",
    "                    link = f\"https://scholarlypublications.universiteitleiden.nl/handle/1887/{doc_id}\"\n",
    "                    linkPdf = f\"https://scholarlypublications.universiteitleiden.nl/access/item%3A{doc_id_int}/download\"\n",
    "\n",
    "                    # extract through link url. The directory can be found in <li class='ubl-file-download'> <a href='...'>\n",
    "                    # only if <a href in ubl-file-view is \"full\"\n",
    "                    r = requests.get(link)\n",
    "                    # random number between 0.5 and 2.5 seconds\n",
    "                    rndSleep = round(random.uniform(0.5, 2.5), 2)\n",
    "                    sleep(rndSleep)\n",
    "                    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "                    found, dsfound, esfound = False, False, False\n",
    "                    for _res in soup.findAll('li', {'class':'ubl-file-view'}):\n",
    "                        if _res.a is not None:\n",
    "                            if _res.a.contents[0].strip().lower() == 'full text':\n",
    "                                _pdfdir = _res.a['href']\n",
    "                                found = True\n",
    "                            elif _res.a.contents[0].strip().lower() == 'summary in dutch':\n",
    "                                _dutch_summary = _res.a['href']\n",
    "                                dsfound = True\n",
    "                            elif _res.a.contents[0].strip().lower() == 'summary in english':\n",
    "                                _english_summary = _res.a['href']\n",
    "                                esfound = True\n",
    "\n",
    "                    linkPdfAlt = f\"https://scholarlypublications.universiteitleiden.nl{_pdfdir}\" if found else None\n",
    "                    DutchSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_dutch_summary}\" if dsfound else None\n",
    "                    EnglishSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_english_summary}\" if esfound else None\n",
    "                    \n",
    "                    try:\n",
    "                        lang = meta['language'][0]\n",
    "                    except:\n",
    "                        lang = None\n",
    "                except Exception as e:\n",
    "                    error_list.append(f'Error: {e} for link: {link}, with meta data: {meta[\"identifier\"]}')\n",
    "                    pass\n",
    "                    #raise ValueError(f'Could not find pdf link for {link}, with error raised: {e}')\n",
    "            elif institute == 'UU':                \n",
    "                link = meta['identifier'][0]\n",
    "                baselink = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "                linkPdf = baselink + '/full.pdf'\n",
    "\n",
    "                try:\n",
    "                    linkPdfAlt = baselink +'/'+meta['creator'][0].split(',')[0].lower()+'.pdf'\n",
    "                except:\n",
    "                    linkPdfAlt = None\n",
    "                \n",
    "                DutchSummaryLink = None\n",
    "                EnglishSummaryLink = None\n",
    "                try:\n",
    "                    lang = meta['language'][0]\n",
    "                except:\n",
    "                    lang = None\n",
    "\n",
    "            elif institute == 'Radboud':\n",
    "                try:\n",
    "                    found_link = False\n",
    "                    found_pdf_link = False\n",
    "                    for l in meta['identifier']:\n",
    "                        if l is not None:\n",
    "                            if (('http:' in l) or ('https:' in l)) & (l.split(\"/\")[-1].endswith('.pdf')):\n",
    "                                linkPdf = l\n",
    "                                found_pdf_link = True                            \n",
    "                            elif (('http:' in l) or ('https:' in l)):\n",
    "                                link = l\n",
    "                                found_link = True                           \n",
    "\n",
    "                    if not found_link:                        \n",
    "                        error_list.append(f'No link found for {meta[\"identifier\"]}')  \n",
    "                        continue\n",
    "                    if not found_pdf_link:\n",
    "                        error_list.append(f'No pdf link found for {meta[\"identifier\"]}')\n",
    "                        linkPdf = None\n",
    "\n",
    "                    linkPdfAlt = None\n",
    "                    DutchSummaryLink = None\n",
    "                    EnglishSummaryLink = None\n",
    "                    try:\n",
    "                        lang = meta['language'][0]\n",
    "                    except:\n",
    "                        lang = None\n",
    "                except Exception as e:\n",
    "                    error_list.append(f'Error: {e} for link: {link}, with meta data: {meta[\"identifier\"]}')\n",
    "                    continue\n",
    "                    #raise ValueError(f'Could not find pdf link for {link}, with error raised: {e}')                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_links(links, institute=None):\n",
    "    pdf_links = []\n",
    "    inclusion_terms = ['abstract', 'full', 'complete', 'samenvatting', 'summary', 'thesis']\n",
    "    if institute in ['VU']:\n",
    "        for link in links:\n",
    "            if (link.lower().startswith('http')) & \\\n",
    "                    (link.lower().endswith('.pdf')):\n",
    "                if any([t in link.lower() for t in inclusion_terms]):\n",
    "                    pdf_links.append(link)\n",
    "    elif institute in ['Radboud']:\n",
    "        \n",
    "\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pdf_list = []\n",
    "for l in link_list:\n",
    "    links = extract_pdf_links(l['Link'], institute=institute)\n",
    "    tmp = []\n",
    "    for _l in links:\n",
    "        pdfPath = _l.split(\"/\")[-1].replace(\"%20\", \"_\").rstrip('.pdf')\n",
    "        pdfPath = institute + \"_\" + l['Creator'][0] + \"_\" + l['Date'][0] + \"_\" + pdfPath\n",
    "        pdfPath = pdfPath.replace(\",\", \"\")\n",
    "        pdfPath = pdfPath.replace(\":\", \"\")\n",
    "        pdfPath = pdfPath.replace(\".\", \"\")\n",
    "        pdfPath = pdfPath.replace(\" \", \"\")\n",
    "        pdfPath = os.path.join(pdf_path, pdfPath+\".pdf\")\n",
    "        tmp.append((_l, pdfPath))\n",
    "    l['pdf_links'] = tmp\n",
    "    link_pdf_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries, based on the title\n",
    "title_set = set()\n",
    "unique_link_list = []\n",
    "for el in link_pdf_list:\n",
    "    try:\n",
    "        title = el['Title'][0]\n",
    "        if title not in title_set:\n",
    "            unique_link_list.append(el)\n",
    "            title_set.add(title)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
    "\n",
    "# add sleep\n",
    "\n",
    "def pdf_writer(pdf_url, _pdf_path):\n",
    "    r = requests.get(pdf_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for i,chunk in enumerate(r.iter_content(chunk_size=1024)): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return True, 200\n",
    "    else:\n",
    "        return False, r.status_code\n",
    "\n",
    "pdf_error_list = []\n",
    "skipped_list = []\n",
    "src_list = []\n",
    "rcode_list = []\n",
    "extract_full_text = True\n",
    "for link in tqdm(unique_link_list):\n",
    "    for lt in link['pdf_links']:\n",
    "        _pdf_path = lt[1]\n",
    "        pdf_url = lt[0]\n",
    "        return_code = None\n",
    "\n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "        else:\n",
    "            # try to download the pdf\n",
    "            try:\n",
    "                write_file, return_code = pdf_writer(pdf_url, _pdf_path)\n",
    "            except Exception as e:\n",
    "                pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')  \n",
    "\n",
    "            if return_code == 429:\n",
    "                print(f'Rate limit exceeded...sleeping 30 seconds')\n",
    "                sleep(30)\n",
    "                continue\n",
    "            else:\n",
    "                sleep(4)  \n",
    "    \n",
    "print(f'Found {len(pdf_error_list)} errors while downloading pdfs')\n",
    "print(f'Skipped {len(skipped_list)} pdfs because they already existed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
