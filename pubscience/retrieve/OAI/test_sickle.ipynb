{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "import lxml\n",
    "import bs4\n",
    "import random\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = ['cardiovascular', 'cardiogram', 'cardiology', 'heart', 'vascular', 'angiogram', 'cardiologie', 'hartziekte', 'vaatziekte', 'medical', 'clinical', 'surgical']\n",
    "institute = 'VU'\n",
    "base_url =   'https://research.vu.nl/ws/oai?metadataPrefix=oai_dc' #'https://repository.ubn.ru.nl/oai/openaire'  https://scholarlypublications.universiteitleiden.nl/oai2, http://dspace.library.uu.nl/oai/dissertation\n",
    "pdf_path = '//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/MEDICAL_TEXT/RAW/PhDTheses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIRE_institutes = ['VU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickler = sickle.Sickle(base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = sickler.ListSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets = {}\n",
    "for s in sets:\n",
    "    Sets[s.setSpec]  = s.setName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: publications:withFiles contains keyword\n"
     ]
    }
   ],
   "source": [
    "keywords = ['medisch', 'medical', 'diss', 'phd', 'thesis']\n",
    "if institute in OpenAIRE_institutes:\n",
    "    keywords = keywords + ['publications:withfiles']\n",
    "\n",
    "Sets_to_mine = []\n",
    "for key, val in Sets.items():\n",
    "    if any([c in val.lower() for c in keywords]) | any([c in key.lower() for c in keywords]):\n",
    "        print(f'Set: {key} contains keyword')\n",
    "        Sets_to_mine.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BadArgument",
     "evalue": "The request includes illegal arguments or is missing required arguments. resumptionToken is exclusive!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadArgument\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mt:\\laupodteam\\AIOS\\Bram\\notebooks\\code_dev\\PubScience\\pubscience\\retrieve\\OAI\\test_sickle.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m set_to_mine \u001b[39min\u001b[39;00m tqdm(Sets_to_mine):\n\u001b[0;32m      <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     records \u001b[39m=\u001b[39m sickler\u001b[39m.\u001b[39mListRecords(metadataPrefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, ignore_deleted\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mset\u001b[39m\u001b[39m=\u001b[39mset_to_mine) \u001b[39m# dissertation com_1874_298213\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[0;32m      <a href='vscode-notebook-cell:/t%3A/laupodteam/AIOS/Bram/notebooks/code_dev/PubScience/pubscience/retrieve/OAI/test_sickle.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         records_lists[set_to_mine]\u001b[39m.\u001b[39mappend(record)\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sickle\\iterator.py:52\u001b[0m, in \u001b[0;36mBaseOAIIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext()\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sickle\\iterator.py:151\u001b[0m, in \u001b[0;36mOAIItemIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m mapped\n\u001b[0;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresumption_token \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresumption_token\u001b[39m.\u001b[39mtoken:\n\u001b[1;32m--> 151\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_response()\n\u001b[0;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sickle\\iterator.py:138\u001b[0m, in \u001b[0;36mOAIItemIterator._next_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_response\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[39msuper\u001b[39;49m(OAIItemIterator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_next_response()\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moai_response\u001b[39m.\u001b[39mxml\u001b[39m.\u001b[39miterfind(\n\u001b[0;32m    140\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m.//\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msickle\u001b[39m.\u001b[39moai_namespace \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39melement)\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sickle\\iterator.py:91\u001b[0m, in \u001b[0;36mBaseOAIIterator._next_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m description \u001b[39m=\u001b[39m error\u001b[39m.\u001b[39mtext \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m     92\u001b[0m         oaiexceptions, code[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mupper() \u001b[39m+\u001b[39m code[\u001b[39m1\u001b[39m:])(description)\n\u001b[0;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m oaiexceptions\u001b[39m.\u001b[39mOAIError(description)\n",
      "\u001b[1;31mBadArgument\u001b[0m: The request includes illegal arguments or is missing required arguments. resumptionToken is exclusive!"
     ]
    }
   ],
   "source": [
    "# get records \n",
    "from collections import defaultdict\n",
    "records_lists = defaultdict(list)\n",
    "for set_to_mine in tqdm(Sets_to_mine):\n",
    "    records = sickler.ListRecords(metadataPrefix='oai_dc', \n",
    "                                  ignore_deleted=True, \n",
    "                                  set=set_to_mine) # dissertation com_1874_298213\n",
    "    for record in records:\n",
    "        records_lists[set_to_mine].append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13152/13152 [00:01<00:00, 8204.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 823 relevant records in set: col_2066_119645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88630/88630 [00:08<00:00, 9898.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 704 relevant records in set: col_2066_119636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2010.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 relevant records in set: col_2066_69015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "error_list = []\n",
    "for set_to_mine in Sets_to_mine:\n",
    "    relevant_counter = 0\n",
    "    for r in tqdm(records_lists[set_to_mine]):\n",
    "        meta = r.get_metadata()\n",
    "        relevant = False\n",
    "        \n",
    "        try:\n",
    "            if any([t in subj for subj in meta['subject'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj for subj in meta['title'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if any([t in subj for subj in meta['description'] for t in tlist]):\n",
    "                relevant = True\n",
    "        except:\n",
    "            pass \n",
    "        \n",
    "        try:\n",
    "            if ('doctoral' in meta['type'][0].lower()) & relevant:\n",
    "                relevant = True\n",
    "            else:\n",
    "                relevant = False\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if ('embargo' in meta['rights'][0].lower()) & relevant:\n",
    "                relevant = False\n",
    "        except:\n",
    "            pass  \n",
    "        \n",
    "        if relevant:\n",
    "            relevant_counter += 1\n",
    "            if institute == 'LUMC':\n",
    "                try:\n",
    "                    link = meta['identifier'][-1] \n",
    "                    # identify first url in list\n",
    "                    found_link = False\n",
    "                    for l in meta['identifier']:\n",
    "                        if ('http:' in l) or ('https:' in l):\n",
    "                            link = l\n",
    "                            found_link = True\n",
    "                            break\n",
    "                    if not found_link:\n",
    "                        error_list.append(f'No link found for {meta[\"identifier\"]}')  \n",
    "\n",
    "                    doc_id = link.split('/')[-1]\n",
    "                    doc_id_int = int(doc_id)+2\n",
    "\n",
    "                    link = f\"https://scholarlypublications.universiteitleiden.nl/handle/1887/{doc_id}\"\n",
    "                    linkPdf = f\"https://scholarlypublications.universiteitleiden.nl/access/item%3A{doc_id_int}/download\"\n",
    "\n",
    "                    # extract through link url. The directory can be found in <li class='ubl-file-download'> <a href='...'>\n",
    "                    # only if <a href in ubl-file-view is \"full\"\n",
    "                    r = requests.get(link)\n",
    "                    # random number between 0.5 and 2.5 seconds\n",
    "                    rndSleep = round(random.uniform(0.5, 2.5), 2)\n",
    "                    sleep(rndSleep)\n",
    "                    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "                    found, dsfound, esfound = False, False, False\n",
    "                    for _res in soup.findAll('li', {'class':'ubl-file-view'}):\n",
    "                        if _res.a is not None:\n",
    "                            if _res.a.contents[0].strip().lower() == 'full text':\n",
    "                                _pdfdir = _res.a['href']\n",
    "                                found = True\n",
    "                            elif _res.a.contents[0].strip().lower() == 'summary in dutch':\n",
    "                                _dutch_summary = _res.a['href']\n",
    "                                dsfound = True\n",
    "                            elif _res.a.contents[0].strip().lower() == 'summary in english':\n",
    "                                _english_summary = _res.a['href']\n",
    "                                esfound = True\n",
    "\n",
    "                    linkPdfAlt = f\"https://scholarlypublications.universiteitleiden.nl{_pdfdir}\" if found else None\n",
    "                    DutchSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_dutch_summary}\" if dsfound else None\n",
    "                    EnglishSummaryLink = f\"https://scholarlypublications.universiteitleiden.nl{_english_summary}\" if esfound else None\n",
    "                    \n",
    "                    try:\n",
    "                        lang = meta['language'][0]\n",
    "                    except:\n",
    "                        lang = None\n",
    "                except Exception as e:\n",
    "                    error_list.append(f'Error: {e} for link: {link}, with meta data: {meta[\"identifier\"]}')\n",
    "                    pass\n",
    "                    #raise ValueError(f'Could not find pdf link for {link}, with error raised: {e}')\n",
    "            elif institute == 'UU':                \n",
    "                link = meta['identifier'][0]\n",
    "                baselink = link.replace('dspace.library.uu.nl/', 'dspace.library.uu.nl/bitstream/')\n",
    "                linkPdf = baselink + '/full.pdf'\n",
    "\n",
    "                try:\n",
    "                    linkPdfAlt = baselink +'/'+meta['creator'][0].split(',')[0].lower()+'.pdf'\n",
    "                except:\n",
    "                    linkPdfAlt = None\n",
    "                \n",
    "                DutchSummaryLink = None\n",
    "                EnglishSummaryLink = None\n",
    "                try:\n",
    "                    lang = meta['language'][0]\n",
    "                except:\n",
    "                    lang = None\n",
    "\n",
    "            elif institute == 'Radboud':\n",
    "                try:\n",
    "                    found_link = False\n",
    "                    found_pdf_link = False\n",
    "                    for l in meta['identifier']:\n",
    "                        if l is not None:\n",
    "                            if (('http:' in l) or ('https:' in l)) & (l.split(\"/\")[-1].endswith('.pdf')):\n",
    "                                linkPdf = l\n",
    "                                found_pdf_link = True                            \n",
    "                            elif (('http:' in l) or ('https:' in l)):\n",
    "                                link = l\n",
    "                                found_link = True                           \n",
    "\n",
    "                    if not found_link:                        \n",
    "                        error_list.append(f'No link found for {meta[\"identifier\"]}')  \n",
    "                        continue\n",
    "                    if not found_pdf_link:\n",
    "                        error_list.append(f'No pdf link found for {meta[\"identifier\"]}')\n",
    "                        linkPdf = None\n",
    "\n",
    "                    linkPdfAlt = None\n",
    "                    DutchSummaryLink = None\n",
    "                    EnglishSummaryLink = None\n",
    "                    try:\n",
    "                        lang = meta['language'][0]\n",
    "                    except:\n",
    "                        lang = None\n",
    "                except Exception as e:\n",
    "                    error_list.append(f'Error: {e} for link: {link}, with meta data: {meta[\"identifier\"]}')\n",
    "                    continue\n",
    "                    #raise ValueError(f'Could not find pdf link for {link}, with error raised: {e}')                \n",
    "\n",
    "            try:\n",
    "                link_list.append({'Set':set_to_mine, \n",
    "                                'Link': link, \n",
    "                                'PdfLink': linkPdf,\n",
    "                                'PdfLinkAlt': linkPdfAlt,\n",
    "                                'DutchSummaryLink': DutchSummaryLink,\n",
    "                                'EnglishSummaryLink': EnglishSummaryLink,\n",
    "                                'Title': meta['title'][0],\n",
    "                                'Description': meta['description'][0],\n",
    "                                'Date': meta['date'][0],\n",
    "                                'Language': lang,\n",
    "                                }\n",
    "                                )\n",
    "            except Exception as e:\n",
    "                link_list.append({'Set':set_to_mine, \n",
    "                                'Link': link, \n",
    "                                'PdfLink': linkPdf,                                \n",
    "                                'PdfLinkAlt': linkPdfAlt,\n",
    "                                'DutchSummaryLink': None,\n",
    "                                'EnglishSummaryLink': None,\n",
    "                                'Title': None,\n",
    "                                'Description': None,\n",
    "                                'Date': None,\n",
    "                                'Language': None,\n",
    "                                }\n",
    "                                )\n",
    "                error_list.append(f'Error: {e} for link: {link}')\n",
    "    print(f'Found {relevant_counter} relevant records in set: {set_to_mine}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries, based on the title\n",
    "title_set = set()\n",
    "unique_link_list = []\n",
    "for el in link_list:\n",
    "    try:\n",
    "        title = el['Title']\n",
    "        if title not in title_set:\n",
    "            unique_link_list.append(el)\n",
    "            title_set.add(title)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(803, 1527)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_link_list), len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 155/803 [09:49<51:42,  4.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 269/803 [23:13<56:27,  6.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 384/803 [35:50<41:49,  5.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 489/803 [47:55<28:20,  5.42s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 522/803 [51:20<25:37,  5.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 534/803 [52:45<23:41,  5.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 566/803 [56:53<21:19,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 699/803 [1:11:33<08:55,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded...sleeping 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 803/803 [1:18:51<00:00,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116 errors while downloading pdfs\n",
      "Skipped 106 pdfs because they already existed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now we parse the link list and download the pdfs\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
    "\n",
    "# add sleep\n",
    "\n",
    "def pdf_writer(pdf_url, _pdf_path):\n",
    "    r = requests.get(pdf_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(_pdf_path, 'wb') as f:\n",
    "            for i,chunk in enumerate(r.iter_content(chunk_size=1024)): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return True, 200\n",
    "    else:\n",
    "        return False, r.status_code\n",
    "\n",
    "pdf_error_list = []\n",
    "skipped_list = []\n",
    "src_list = []\n",
    "rcode_list = []\n",
    "extract_full_text = True\n",
    "for link in tqdm(unique_link_list):\n",
    "    # MAIN PDF\n",
    "    pdf_name = f\"{institute}_{link['Link'].split('/')[-1]}.pdf\"\n",
    "    _pdf_path = os.path.join(pdf_path, pdf_name)\n",
    "    return_code = None\n",
    "    if extract_full_text:\n",
    "        pdf_url = link['PdfLink']\n",
    "        # check if pdf already exists\n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path}')\n",
    "        else:\n",
    "            # try to download the pdf\n",
    "            try:\n",
    "                write_file, return_code = pdf_writer(pdf_url, _pdf_path)\n",
    "                if write_file:\n",
    "                    src_list.append(f'Pdf downloaded from original link: {pdf_url}')            \n",
    "                elif (not write_file) & (link['PdfLinkAlt'] is not None):\n",
    "                    pdf_url = link['PdfLinkAlt']\n",
    "                    write_file, return_code = pdf_writer(pdf_url, _pdf_path)\n",
    "\n",
    "                    if write_file:                \n",
    "                        src_list.append(f'Pdf downloaded from alternative link: {pdf_url}')\n",
    "                    else:\n",
    "                        pdf_error_list.append(f'Error: {return_code}. No success for both : {link[\"PdfLinkAlt\"]} and {link[\"PdfLink\"]}')\n",
    "                else:\n",
    "                    pdf_error_list.append(f'Error: {return_code}. No success for {link[\"PdfLink\"]} and PdfLinkAlt was missing')\n",
    "            except Exception as e:\n",
    "                pdf_error_list.append(f'Error: {e} for link: {link[\"Link\"]}')  \n",
    "            if return_code == 429:\n",
    "                print(f'Rate limit exceeded...sleeping 30 seconds')\n",
    "                sleep(30)\n",
    "                continue\n",
    "            else:\n",
    "                sleep(4)\n",
    "\n",
    "    if link['DutchSummaryLink'] is not None:\n",
    "        pdf_url = link['DutchSummaryLink']\n",
    "        _pdf_path_ds = _pdf_path.replace('.pdf', '_dutch_summary.pdf')\n",
    "        if os.path.isfile(_pdf_path):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path_ds}')\n",
    "        else:\n",
    "            write_file, return_code = pdf_writer(pdf_url, _pdf_path_ds)\n",
    "            \n",
    "            if not write_file:\n",
    "                pdf_error_list.append(f'Error: {return_code}. No success for dutch summary: {pdf_url}')\n",
    "\n",
    "    if link['EnglishSummaryLink'] is not None:\n",
    "        pdf_url = link['EnglishSummaryLink']\n",
    "        _pdf_path_es = _pdf_path.replace('.pdf', '_english_summary.pdf')\n",
    "        if os.path.isfile(_pdf_path_es):\n",
    "            skipped_list.append(f'Pdf already exists: {_pdf_path_es}')\n",
    "        else:\n",
    "            write_file, return_code = pdf_writer(pdf_url, _pdf_path_es)\n",
    "            if not write_file:\n",
    "                pdf_error_list.append(f'Error: {return_code}. No success for english summary: {pdf_url}')    \n",
    "    \n",
    "print(f'Found {len(pdf_error_list)} errors while downloading pdfs')\n",
    "print(f'Skipped {len(skipped_list)} pdfs because they already existed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
