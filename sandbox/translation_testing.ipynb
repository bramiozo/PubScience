{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79274d97240ea411",
   "metadata": {},
   "source": [
    "# Translation of non-Dutch corpora\n",
    "\n",
    "* MIMIC III\n",
    "* MIMIC III CXR\n",
    "* MIMIC IV\n",
    "* eICU\n",
    "* ApolloCorpus\n",
    "* Meditron guidelines\n",
    "\n",
    "Translate using:\n",
    "* NLLB, \n",
    "* MariaMT, \n",
    "* DeepL, \n",
    "* Google Translate, \n",
    "* ChatGPT, \n",
    "* Claude, \n",
    "* Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b7f506f1769f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig, MarianMTModel\n",
    "import torch.cuda\n",
    "from torch import bfloat16\n",
    "\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv(dotenv_path='../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d330f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv('HF_DS_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8faef0182f79c717",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 'MultiNLI_Dutch_translated_with_Marianmt' \n",
    " # 'SNLI_Dutch_translated_with_Marianmt'\n",
    " \n",
    "DS_NAME =  'Apollo_Dutch_translated_with_NLLB200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c1ab906f35edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "base = \"medalpaca\"\n",
    "sub = [\"medical_meadow_mediqa\",\n",
    "      #\"medical_meadow_usmle_self_assessment\",\n",
    "      # \"medical_meadow_mmmlu\",\n",
    "      # \"medical_meadow_medical_flashcards\",\n",
    "      # \"medical_meadow_wikidoc_patient_information\",\n",
    "      # \"medical_meadow_wikidoc\",\n",
    "      # \"medical_meadow_pubmed_causal\",\n",
    "      # \"medical_meadow_medqa\",\n",
    "      # \"medical_meadow_health_advice\",\n",
    "      # \"medical_meadow_cord19\"\n",
    "      ]\n",
    "MedAlpaca_sets = [load_dataset(f\"{base}/{s}\", split=\"train\") for s in sub]\n",
    "MedAlpaca = concatenate_datasets(MedAlpaca_sets)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad99824896ea2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid pattern: '**' can only be an entire path component",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# integer-label triplet\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m DS \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFreedomIntelligence/ApolloCorpus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\datasets\\load.py:1773\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1768\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   1769\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1770\u001b[0m )\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1773\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1774\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   1775\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   1776\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1777\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1778\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1779\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1780\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1781\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1782\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1783\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[0;32m   1784\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1786\u001b[0m )\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\datasets\\load.py:1502\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1501\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m-> 1502\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1512\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\datasets\\load.py:1219\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1215\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1216\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1217\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1218\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m-> 1219\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1223\u001b[0m     )\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\datasets\\load.py:1203\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[0;32m   1189\u001b[0m             path,\n\u001b[0;32m   1190\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1193\u001b[0m             dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[0;32m   1194\u001b[0m         )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e1:  \u001b[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\datasets\\load.py:769\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetModule:\n\u001b[0;32m    760\u001b[0m     hfh_dataset_info \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[0;32m    761\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    762\u001b[0m         revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m    763\u001b[0m         token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token,\n\u001b[0;32m    764\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    767\u001b[0m         sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[0;32m    768\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mget_data_patterns_in_dataset_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhfh_dataset_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    770\u001b[0m     )\n\u001b[0;32m    771\u001b[0m     data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_hf_repo(\n\u001b[0;32m    772\u001b[0m         patterns,\n\u001b[0;32m    773\u001b[0m         dataset_info\u001b[38;5;241m=\u001b[39mhfh_dataset_info,\n\u001b[0;32m    774\u001b[0m         base_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir,\n\u001b[0;32m    775\u001b[0m         allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[0;32m    776\u001b[0m     )\n\u001b[0;32m    777\u001b[0m     split_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    778\u001b[0m         split: infer_module_for_data_files(data_files_list, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token)\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m split, data_files_list \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    780\u001b[0m     }\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\datasets\\data_files.py:662\u001b[0m, in \u001b[0;36mget_data_patterns_in_dataset_repository\u001b[1;34m(dataset_info, base_path)\u001b[0m\n\u001b[0;32m    660\u001b[0m resolver \u001b[38;5;241m=\u001b[39m partial(_resolve_single_pattern_in_dataset_repository, dataset_info, base_path\u001b[38;5;241m=\u001b[39mbase_path)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_data_files_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptyDatasetError(\n\u001b[0;32m    665\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset repository at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain any data files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\datasets\\data_files.py:223\u001b[0m, in \u001b[0;36m_get_data_files_patterns\u001b[1;34m(pattern_resolver)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m--> 223\u001b[0m         data_files \u001b[38;5;241m=\u001b[39m \u001b[43mpattern_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    225\u001b[0m             non_empty_splits\u001b[38;5;241m.\u001b[39mappend(split)\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\datasets\\data_files.py:473\u001b[0m, in \u001b[0;36m_resolve_single_pattern_in_dataset_repository\u001b[1;34m(dataset_info, pattern, base_path, allowed_extensions)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    472\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 473\u001b[0m glob_iter \u001b[38;5;241m=\u001b[39m [PurePath(filepath) \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPurePath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(filepath)]\n\u001b[0;32m    474\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    475\u001b[0m     filepath\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m glob_iter\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    483\u001b[0m     )\n\u001b[0;32m    484\u001b[0m ]  \u001b[38;5;66;03m# ignore .ipynb and __pycache__, but keep /../\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\fsspec\\spec.py:606\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[1;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    604\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind(root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 606\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[43mglob_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mends_with_sep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n\u001b[0;32m    609\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    610\u001b[0m     p: info\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(allpaths\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    619\u001b[0m }\n",
      "File \u001b[1;32md:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\fsspec\\utils.py:734\u001b[0m, in \u001b[0;36mglob_translate\u001b[1;34m(pat)\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m part:\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid pattern: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only be an entire path component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m     )\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part:\n\u001b[0;32m    738\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(_translate(part, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_sep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, not_sep))\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
     ]
    }
   ],
   "source": [
    "# integer-label triplet\n",
    "DS = load_dataset('FreedomIntelligence/ApolloCorpus')  # multi_nli, ('ms_marco', 'v2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12557995c79a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facebook/nllb-200-3.3B\n",
    "# facebook/m2m100_418M\n",
    "# facebook/mbart-large-50-many-to-many-mmt\n",
    "# t5-large\n",
    "# DeepL API\n",
    "# vvn/en-to-dutch-marianmt\n",
    "\n",
    "manual_dmap = False\n",
    "MODEL = \"facebook/nllb-200-3.3B\"\n",
    "\n",
    "if manual_dmap:\n",
    "    from accelerate import infer_auto_device_map, init_empty_weights\n",
    "    config = AutoConfig.from_pretrained(MODEL)\n",
    "    with init_empty_weights():\n",
    "        zero_model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "    device_map = infer_auto_device_map(zero_model,\n",
    "                                    max_memory={0: \"2GiB\", 1: \"2GiB\", 2: \"2GiB\", 3: \"2GiB\", \"cpu\": \"20GiB\"},\n",
    "                                    no_split_module_classes=[])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "ntm_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL,\n",
    "                                                  device_map='auto')\n",
    "#ntm_model.to(device_map)\n",
    "# # vvn/en-to-dutch-marianmt FremyCompany/opus-mt-nl-en-healthcare\n",
    "\n",
    "device = \"cuda:0\" if (torch.cuda.is_available()) & (torch.cuda.device_count()==1) else \"cpu\"\n",
    "if torch.cuda.device_count()<=1:\n",
    "    if device =='cuda:0':\n",
    "        #ntm_model.half()\n",
    "        ntm_model.to(device)\n",
    "        ntm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b71018cd767b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token = []\n",
    "for r in tqdm(DS['validation_matched']):\n",
    "    num_tokens = len(tokenizer(r['premise'], \n",
    "                               r['hypothesis'], \n",
    "                               return_tensors='pt')['input_ids'][0])\n",
    "    num_token.append(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9095ad321759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eng_XX to nl_XX, or from eng_Latn to nld_Latn\n",
    "\n",
    "def get_translations(BATCH, device='cuda:0', multilingual=False):\n",
    "    tokens = tokenizer(list(BATCH), return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    if multilingual:\n",
    "        outputs = ntm_model.generate(**tokens, forced_bos_token_id=tokenizer.lang_code_to_id[\"nl_XX\"], max_length=512)\n",
    "    else:\n",
    "        outputs = ntm_model.generate(**tokens, max_length=512)    \n",
    "    translated_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return [t.replace(\"▁\", \" \").strip() for t in translated_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5f1343ce14054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relevance(txt: str) -> bool:\n",
    "    # check if string is relevant\n",
    "    return True\n",
    "\n",
    "def Process(txts: List[Tuple[str]], tuple_len=2)->List[Tuple[str]]:\n",
    "    # filter\n",
    "    #for t in txts:\n",
    "      # check if string is relevant\n",
    "      #if Relevance(t)==False:\n",
    "      #  return None\n",
    "    # translate        \n",
    "    assert(tuple_len in [2,3]), \"tuple_len must be 2 or 3\"\n",
    "    \n",
    "    # TODO, make this for abitrarily long tuples\n",
    "    part_1 = get_translations([t[0] for t in txts], multilingual=False)\n",
    "    part_2 = get_translations([t[1] for t in txts], multilingual=False)\n",
    "    \n",
    "    if tuple_len == 3:\n",
    "        part_3 = get_translations([t[2] for t in txts], multilingual=False)\n",
    "    \n",
    "    return list(zip(part_1, part_2, part_3)) if tuple_len == 3 else list(zip(part_1, part_2))\n",
    "\n",
    "def datasetParser(dataset, text_keys=['premise', 'hypothesis'], pass_keys=['label'], batch_size=32, test=True):\n",
    "    dList = [] \n",
    "    batch = []\n",
    "    labels =[]\n",
    "    \n",
    "    for d in tqdm(dataset):\n",
    "        # make batches\n",
    "        if batch.__len__() == batch_size:\n",
    "            processed = Process(txts = batch, tuple_len=len(text_keys))                               \n",
    "            for i, _processed in enumerate(processed):\n",
    "                tdict = {text_keys[j]: s for j,s in enumerate(_processed)}\n",
    "                \n",
    "                for k in pass_keys:\n",
    "                    tdict[k] = labels[i][pass_keys.index(k)]\n",
    "                      \n",
    "                dList.append(tdict)\n",
    "            batch = []\n",
    "            labels = []\n",
    "            if test:\n",
    "                return dList\n",
    "        else:\n",
    "            batch.append(tuple([d[k] for k in text_keys]))\n",
    "            labels.append(tuple([d[k] for k in pass_keys]))\n",
    "    return dList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14864dfea4a84a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"keys\" in DS.__dir__():\n",
    "        print(f\"Processing {len(DS.keys())} datasets\")\n",
    "        for k in DS.keys():\n",
    "                print(f\"Processing {k}, with {DS[k].num_rows} rows\")\n",
    "        FinalDict = {k: Dataset.from_list(datasetParser(DS[k], \n",
    "                                                        text_keys=TEXT_KEYS, \n",
    "                                                        pass_keys=PASS_KEYS,\n",
    "                                                        batch_size=128))\n",
    "                        for k in DS.keys()\n",
    "                }\n",
    "else:\n",
    "        FinalDict = {\"train\": Dataset.from_list(datasetParser(DS, \n",
    "                                                              text_keys=TEXT_KEYS, \n",
    "                                                              pass_keys=PASS_KEYS,\n",
    "                                                              batch_size=128))}\n",
    "        \n",
    "FinalDataset = DatasetDict(FinalDict)\n",
    "FinalDataset.push_to_hub(f\"UMCU/{DS_NAME}\", token=HF_TOKEN, private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
