{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79274d97240ea411",
   "metadata": {},
   "source": [
    "# Translation of non-Dutch corpora\n",
    "\n",
    "* MIMIC III\n",
    "* MIMIC III CXR\n",
    "* MIMIC IV\n",
    "* eICU\n",
    "* ApolloCorpus\n",
    "* Meditron guidelines\n",
    "\n",
    "Translate using:\n",
    "* NLLB, \n",
    "* MariaMT, \n",
    "* DeepL, \n",
    "* Google Translate, \n",
    "* ChatGPT, \n",
    "* Claude, \n",
    "* Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b7f506f1769f1",
   "metadata": {},
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig, MarianMTModel\n",
    "import torch.cuda\n",
    "from torch import bfloat16\n",
    "\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv(dotenv_path='../.env')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d330f090",
   "metadata": {},
   "source": [
    "HF_TOKEN = os.getenv('HF_DS_TOKEN')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8faef0182f79c717",
   "metadata": {},
   "source": [
    " # 'MultiNLI_Dutch_translated_with_Marianmt' \n",
    " # 'SNLI_Dutch_translated_with_Marianmt'\n",
    " \n",
    "DS_NAME =  'Apollo_Dutch_translated_with_NLLB200'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c1ab906f35edd",
   "metadata": {},
   "source": [
    "'''\n",
    "base = \"medalpaca\"\n",
    "sub = [\"medical_meadow_mediqa\",\n",
    "      #\"medical_meadow_usmle_self_assessment\",\n",
    "      # \"medical_meadow_mmmlu\",\n",
    "      # \"medical_meadow_medical_flashcards\",\n",
    "      # \"medical_meadow_wikidoc_patient_information\",\n",
    "      # \"medical_meadow_wikidoc\",\n",
    "      # \"medical_meadow_pubmed_causal\",\n",
    "      # \"medical_meadow_medqa\",\n",
    "      # \"medical_meadow_health_advice\",\n",
    "      # \"medical_meadow_cord19\"\n",
    "      ]\n",
    "MedAlpaca_sets = [load_dataset(f\"{base}/{s}\", split=\"train\") for s in sub]\n",
    "MedAlpaca = concatenate_datasets(MedAlpaca_sets)\n",
    "'''"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad99824896ea2a",
   "metadata": {},
   "source": [
    "# integer-label triplet\n",
    "DS = load_dataset('FreedomIntelligence/ApolloCorpus')  # multi_nli, ('ms_marco', 'v2.1')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12557995c79a523",
   "metadata": {},
   "source": [
    "# facebook/nllb-200-3.3B\n",
    "# facebook/m2m100_418M\n",
    "# facebook/mbart-large-50-many-to-many-mmt\n",
    "# t5-large\n",
    "# DeepL API\n",
    "# vvn/en-to-dutch-marianmt\n",
    "\n",
    "manual_dmap = False\n",
    "MODEL = \"facebook/nllb-200-3.3B\"\n",
    "\n",
    "if manual_dmap:\n",
    "    from accelerate import infer_auto_device_map, init_empty_weights\n",
    "    config = AutoConfig.from_pretrained(MODEL)\n",
    "    with init_empty_weights():\n",
    "        zero_model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "    device_map = infer_auto_device_map(zero_model,\n",
    "                                    max_memory={0: \"2GiB\", 1: \"2GiB\", 2: \"2GiB\", 3: \"2GiB\", \"cpu\": \"20GiB\"},\n",
    "                                    no_split_module_classes=[])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "ntm_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL,\n",
    "                                                  device_map='auto')\n",
    "#ntm_model.to(device_map)\n",
    "# # vvn/en-to-dutch-marianmt FremyCompany/opus-mt-nl-en-healthcare\n",
    "\n",
    "device = \"cuda:0\" if (torch.cuda.is_available()) & (torch.cuda.device_count()==1) else \"cpu\"\n",
    "if torch.cuda.device_count()<=1:\n",
    "    if device =='cuda:0':\n",
    "        #ntm_model.half()\n",
    "        ntm_model.to(device)\n",
    "        ntm_model.eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b71018cd767b3",
   "metadata": {},
   "source": [
    "num_token = []\n",
    "for r in tqdm(DS['validation_matched']):\n",
    "    num_tokens = len(tokenizer(r['premise'], \n",
    "                               r['hypothesis'], \n",
    "                               return_tensors='pt')['input_ids'][0])\n",
    "    num_token.append(num_tokens)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9095ad321759e",
   "metadata": {},
   "source": [
    "# from eng_XX to nl_XX, or from eng_Latn to nld_Latn\n",
    "\n",
    "def get_translations(BATCH, device='cuda:0', multilingual=False):\n",
    "    tokens = tokenizer(list(BATCH), return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    if multilingual:\n",
    "        outputs = ntm_model.generate(**tokens, forced_bos_token_id=tokenizer.lang_code_to_id[\"nl_XX\"], max_length=512)\n",
    "    else:\n",
    "        outputs = ntm_model.generate(**tokens, max_length=512)    \n",
    "    translated_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return [t.replace(\"▁\", \" \").strip() for t in translated_sentences]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5f1343ce14054",
   "metadata": {},
   "source": [
    "def Relevance(txt: str) -> bool:\n",
    "    # check if string is relevant\n",
    "    return True\n",
    "\n",
    "def Process(txts: List[Tuple[str]], tuple_len=2)->List[Tuple[str]]:\n",
    "    # filter\n",
    "    #for t in txts:\n",
    "      # check if string is relevant\n",
    "      #if Relevance(t)==False:\n",
    "      #  return None\n",
    "    # translate        \n",
    "    assert(tuple_len in [2,3]), \"tuple_len must be 2 or 3\"\n",
    "    \n",
    "    # TODO, make this for abitrarily long tuples\n",
    "    part_1 = get_translations([t[0] for t in txts], multilingual=False)\n",
    "    part_2 = get_translations([t[1] for t in txts], multilingual=False)\n",
    "    \n",
    "    if tuple_len == 3:\n",
    "        part_3 = get_translations([t[2] for t in txts], multilingual=False)\n",
    "    \n",
    "    return list(zip(part_1, part_2, part_3)) if tuple_len == 3 else list(zip(part_1, part_2))\n",
    "\n",
    "def datasetParser(dataset, text_keys=['premise', 'hypothesis'], pass_keys=['label'], batch_size=32, test=True):\n",
    "    dList = [] \n",
    "    batch = []\n",
    "    labels =[]\n",
    "    \n",
    "    for d in tqdm(dataset):\n",
    "        # make batches\n",
    "        if batch.__len__() == batch_size:\n",
    "            processed = Process(txts = batch, tuple_len=len(text_keys))                               \n",
    "            for i, _processed in enumerate(processed):\n",
    "                tdict = {text_keys[j]: s for j,s in enumerate(_processed)}\n",
    "                \n",
    "                for k in pass_keys:\n",
    "                    tdict[k] = labels[i][pass_keys.index(k)]\n",
    "                      \n",
    "                dList.append(tdict)\n",
    "            batch = []\n",
    "            labels = []\n",
    "            if test:\n",
    "                return dList\n",
    "        else:\n",
    "            batch.append(tuple([d[k] for k in text_keys]))\n",
    "            labels.append(tuple([d[k] for k in pass_keys]))\n",
    "    return dList"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14864dfea4a84a5c",
   "metadata": {},
   "source": [
    "if \"keys\" in DS.__dir__():\n",
    "        print(f\"Processing {len(DS.keys())} datasets\")\n",
    "        for k in DS.keys():\n",
    "                print(f\"Processing {k}, with {DS[k].num_rows} rows\")\n",
    "        FinalDict = {k: Dataset.from_list(datasetParser(DS[k], \n",
    "                                                        text_keys=TEXT_KEYS, \n",
    "                                                        pass_keys=PASS_KEYS,\n",
    "                                                        batch_size=128))\n",
    "                        for k in DS.keys()\n",
    "                }\n",
    "else:\n",
    "        FinalDict = {\"train\": Dataset.from_list(datasetParser(DS, \n",
    "                                                              text_keys=TEXT_KEYS, \n",
    "                                                              pass_keys=PASS_KEYS,\n",
    "                                                              batch_size=128))}\n",
    "        \n",
    "FinalDataset = DatasetDict(FinalDict)\n",
    "FinalDataset.push_to_hub(f\"UMCU/{DS_NAME}\", token=HF_TOKEN, private=True)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
