{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all ARGUS documents (from 2019 and 2024):\n",
    "* deid them\n",
    "* store them in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deduce\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "deid = deduce.Deduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../.env')\n",
    "ARGUS_PATH = os.getenv('ARGUS_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_2019 = pd.read_csv(os.path.join(ARGUS_PATH, '20190118/st9_ct_radio_vrsl_18jan2019.csv'), sep=';', encoding='latin1')\n",
    "radio_2019_2 = pd.read_csv(os.path.join(ARGUS_PATH, '20190909/ST9_CT_RADIO_VRSL_09SEP2019.csv'), sep=';', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_2019 = radio_2019[['pateventid', 'verr_datum', 'ONDERZDAT', 'MEMO' ,'tekst', 'plattetext', 'reporttxt']].dropna(subset=['tekst', 'plattetext', 'reporttxt'], how='all')\n",
    "radio_2019['TEXT'] = radio_2019[['tekst', 'plattetext', 'reporttxt']].apply(lambda x: \"\\n\".join([t for t in x if not pd.isna(t)]), axis=1)\n",
    "radio_2019 = radio_2019.drop(['tekst', 'plattetext', 'reporttxt'], axis=1)\n",
    "\n",
    "radio_2019_2 = radio_2019_2[['pateventid', 'verr_datum', 'ONDERZDAT', 'MEMO' ,'tekst', 'plattetext', 'reporttxt']].dropna(subset=['tekst', 'plattetext', 'reporttxt'], how='all')\n",
    "radio_2019_2['TEXT'] = radio_2019_2[['tekst', 'plattetext', 'reporttxt']].apply(lambda x: \"\\n\".join([t for t in x if not pd.isna(t)]), axis=1)\n",
    "radio_2019_2 = radio_2019_2.drop(['tekst', 'plattetext', 'reporttxt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove repeated characters\n",
    "radio_2019['TEXT'] = radio_2019['TEXT'].str.replace(r'[^\\w]{3,}', ' ', regex=True)\n",
    "radio_2019['TEXT_ID'] = radio_2019['TEXT'].apply(lambda x: deid.deidentify(x).deidentified_text)\n",
    "radio_2019 = radio_2019.sort_values(by='verr_datum')\n",
    "\n",
    "radio_2019_2['TEXT'] = radio_2019_2['TEXT'].str.replace(r'[^\\w]{3,}', ' ', regex=True)\n",
    "radio_2019_2['TEXT_ID'] = radio_2019_2['TEXT'].apply(lambda x: deid.deidentify(x).deidentified_text)\n",
    "radio_2019_2 = radio_2019_2.sort_values(by='verr_datum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_old = pd.concat([radio_2019, radio_2019_2], axis=0)[['pateventid', 'verr_datum', 'TEXT_ID']].sort_values(by='verr_datum').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_new = pd.read_parquet(os.path.join(ARGUS_PATH, '20240909/parquet/radio_reports.parquet'))\n",
    "radio_new = radio_new.sort_values(by='onderzoeks_dt')\n",
    "radio_new = radio_new.drop_duplicates(subset=['studyId_0771','content_attachment1_plain_data'])\n",
    "radio_new = radio_new.dropna(subset=['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_new['TEXT'] = radio_new['content_attachment1_plain_data'].str.replace(r'[^\\w]{3,}', ' ', regex=True)\n",
    "radio_new['TEXT_ID'] = radio_new['TEXT'].apply(lambda x: deid.deidentify(x).deidentified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_new = radio_new[['studyId_0771', 'onderzoeks_dt', 'TEXT_ID']].reset_index(drop=True)\n",
    "radio_new['onderzoeks_dt'] = radio_new.onderzoeks_dt.dt.date\n",
    "radio_new = radio_new.rename(columns={'onderzoeks_dt': 'verr_datum', 'studyId_0771': 'pateventid'})\n",
    "radio_new['idx'] = radio_new.groupby('pateventid').cumcount()+1\n",
    "radio_new['pateventid'] = radio_new['pateventid'].astype(str)\n",
    "radio_new['pateventid'] = radio_new['pateventid'] + \"_\" + radio_new['idx'].astype(str)\n",
    "radio_new = radio_new.drop('idx', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_new = radio_new.set_index(['pateventid', 'verr_datum'])\n",
    "radio_new = radio_new.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_old = radio_old.groupby(['pateventid', 'verr_datum']).TEXT_ID.apply(lambda x: \"\\n\\n\".join(x)).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_old.to_json(os.path.join(ARGUS_PATH, 'radio_old.json'))\n",
    "radio_new.to_json(os.path.join(ARGUS_PATH, 'radio_new.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import sample, shuffle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_new = json.load(open(os.path.join(ARGUS_PATH, 'radio_new.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to make 7 sets of 50 documents\n",
    "# Each document is represented twice in the 7 sets.\n",
    "# We have 350 documents in total with 175 unique documents.\n",
    "\n",
    "# 1. select 175 keys\n",
    "# 2. randomly assign each key to 2 sets until all sets have 50 documents\n",
    "\n",
    "KEYS = list(radio_new.keys())\n",
    "shuffle(KEYS)\n",
    "random_selection = sample(KEYS, 175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupsDict = defaultdict(set)\n",
    "AvailableGroups = set(range(7))\n",
    "while len(AvailableGroups) > 0:\n",
    "    for key in random_selection:\n",
    "        if len(AvailableGroups)>1:\n",
    "            groups = sample(list(AvailableGroups), 2)\n",
    "            for group in groups:\n",
    "                GroupsDict[group].add(key)\n",
    "                if len(GroupsDict[group]) == 50:\n",
    "                    AvailableGroups.discard(group)\n",
    "        elif len(AvailableGroups) == 1:\n",
    "            group = AvailableGroups.pop()\n",
    "            GroupsDict[group].add(key)\n",
    "            if len(GroupsDict[group]) == 50:\n",
    "                AvailableGroups.discard(group)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_counter = defaultdict(int)\n",
    "\n",
    "for group, keys in GroupsDict.items():\n",
    "    for key in keys:\n",
    "        key_counter[key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_clean = re.compile(r'[()\\, \\']')\n",
    "for groupID,v in GroupsDict.items():\n",
    "    print(f'Group {groupID}: {len(v)}')\n",
    "    os.makedirs(os.path.join(ARGUS_PATH, 'A_W_W', f'Groep{str(groupID+1)}'), exist_ok=True)\n",
    "\n",
    "    for key in v:\n",
    "        txt = radio_new[key]\n",
    "        _key = re_clean.sub(\"\", key.replace(\"datetime.date\",\"-\"))\n",
    "        write_path = os.path.join(ARGUS_PATH, 'A_W_W', f'Groep{str(groupID+1)}', f'{_key}.txt')\n",
    "        # write to .txt file in os.path.join(ARGUS_PATH, 'A_W_W', f'Groep{str(groupID+1)}')\n",
    "        with open(write_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWW = {k:v for k,v in radio_new.items() if k in random_selection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bes3\\AppData\\Local\\Temp\\ipykernel_45452\\158308400.py:1: ResourceWarning: unclosed file <_io.TextIOWrapper name='T://lab_research/RES-Folder-UPOD/ARGUS24/E_ResearchData/2_ResearchData\\\\A_W_W\\\\AWW.json' mode='w' encoding='latin1'>\n",
      "  json.dump(AWW, open(os.path.join(ARGUS_PATH, 'A_W_W', 'AWW.json'), 'w', encoding='latin1'), indent=4, ensure_ascii=False)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "json.dump(AWW, open(os.path.join(ARGUS_PATH, 'A_W_W', 'AWW.json'), 'w',\n",
    "                     encoding='latin1'), indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubscience-GFzZkKDp-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
